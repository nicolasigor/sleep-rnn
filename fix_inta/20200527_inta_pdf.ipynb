{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# from sleeprnn.data.inta_ss import IntaSS, NAMES\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.helpers import reader, misc, plotter\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "NAMES = [\n",
    "    'ADGU101504',\n",
    "    'ALUR012904',\n",
    "    'BECA011405',\n",
    "    'BRCA062405',\n",
    "    'BRLO041102',\n",
    "    'BTOL083105',\n",
    "    'BTOL090105',\n",
    "    'CAPO092605',\n",
    "    'CRCA020205',\n",
    "    'ESCI031905',\n",
    "    'TAGO061203']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = IntaSS(load_checkpoint=True)\n",
    "dataset_name = dataset.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 200\n",
    "marked_channel = 'F4-C4'\n",
    "dataset_dir = os.path.abspath(os.path.join('..', 'resources/datasets/inta'))\n",
    "page_duration = 20\n",
    "page_size = page_duration * fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading S03\n",
      "Filtering F4-C4 channel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntapia/miniconda3/envs/tf/lib/python3.7/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering C4-O2 channel\n",
      "Filtering F3-C3 channel\n",
      "Filtering C3-O1 channel\n",
      "Filtering C4-C3 channel\n",
      "Using F4-C4 channel\n",
      "Stamp with too many samples removed (2745)\n",
      "Stamp with too many samples removed (3262)\n",
      "Stamp with too many samples removed (2006)\n",
      "Stamp with too many samples removed (4128)\n",
      "Stamp with too many samples removed (2137)\n",
      "Stamp with too many samples removed (2346)\n",
      "Stamp with too many samples removed (2391)\n",
      "Stamp with too many samples removed (2173)\n",
      "Stamp with too many samples removed (1794)\n",
      "Stamp with too many samples removed (1670)\n",
      "Stamp with too many samples removed (4905)\n",
      "Stamp with too many samples removed (1597)\n",
      "Stamp with too many samples removed (1643)\n",
      "Stamp with too many samples removed (2122)\n",
      "Stamp with too many samples removed (2024)\n",
      "Stamp with too many samples removed (1544)\n",
      "Stamp with too many samples removed (1769)\n",
      "Stamp with too many samples removed (2034)\n",
      "Stamp with too many samples removed (2756)\n",
      "Stamp with too many samples removed (4536)\n",
      "V1 (435, 2) Min dur [s] 0.34 Max dur [s] 2.85\n",
      "V2 (128, 2) Min dur [s] 0.335 Max dur [s] 2.665\n",
      "\n",
      "Size of overlapping groups for Valid 1\n",
      "1 marks: 435 times\n",
      "\n",
      "Size of overlapping groups for Valid 2\n",
      "1 marks: 117 times\n",
      "2 marks: 4 times\n",
      "3 marks: 1 times\n",
      "This pages (1418,)\n"
     ]
    }
   ],
   "source": [
    "# Order: (from worst to best)\n",
    "# 11 TAGO [x] | 269 conflict pages\n",
    "# 08 CAPO [x] | 82\n",
    "# 02 ALUR [...] | 314\n",
    "# 06 BTOL08 | 9\n",
    "# 04 BRCA | 479\n",
    "# 09 CRCA | 308\n",
    "# 10 ESCI | 69\n",
    "# 05 BRLO | 156\n",
    "# 07 BTOL09 | 22\n",
    "# 03 BECA | 3\n",
    "# 01 ADGU | 232\n",
    "\n",
    "# Load stamps of subject\n",
    "subject_id = 3\n",
    "\n",
    "print('Loading S%02d' % subject_id)\n",
    "path_stamps = os.path.join(dataset_dir, 'label/spindle/original/', 'SS_%s.txt' % NAMES[subject_id - 1])\n",
    "path_signals = os.path.join(dataset_dir, 'register', '%s.rec' % NAMES[subject_id - 1]) \n",
    "signal_dict = reader.read_signals_from_edf(path_signals)\n",
    "signal_names = list(signal_dict.keys())\n",
    "to_show_names = misc.get_inta_eeg_names(signal_names) + misc.get_inta_eog_emg_names(signal_names)\n",
    "for single_name in misc.get_inta_eeg_names(signal_names):\n",
    "    this_signal = signal_dict[single_name]\n",
    "    print('Filtering %s channel' % single_name)\n",
    "    this_signal = utils.broad_filter(this_signal, fs)\n",
    "    signal_dict[single_name] = this_signal\n",
    "raw_stamps_1, raw_stamps_2 = reader.load_raw_inta_stamps(path_stamps, path_signals, min_samples=20, chn_idx=0)\n",
    "durations_1 = (raw_stamps_1[:, 1] - raw_stamps_1[:, 0]) / fs\n",
    "durations_2 = (raw_stamps_2[:, 1] - raw_stamps_2[:, 0]) / fs\n",
    "print('V1', raw_stamps_1.shape, 'Min dur [s]', durations_1.min(), 'Max dur [s]', durations_1.max())\n",
    "print('V2', raw_stamps_2.shape, 'Min dur [s]', durations_2.min(), 'Max dur [s]', durations_2.max())\n",
    "overlap_m = utils.get_overlap_matrix(raw_stamps_1, raw_stamps_1)\n",
    "groups_overlap_1 = utils.overlapping_groups(overlap_m)\n",
    "overlap_m = utils.get_overlap_matrix(raw_stamps_2, raw_stamps_2)\n",
    "groups_overlap_2 = utils.overlapping_groups(overlap_m)\n",
    "n_overlaps_1 = [len(single_group) for single_group in groups_overlap_1]\n",
    "values_1, counts_1 = np.unique(n_overlaps_1, return_counts=True)\n",
    "print('\\nSize of overlapping groups for Valid 1')\n",
    "for value, count in zip(values_1, counts_1):\n",
    "    print('%d marks: %d times' % (value, count))\n",
    "n_overlaps_2 = [len(single_group) for single_group in groups_overlap_2]\n",
    "values_2, counts_2 = np.unique(n_overlaps_2, return_counts=True)\n",
    "print('\\nSize of overlapping groups for Valid 2')\n",
    "for value, count in zip(values_2, counts_2):\n",
    "    print('%d marks: %d times' % (value, count))\n",
    "max_overlaps = np.max([values_1.max(), values_2.max()]) - 1\n",
    "this_pages = np.arange(1, signal_dict[marked_channel].size//page_size - 1)\n",
    "print('This pages', this_pages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marks automatically added: (437, 2)\n",
      "Remaining conflicts:\n",
      "    V1: 0\n",
      "    V2: 3\n"
     ]
    }
   ],
   "source": [
    "# Select marks without doubt\n",
    "groups_in_doubt_v1_list = []\n",
    "groups_in_doubt_v2_list = []\n",
    "\n",
    "iou_to_accept = 0.8\n",
    "marks_without_doubt = []\n",
    "overlap_between_1_and_2 = utils.get_overlap_matrix(raw_stamps_1, raw_stamps_2)\n",
    "\n",
    "for single_group in groups_overlap_2:\n",
    "    if len(single_group) == 1:\n",
    "        marks_without_doubt.append(raw_stamps_2[single_group[0], :])\n",
    "    elif len(single_group) == 2:\n",
    "        # check if IOU between marks is close 1, if close, then just choose newer (second one)\n",
    "        option1_mark = raw_stamps_2[single_group[0], :]\n",
    "        option2_mark = raw_stamps_2[single_group[1], :]\n",
    "        iou_between_marks = metrics.get_iou(option1_mark, option2_mark)\n",
    "        if iou_between_marks >= iou_to_accept:\n",
    "            marks_without_doubt.append(option2_mark)\n",
    "        else:\n",
    "            groups_in_doubt_v2_list.append(single_group)\n",
    "    else:\n",
    "        groups_in_doubt_v2_list.append(single_group)\n",
    "        \n",
    "for single_group in groups_overlap_1:\n",
    "    is_in_doubt = False\n",
    "    # Check if entire group is overlapping\n",
    "    all_are_overlapping_2 = np.all(overlap_between_1_and_2[single_group, :].sum(axis=1))\n",
    "    if not all_are_overlapping_2:\n",
    "        # Consider the mark\n",
    "        if len(single_group) == 1:\n",
    "            # Since has size 1 and is no overlapping 2, accept it\n",
    "            marks_without_doubt.append(raw_stamps_1[single_group[0], :])\n",
    "        elif len(single_group) == 2:\n",
    "            # check if IOU between marks is close 1, if close, then just choose newer (second one) since there is no intersection\n",
    "            option1_mark = raw_stamps_1[single_group[0], :]\n",
    "            option2_mark = raw_stamps_1[single_group[1], :]\n",
    "            iou_between_marks = metrics.get_iou(option1_mark, option2_mark)\n",
    "            if iou_between_marks >= iou_to_accept:\n",
    "                marks_without_doubt.append(raw_stamps_1[single_group[1], :])\n",
    "            else:\n",
    "                is_in_doubt = True\n",
    "        else:\n",
    "            is_in_doubt = True\n",
    "    if is_in_doubt:\n",
    "        groups_in_doubt_v1_list.append(single_group)\n",
    "\n",
    "marks_without_doubt = np.stack(marks_without_doubt, axis=0)\n",
    "marks_without_doubt = np.sort(marks_without_doubt, axis=0)\n",
    "print('Marks automatically added:', marks_without_doubt.shape)\n",
    "print('Remaining conflicts:')\n",
    "print('    V1: %d' % len(groups_in_doubt_v1_list))\n",
    "print('    V2: %d' % len(groups_in_doubt_v2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of pages with conflict 3\n"
     ]
    }
   ],
   "source": [
    "show_complete_conflict_detail = False\n",
    "\n",
    "conflict_pages = []\n",
    "\n",
    "if show_complete_conflict_detail:\n",
    "    print('Conflict detail')\n",
    "for single_group in groups_in_doubt_v1_list:\n",
    "    group_stamps = raw_stamps_1[single_group, :]\n",
    "    min_sample = group_stamps.min()\n",
    "    max_sample = group_stamps.max()\n",
    "    center_group = (min_sample + max_sample) / 2\n",
    "    integer_page = int(center_group / page_size)\n",
    "    decimal_part = np.round(2 * (center_group % page_size) / page_size) / 2 - 0.5\n",
    "    page_location = integer_page + decimal_part\n",
    "    conflict_pages.append(page_location)\n",
    "    if show_complete_conflict_detail:\n",
    "        print('V1 - Group of size %d at page %1.1f' % (group_stamps.shape[0], page_location ))\n",
    "\n",
    "for single_group in groups_in_doubt_v2_list:\n",
    "    group_stamps = raw_stamps_2[single_group, :]\n",
    "    min_sample = group_stamps.min()\n",
    "    max_sample = group_stamps.max()\n",
    "    center_group = (min_sample + max_sample) / 2\n",
    "    integer_page = int(center_group / page_size)\n",
    "    decimal_part = np.round(2 * (center_group % page_size) / page_size) / 2 - 0.5\n",
    "    page_location = integer_page + decimal_part\n",
    "    conflict_pages.append(page_location)\n",
    "    if show_complete_conflict_detail:\n",
    "        print('V2 - Group of size %d at page %1.1f' % (group_stamps.shape[0], page_location ))\n",
    "conflict_pages = np.unique(conflict_pages)\n",
    "\n",
    "print('')\n",
    "print('Number of pages with conflict %d' % conflict_pages.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found for \"Revisionn_SS_BECA011405.txt\":\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Add available final versions of marks\n",
    "string_to_search = 'Revisionn_SS_%s.txt' % NAMES[subject_id-1]\n",
    "available_files = os.listdir('mark_files')\n",
    "res = [f for f in available_files if string_to_search in f]\n",
    "print('Files found for \"%s\":' % string_to_search)\n",
    "print(res)\n",
    "if res:\n",
    "    this_final_marks = np.loadtxt(os.path.join('mark_files', res[0]))\n",
    "    this_final_marks = this_final_marks[:, [0, 1]]\n",
    "else:\n",
    "    this_final_marks = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotter functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_page_conflict(page_chosen, ax, show_final=False):\n",
    "    # conflict id starts from 1.\n",
    "    signal_uv_to_display = 20\n",
    "    microvolt_per_second = 200  # Aspect ratio\n",
    "    page_start = page_chosen * page_size\n",
    "    page_end = page_start + page_size\n",
    "    segment_stamps = utils.filter_stamps(marks_without_doubt, page_start, page_end)\n",
    "    segment_stamps_valid_1 = utils.filter_stamps(raw_stamps_1, page_start, page_end)\n",
    "    segment_stamps_valid_2 = utils.filter_stamps(raw_stamps_2, page_start, page_end)\n",
    "    segment_stamps_final = utils.filter_stamps(this_final_marks, page_start, page_end) if show_final else []    \n",
    "    time_axis = np.arange(page_start, page_end) / fs\n",
    "    x_ticks = np.arange(time_axis[0], time_axis[-1]+1, 1)\n",
    "    dy_valid = 40\n",
    "    shown_valid = False\n",
    "    valid_label = 'Candidate mark'\n",
    "    # Show valid 1\n",
    "    valid_start = -100\n",
    "    shown_groups_1 = []\n",
    "    for j, this_stamp in enumerate(segment_stamps_valid_1):\n",
    "        idx_stamp = np.where([np.all(this_stamp == single_stamp) for single_stamp in raw_stamps_1])[0]\n",
    "        idx_group = np.where([idx_stamp in single_group for single_group in groups_overlap_1])[0][0].item()\n",
    "        shown_groups_1.append(idx_group)\n",
    "    shown_groups_1 = np.unique(shown_groups_1)\n",
    "    max_size_shown = 0\n",
    "    for single_group in shown_groups_1:\n",
    "        group_stamps = [raw_stamps_1[single_idx] for single_idx in groups_overlap_1[single_group]]\n",
    "        group_stamps = np.stack(group_stamps, axis=0)\n",
    "        group_size = group_stamps.shape[0]\n",
    "        if group_size > max_size_shown:\n",
    "            max_size_shown = group_size\n",
    "        for j, single_stamp in enumerate(group_stamps):\n",
    "            stamp_idx = int(1 * 1e4 + groups_overlap_1[single_group][j])\n",
    "            color_for_display = viz.PALETTE['red']\n",
    "            single_stamp = np.clip(single_stamp.copy(), a_min=page_start, a_max=page_end)  # new\n",
    "            ax.plot(\n",
    "                single_stamp/fs, [valid_start-j*dy_valid, valid_start-j*dy_valid], \n",
    "                color=color_for_display, linewidth=1.5, label=valid_label)\n",
    "            if (page_end - single_stamp[1])/fs > 0.5:  # new\n",
    "                ax.annotate(stamp_idx, (single_stamp[1]/fs+0.05, valid_start-j*dy_valid-10), fontsize=7)\n",
    "            shown_valid = True\n",
    "            valid_label = None\n",
    "    valid_1_center = valid_start - (max_size_shown//2) * dy_valid\n",
    "    # Show valid 2\n",
    "    valid_start = - max_size_shown * dy_valid - 200\n",
    "    shown_groups_2 = []\n",
    "    for j, this_stamp in enumerate(segment_stamps_valid_2):\n",
    "        idx_stamp = np.where([np.all(this_stamp == single_stamp) for single_stamp in raw_stamps_2])[0]\n",
    "        idx_group = np.where([idx_stamp in single_group for single_group in groups_overlap_2])[0][0].item()\n",
    "        shown_groups_2.append(idx_group)\n",
    "    shown_groups_2 = np.unique(shown_groups_2)\n",
    "    max_size_shown = 0\n",
    "    for single_group in shown_groups_2:\n",
    "        group_stamps = [raw_stamps_2[single_idx] for single_idx in groups_overlap_2[single_group]]\n",
    "        group_stamps = np.stack(group_stamps, axis=0)\n",
    "        group_size = group_stamps.shape[0]\n",
    "        if group_size > max_size_shown:\n",
    "            max_size_shown = group_size\n",
    "        for j, single_stamp in enumerate(group_stamps):\n",
    "            stamp_idx = int(2 * 1e4 + groups_overlap_2[single_group][j])\n",
    "            color_for_display = viz.PALETTE['red']\n",
    "            single_stamp = np.clip(single_stamp.copy(), a_min=page_start, a_max=page_end)  # new\n",
    "            ax.plot(\n",
    "                single_stamp/fs, [valid_start-j*dy_valid, valid_start-j*dy_valid], \n",
    "                color=color_for_display, linewidth=1.5, label=valid_label)\n",
    "            if (page_end - single_stamp[1])/fs > 0.5:  # new\n",
    "                ax.annotate(stamp_idx, (single_stamp[1]/fs+0.05, valid_start-j*dy_valid-10), fontsize=7)\n",
    "            shown_valid = True\n",
    "            valid_label = None\n",
    "    valid_2_center = valid_start - (max_size_shown//2) * dy_valid\n",
    "    # Signal\n",
    "    y_max = 150\n",
    "    y_sep = 300\n",
    "    start_signal_plot = valid_start - max_size_shown * dy_valid - y_sep\n",
    "    y_minor_ticks = []\n",
    "    for k, name in enumerate(to_show_names):\n",
    "        if name == 'F4-C4':\n",
    "            stamp_center = start_signal_plot-y_sep*k\n",
    "        #if name == 'EMG':\n",
    "        #    continue\n",
    "        segment_fs = fs\n",
    "        segment_start = int(page_chosen * page_duration * segment_fs)\n",
    "        segment_end = int(segment_start + page_duration * segment_fs)\n",
    "        segment_signal = signal_dict[name][segment_start:segment_end]\n",
    "        segment_time_axis = np.arange(segment_start, segment_end) / segment_fs\n",
    "        ax.plot(\n",
    "            segment_time_axis, start_signal_plot-y_sep*k + segment_signal, linewidth=0.7, color=viz.PALETTE['grey'])\n",
    "        y_minor_ticks.append(start_signal_plot-y_sep*k + signal_uv_to_display)\n",
    "        y_minor_ticks.append(start_signal_plot-y_sep*k - signal_uv_to_display)\n",
    "    plotter.add_scalebar(\n",
    "        ax, matchx=False, matchy=False, hidex=False, hidey=False, sizex=1, sizey=100, \n",
    "        labelx='1 s', labely='100 uV', loc=1)\n",
    "    expert_shown = False\n",
    "    for expert_stamp in segment_stamps:\n",
    "        expert_stamp = np.clip(expert_stamp.copy(), a_min=page_start, a_max=page_end)  # new\n",
    "        label = None if expert_shown else 'Accepted mark (automatic)'\n",
    "        ax.plot(\n",
    "            expert_stamp / fs, [stamp_center-50, stamp_center-50], \n",
    "            color=viz.PALETTE['green'], linewidth=2, label=label)\n",
    "        expert_shown = True\n",
    "    expert_manual_shown = False\n",
    "    for final_stamp in segment_stamps_final:\n",
    "        final_stamp = np.clip(final_stamp.copy(), a_min=page_start, a_max=page_end)  # new\n",
    "        label = None if expert_manual_shown else 'Expert Final Version'\n",
    "        ax.fill_between(\n",
    "            final_stamp / fs, 100+stamp_center, -100+stamp_center, \n",
    "            facecolor=viz.PALETTE['grey'], alpha=0.4,  label=label, edgecolor='k')\n",
    "        expert_manual_shown = True\n",
    "    ticks_valid = [valid_1_center, valid_2_center]\n",
    "    ticks_signal = [start_signal_plot-y_sep*k for k in range(len(to_show_names))]\n",
    "    ticklabels_valid = ['V1', 'V2']\n",
    "    total_ticks = ticks_valid + ticks_signal\n",
    "    total_ticklabels = ticklabels_valid + to_show_names[:-2] + ['MOR', 'EMG']\n",
    "    ax.set_yticks(total_ticks)\n",
    "    ax.set_yticklabels(total_ticklabels)\n",
    "    ax.set_xlim([time_axis[0], time_axis[-1]])\n",
    "    ax.set_ylim([-y_max - 30 + ticks_signal[-1], 100])\n",
    "    ax.set_title('Subject %d (%s INTA). Page in record: %1.1f. (intervals of 0.5s are shown as a vertical grid).' \n",
    "                 % (subject_id, NAMES[subject_id-1], page_chosen), fontsize=10, y=1.05)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticks(np.arange(time_axis[0], time_axis[-1], 0.5), minor=True)\n",
    "    ax.grid(b=True, axis='x', which='minor')\n",
    "    ax.tick_params(labelsize=7.5, labelbottom=True ,labeltop=True, bottom=True, top=True)\n",
    "    ax.set_aspect(1/microvolt_per_second)\n",
    "    ax.set_xlabel('Time [s]', fontsize=8)\n",
    "    if expert_shown or shown_valid:\n",
    "        lg = ax.legend(loc='lower left', fontsize=8)\n",
    "        for lh in lg.legendHandles:\n",
    "            lh.set_alpha(1.0)\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_conflict = 127  # min is 1\n",
    "last_conflict =  140 # None\n",
    "\n",
    "folder_name = '%s_conflicts' % NAMES[subject_id - 1]\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "n_conflicts = conflict_pages.size\n",
    "if last_conflict is None:\n",
    "    last_conflict = n_conflicts\n",
    "print('Total conflicting pages: %d' % n_conflicts)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 1+len(to_show_names)), dpi=180)\n",
    "for conflict_id in range(start_from_conflict, last_conflict + 1):\n",
    "    fname = os.path.join(folder_name, 'conflict_%03d.pdf' % conflict_id)\n",
    "    ax.clear()\n",
    "    page_chosen = conflict_pages[conflict_id-1]\n",
    "    ax = plot_page_conflict(page_chosen, ax)\n",
    "    plt.savefig(fname, dpi=200, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Correction Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_conflict = 127\n",
    "optional_end_conflict = 127 + 7\n",
    "\n",
    "if optional_end_conflict is None:\n",
    "    optional_end_conflict = n_conflicts\n",
    "\n",
    "folder_name = '%s_conflicts_final' % NAMES[subject_id - 1]\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "n_conflicts = conflict_pages.size\n",
    "print('Total conflicting pages: %d' % n_conflicts)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 1+len(to_show_names)), dpi=180)\n",
    "for conflict_id in range(start_from_conflict, optional_end_conflict + 1):\n",
    "    fname = os.path.join(folder_name, 'conflict_%03d.pdf' % conflict_id)\n",
    "    ax.clear()\n",
    "    page_chosen = conflict_pages[conflict_id-1]\n",
    "    ax = plot_page_conflict(page_chosen, ax, show_final=True)\n",
    "    plt.savefig(fname, dpi=200, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual validation of all N2-N3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original selected pages: 385\n",
      "582\n"
     ]
    }
   ],
   "source": [
    "original_page_duration = 30\n",
    "\n",
    "path_states = os.path.join(dataset_dir, 'label/state/', 'StagesOnly_%s.txt' % NAMES[subject_id - 1])\n",
    "states = np.loadtxt(path_states, dtype='i', delimiter=' ')\n",
    "# Crop signal and states to a valid length\n",
    "block_duration = 60\n",
    "block_size = block_duration * fs\n",
    "n_blocks = np.floor(signal_dict['F4-C4'].size / block_size)\n",
    "max_sample = int(n_blocks * block_size)\n",
    "max_page = int(max_sample / (original_page_duration * fs))\n",
    "hypnogram_original = states[:max_page]\n",
    "\n",
    "# Collect stages\n",
    "# Sleep states dictionary for INTA:\n",
    "# 1:SQ4   2:SQ3   3:SQ2   4:SQ1   5:REM   6:WA\n",
    "stages_valid = [3]\n",
    "\n",
    "signal_total_duration = len(hypnogram_original) * original_page_duration\n",
    "select_pages_original = np.sort(np.concatenate([np.where(hypnogram_original == state_id)[0] for state_id in stages_valid]))\n",
    "print(\"Original selected pages: %d\" % len(select_pages_original))\n",
    "onsets_original = select_pages_original * original_page_duration\n",
    "offsets_original = (select_pages_original + 1) * original_page_duration\n",
    "total_pages = int(np.ceil(signal_total_duration / page_duration))\n",
    "select_pages_onehot = np.zeros(total_pages, dtype=np.int16)\n",
    "for i in range(total_pages):\n",
    "    onset_new_page = i * page_duration\n",
    "    offset_new_page = (i + 1) * page_duration\n",
    "    for j in range(select_pages_original.size):\n",
    "        intersection = (onset_new_page < offsets_original[j]) and (onsets_original[j] < offset_new_page)\n",
    "        if intersection:\n",
    "            select_pages_onehot[i] = 1\n",
    "            break\n",
    "select_pages = np.where(select_pages_onehot == 1)[0]\n",
    "print(select_pages.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "\n",
    "start_from_page = 1  # min is 1\n",
    "last_page =  10 # None\n",
    "\n",
    "folder_name = '%s_SQ2-SQ3' % NAMES[subject_id - 1]\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "n_selected_pages = select_pages.size\n",
    "if last_page is None:\n",
    "    last_page = n_selected_pages\n",
    "print('Total selected pages: %d' % n_selected_pages)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 1+len(to_show_names)), dpi=180)\n",
    "for page_id in range(start_from_page, last_page + 1):\n",
    "    fname = os.path.join(folder_name, 'page_%03d.pdf' % page_id)\n",
    "    ax.clear()\n",
    "    page_chosen = select_pages[page_id-1]\n",
    "    ax = plot_page_conflict(page_chosen, ax)\n",
    "    plt.savefig(fname, dpi=200, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 168,  169,  170,  171,  172,  173,  174,  175,  176,  177,  178,\n",
       "        179,  180,  181,  182,  183,  184,  185,  186,  187,  188,  189,\n",
       "        190,  191,  192,  193,  194,  195,  196,  197,  198,  199,  200,\n",
       "        201,  202,  203,  204,  205,  206,  207,  208,  209,  210,  211,\n",
       "        212,  213,  214,  215,  216,  217,  218,  219,  220,  221,  222,\n",
       "        223,  224,  225,  226,  227,  228,  229,  230,  231,  232,  357,\n",
       "        358,  359,  360,  361,  362,  363,  364,  365,  366,  367,  368,\n",
       "        369,  370,  371,  372,  373,  374,  375,  376,  377,  378,  379,\n",
       "        380,  381,  382,  383,  384,  385,  386,  387,  388,  389,  508,\n",
       "        509,  510,  511,  512,  513,  514,  515,  516,  517,  518,  519,\n",
       "        520,  521,  522,  523,  524,  525,  526,  527,  528,  529,  530,\n",
       "        531,  532,  533,  534,  535,  536,  537,  538,  539,  540,  541,\n",
       "        542,  543,  544,  545,  546,  547,  548,  549,  550,  551,  552,\n",
       "        553,  554,  555,  556,  557,  558,  559,  560,  561,  562,  563,\n",
       "        564,  565,  566,  567,  568,  569,  570,  571,  572,  573,  574,\n",
       "        575,  576,  577,  578,  579,  580,  581,  582,  583,  584,  585,\n",
       "        586,  587,  588,  589,  590,  591,  592,  593,  594,  595,  596,\n",
       "        597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
       "        608,  609,  610,  630,  631,  632,  633,  634,  635,  636,  637,\n",
       "        638,  639,  640,  641,  642,  643,  644,  645,  646,  647,  648,\n",
       "        649,  650,  651,  652,  653,  654,  655,  656,  657,  658,  659,\n",
       "        660,  661,  662,  663,  664,  665,  666,  667,  668,  669,  670,\n",
       "        671,  672,  673,  674,  675,  676,  677,  678,  679,  747,  748,\n",
       "        749,  750,  751,  752,  753,  754,  755,  756,  757,  758,  759,\n",
       "        760,  761,  762,  763,  764,  765,  766,  767,  768,  769,  770,\n",
       "        771,  772,  773,  774,  775,  776,  777,  778,  779,  780,  781,\n",
       "        782,  783,  784,  785,  786,  787,  788,  789,  790,  791,  792,\n",
       "        793,  794,  795,  796,  797,  798,  799,  800,  801,  802,  803,\n",
       "        804,  805,  806,  807,  808,  809,  810,  811,  812,  813,  814,\n",
       "        815,  816,  817,  818,  819,  820,  821,  822,  823,  824,  825,\n",
       "        826,  827,  828,  829,  830,  831,  832,  833,  834,  835,  836,\n",
       "        837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
       "        848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,\n",
       "        859,  860,  861,  862,  863,  864,  865,  866,  867,  868,  869,\n",
       "        870,  871,  872,  873,  874,  875,  876,  877,  878,  879,  880,\n",
       "        881,  882,  883,  884,  885,  886,  887,  888,  889,  890,  891,\n",
       "        892,  893,  894,  895,  896,  897,  898,  899,  900,  901,  902,\n",
       "        903,  904,  905,  906,  907,  908,  909,  910,  911,  912,  913,\n",
       "        914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
       "        925,  926,  927,  928,  929,  930,  931,  932,  933,  934, 1030,\n",
       "       1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041,\n",
       "       1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052,\n",
       "       1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
       "       1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074,\n",
       "       1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085,\n",
       "       1086, 1087, 1088, 1089, 1090, 1091, 1174, 1175, 1176, 1177, 1178,\n",
       "       1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189,\n",
       "       1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200,\n",
       "       1201, 1202, 1203, 1204, 1205, 1206, 1207, 1248, 1249, 1250, 1251,\n",
       "       1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262,\n",
       "       1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273,\n",
       "       1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284,\n",
       "       1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  65]\n",
      " [ 65  98]\n",
      " [ 98 201]\n",
      " [201 251]\n",
      " [251 439]\n",
      " [439 501]\n",
      " [501 535]\n",
      " [535 582]]\n",
      "N cycles: 8\n",
      "\n",
      "Cycle 1/8\n",
      "Cycle size: 65\n",
      "Check success: True\n",
      "\n",
      "Cycle 2/8\n",
      "Cycle size: 33\n",
      "Check success: True\n",
      "\n",
      "Cycle 3/8\n",
      "Cycle size: 103\n",
      "Check success: True\n",
      "\n",
      "Cycle 4/8\n",
      "Cycle size: 50\n",
      "Check success: True\n",
      "\n",
      "Cycle 5/8\n",
      "Cycle size: 188\n",
      "Check success: True\n",
      "\n",
      "Cycle 6/8\n",
      "Cycle size: 62\n",
      "Check success: True\n",
      "\n",
      "Cycle 7/8\n",
      "Cycle size: 34\n",
      "Check success: True\n",
      "\n",
      "Cycle 8/8\n",
      "Cycle size: 47\n",
      "Check success: True\n"
     ]
    }
   ],
   "source": [
    "break_points = np.where(np.diff(select_pages)>1)[0]\n",
    "cycle_bins = break_points + 1\n",
    "cycle_bins = np.concatenate([[0], cycle_bins, [select_pages.size]])\n",
    "cycles = [[cycle_bins[i], cycle_bins[i+1]] for i in range(cycle_bins.size-1)]\n",
    "cycles = np.stack(cycles, axis=0)\n",
    "print(cycles)\n",
    "print(\"N cycles:\", len(cycles))\n",
    "\n",
    "page_selection_list = []\n",
    "for which_cycle in range(len(cycles)):\n",
    "    print(\"\\nCycle %d/%d\" % (which_cycle+1, len(cycles)))\n",
    "    subset_cycle = select_pages[cycles[which_cycle, 0]:cycles[which_cycle, 1]]\n",
    "    page_selection_list.append(subset_cycle)\n",
    "    print(\"Cycle size:\", subset_cycle.size)\n",
    "    print(\"Check success:\", np.all(np.diff(subset_cycle) < 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycle 1/8 with 65 pages\n",
      "\n",
      "Cycle 2/8 with 33 pages\n",
      "\n",
      "Cycle 3/8 with 103 pages\n",
      "\n",
      "Cycle 4/8 with 50 pages\n",
      "\n",
      "Cycle 5/8 with 188 pages\n",
      "\n",
      "Cycle 6/8 with 62 pages\n",
      "\n",
      "Cycle 7/8 with 34 pages\n",
      "\n",
      "Cycle 8/8 with 47 pages\n"
     ]
    }
   ],
   "source": [
    "# save by cycles\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 1+len(to_show_names)), dpi=180)\n",
    "for which_cycle in range(len(cycles)):\n",
    "    subset_cycle = select_pages[cycles[which_cycle, 0]:cycles[which_cycle, 1]]\n",
    "    print(\"\\nCycle %d/%d with %d pages\" % (which_cycle+1, len(cycles), len(subset_cycle)))\n",
    "    folder_name = '%s_SQ2_cycle%d' % (NAMES[subject_id - 1], which_cycle+1)\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    for page_id, page_chosen in enumerate(subset_cycle):\n",
    "        page_id += 1\n",
    "        fname = os.path.join(folder_name, 'page_%03d.pdf' % page_id)\n",
    "        ax.clear()\n",
    "        ax = plot_page_conflict(page_chosen, ax)\n",
    "        plt.savefig(fname, dpi=200, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
