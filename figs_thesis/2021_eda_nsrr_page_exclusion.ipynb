{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants, viz, pkeys\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import det_utils\n",
    "from figs_thesis import fig_utils\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event\n",
    "\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max amplitude preliminaries (MASS-KC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = load_dataset('mass_kc')\n",
    "signals = mass.get_signals(normalize_clip=False)\n",
    "marks = mass.get_stamps(pages_subset='n2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kc_min = []\n",
    "all_kc_max = []\n",
    "for sub_signal, sub_marks in zip(signals, marks):\n",
    "    kc = [sub_signal[m[0]:m[1]+1] for m in sub_marks]\n",
    "    kc_min = np.array([np.min(s) for s in kc])\n",
    "    kc_max = np.array([np.max(s) for s in kc])\n",
    "    all_kc_min.append(kc_min)\n",
    "    all_kc_max.append(kc_max)\n",
    "all_kc_min = np.concatenate(all_kc_min)\n",
    "all_kc_max = np.concatenate(all_kc_max)\n",
    "print(all_kc_min.shape, all_kc_max.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=100)\n",
    "ax = axes[0]\n",
    "ax.hist(all_kc_min)\n",
    "ax.set_title(\"Neg Peak\")\n",
    "ax = axes[1]\n",
    "ax.hist(all_kc_max)\n",
    "ax.set_title(\"Pos Peak\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "prct = 98\n",
    "\n",
    "print(\"Negative Peak prct %d: %1.4f uV\" % (prct, np.percentile(all_kc_min, 100-prct)))\n",
    "print(\"Positive Peak prct %d: %1.4f uV\" % (prct, np.percentile(all_kc_max, prct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal parameters preliminaries (MODA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moda = load_dataset('moda_ss')\n",
    "signals = moda.get_signals(normalize_clip=False)\n",
    "pages = moda.get_pages(pages_subset='n2')\n",
    "\n",
    "moda_page_stats = {\n",
    "    'scale': [],\n",
    "    'exponent': [],\n",
    "    'max_ratio': [],\n",
    "    'standard_deviation': [],\n",
    "    'r2': [],\n",
    "}\n",
    "\n",
    "for sub_signal, sub_pages in tqdm(zip(signals, pages)):\n",
    "    sub_signal = sub_signal.reshape(-1, moda.fs * moda.page_duration)[sub_pages]  # [n_pages, n_samples]\n",
    "    freq, pages_spectrum = utils.compute_pagewise_fft(sub_signal, moda.fs, window_duration=2)\n",
    "    pages_scales, pages_exponents = utils.compute_pagewise_powerlaw(freq, pages_spectrum)  # (n_pages,)\n",
    "    \n",
    "    # Deviation from power law\n",
    "    f_min = 2\n",
    "    f_max = 30\n",
    "    valid_locs = np.where((freq >= f_min) & (freq <= f_max))[0]\n",
    "    dev_f = freq[valid_locs]\n",
    "    dev_x = pages_spectrum[:, valid_locs]\n",
    "    dev_x_law = [fit_s * (dev_f ** fit_e) for fit_s, fit_e in zip(pages_scales, pages_exponents)]\n",
    "    dev_x_law = np.stack(dev_x_law, axis=0)\n",
    "    error = dev_x / dev_x_law  # n_pages, n_freqs\n",
    "    max_error = np.max(error, axis=1)  # to detect weird peaks, shape (n_pages,)\n",
    "    \n",
    "    # for r2, we remove sigma\n",
    "    valid_locs = np.where((dev_f < 10) | (dev_f > 17))[0]\n",
    "    dev_f = dev_f[valid_locs]\n",
    "    log_dev_x = np.log(dev_x[:, valid_locs])\n",
    "    log_dev_x_law = np.log(dev_x_law[:, valid_locs])\n",
    "    squared_data = np.sum((log_dev_x - log_dev_x.mean(axis=1).reshape(-1, 1)) ** 2, axis=1)\n",
    "    squared_residuals = np.sum((log_dev_x - log_dev_x_law) ** 2, axis=1)\n",
    "    r2 = 1 - squared_residuals / squared_data  # (n_pages,)\n",
    "    \n",
    "    moda_page_stats['scale'].append(pages_scales)\n",
    "    moda_page_stats['exponent'].append(pages_exponents)\n",
    "    moda_page_stats['max_ratio'].append(max_error)\n",
    "    moda_page_stats['standard_deviation'].append(sub_signal.std(axis=1))\n",
    "    moda_page_stats['r2'].append(r2)\n",
    "\n",
    "for key in moda_page_stats.keys():\n",
    "    moda_page_stats[key] = np.concatenate(moda_page_stats[key])\n",
    "    print(key, moda_page_stats[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(14, 4), dpi=120)\n",
    "for i_k, key in enumerate(moda_page_stats.keys()):\n",
    "    ax = axes[i_k]\n",
    "    ax.hist(moda_page_stats[key], bins=20)\n",
    "    ax.set_title('MODA\\n%s. Min %1.4f Max %1.4f' % (key, moda_page_stats[key].min(), moda_page_stats[key].max()), fontsize=8)\n",
    "    print(key, moda_page_stats[key].min(), moda_page_stats[key].max())\n",
    "    ax.tick_params(labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moda = load_dataset('moda_ss')\n",
    "signals = moda.get_signals(normalize_clip=False)\n",
    "marks = moda.get_stamps(pages_subset='n2')\n",
    "\n",
    "moda_page_stats = {\n",
    "    'duration': [],\n",
    "    'amplitude': [],\n",
    "}\n",
    "\n",
    "to_visualize = []\n",
    "\n",
    "for sub_signal, sub_marks in tqdm(zip(signals, marks)):\n",
    "    \n",
    "    durations = (sub_marks[:, 1] - sub_marks[:, 0]) / moda.fs\n",
    "    \n",
    "    filt_signal = param_filtering_fn(sub_signal, moda.fs, constants.SPINDLE)\n",
    "    signal_events = [filt_signal[e[0]:e[1]+1] for e in sub_marks]\n",
    "    amplitudes = np.array([param_amplitude_fn(s, moda.fs, constants.SPINDLE) for s in signal_events])\n",
    "    n_marks = durations.size\n",
    "    for i in range(n_marks):\n",
    "        if amplitudes[i] > 100:\n",
    "            # visualize it, with 1 second context\n",
    "            e = sub_marks[i]\n",
    "            e[0] -= moda.fs \n",
    "            e[1] += moda.fs\n",
    "            segment_signal = sub_signal[e[0]:e[1]+1]\n",
    "            to_visualize.append(segment_signal)\n",
    "    \n",
    "    moda_page_stats['duration'].append(durations)\n",
    "    moda_page_stats['amplitude'].append(amplitudes)\n",
    "\n",
    "for key in moda_page_stats.keys():\n",
    "    moda_page_stats[key] = np.concatenate(moda_page_stats[key])\n",
    "    print(key, moda_page_stats[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), dpi=120)\n",
    "for i_k, key in enumerate(moda_page_stats.keys()):\n",
    "    ax = axes[i_k]\n",
    "    ax.hist(moda_page_stats[key], bins=20)\n",
    "    ax.set_title('MODA\\n%s. Min %1.4f Max %1.4f' % (key, moda_page_stats[key].min(), moda_page_stats[key].max()), fontsize=8)\n",
    "    print(key, moda_page_stats[key].min(), moda_page_stats[key].max())\n",
    "    ax.tick_params(labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_to_visualize = 0\n",
    "\n",
    "segment_signal = to_visualize[which_to_visualize]\n",
    "time_axis = np.arange(segment_signal.size) / moda.fs\n",
    "mark_to_viz = np.array([moda.fs, segment_signal.size - moda.fs], dtype=np.int32)\n",
    "filt_signal = param_filtering_fn(segment_signal, moda.fs, constants.SPINDLE)\n",
    "spindle = filt_signal[mark_to_viz[0]:mark_to_viz[1]+1]\n",
    "amplitude = param_amplitude_fn(spindle, moda.fs, constants.SPINDLE)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), dpi=120)\n",
    "ax.plot(time_axis, segment_signal, linewidth=0.8)\n",
    "ax.plot(mark_to_viz / moda.fs, [-75, -75], linewidth=3, color=\"r\", alpha=0.5)\n",
    "ax.set_ylim([-100, 100])\n",
    "ax.set_title(\"amplitude %1.4f uV\" % amplitude)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSRR EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsrr_preds = fig_utils.PredictedNSRR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducción de tendencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "fold_ids_list = np.arange(n_folds)\n",
    "# fold_ids_list = [3]\n",
    "predictions = nsrr_preds.get_predictions(fold_ids_list, nsrr)\n",
    "\n",
    "print(\"Loaded predictions for %d subjects\" % (len(predictions.all_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n2_minutes = 60\n",
    "\n",
    "subject_ids = predictions.all_ids\n",
    "\n",
    "table_byevent = {\n",
    "    'subject_id': [],\n",
    "    'mark_id': [],\n",
    "    'duration': [], \n",
    "    'amplitude': [],\n",
    "    'sigma_to_beta': [],\n",
    "    'entropy': [],\n",
    "}\n",
    "table_bysubject = {\n",
    "    'subject_id': [], \n",
    "    'duration': [],\n",
    "    'amplitude': [],\n",
    "    'density': [], \n",
    "    'proba_event': [], \n",
    "    'n2_minutes': [], \n",
    "    'origin': [],\n",
    "    'age': [], \n",
    "    'female': [],\n",
    "}\n",
    "for i in range(len(subject_ids)):\n",
    "    subject_id = subject_ids[i]\n",
    "    n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "    n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "    \n",
    "    if n2_minutes < min_n2_minutes:\n",
    "        print(\"Skipped by N2 minutes: Subject %s with %d N2 minutes\" % (subject_id, n2_minutes))\n",
    "        continue\n",
    "    \n",
    "    marks = predictions.get_subject_stamps(subject_id)\n",
    "    n_marks = marks.shape[0]\n",
    "    \n",
    "    if n_marks == 0:\n",
    "        print(\"Zero Marks     : Subject %s with %d marks (%d N2 minutes)\" % (subject_id, n_marks, n2_minutes))\n",
    "        durations = np.zeros((0,), dtype=np.float32)\n",
    "        subject_proba = np.zeros((0,), dtype=np.float32)\n",
    "    else:\n",
    "        durations = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "        subject_proba = predictions.get_subject_stamps_probabilities(subject_id)\n",
    "\n",
    "    subject_mean_duration = np.mean(durations)\n",
    "    subject_density = n_marks / n2_minutes\n",
    "    subject_mean_proba = np.mean(subject_proba)\n",
    "    subdataset = subject_id[:-4]\n",
    "\n",
    "    subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "    age = float(subject_data['age'].item())\n",
    "    female = int(subject_data['sex'].item() == 'f')\n",
    "    \n",
    "    # Get signal mark stuff\n",
    "    if n_marks == 0:\n",
    "        amplitudes = np.zeros((0,), dtype=np.float32)\n",
    "        sigma_to_beta = [np.nan]\n",
    "        entropies = [np.nan]\n",
    "    else:\n",
    "        signal = subject_data['signal']\n",
    "        filt_signal = param_filtering_fn(signal, nsrr.fs, constants.SPINDLE)\n",
    "        signal_events = [filt_signal[e[0]:e[1]+1] for e in marks]\n",
    "        amplitudes = np.array([param_amplitude_fn(s, nsrr.fs, constants.SPINDLE) for s in signal_events])\n",
    "        \n",
    "        # Sigma to beta (quality factor)\n",
    "        spindles = [signal[e[0]:e[1]+1] for e in marks]\n",
    "        sigma_to_beta = []\n",
    "        entropies = []\n",
    "        for sp in spindles:\n",
    "            f, y = fig_utils.get_fft_spectrum(sp, nsrr.fs, pad_to_duration=10, f_min=1, f_max=30, apply_hann_window=False)\n",
    "            this_sigma_to_beta = np.mean(y[(f >= 11) * (f <= 16)]) / np.mean(y[(f > 16) * (f <= 30)])\n",
    "            sigma_to_beta.append(this_sigma_to_beta)\n",
    "            \n",
    "            spectrum_subset = y[(f >= 11) * (f <= 30)]\n",
    "            spectrum_subset = spectrum_subset / spectrum_subset.sum()\n",
    "            spectrum_subset = spectrum_subset.astype(np.float64)\n",
    "            entropy = np.sum(-spectrum_subset * np.log(spectrum_subset + 1e-6))\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "        \n",
    "    subject_mean_amplitude = np.mean(amplitudes)\n",
    "    \n",
    "    if n_marks == 0:\n",
    "        marks_id = [-1]\n",
    "        subject_ids_to_append = [subject_id]\n",
    "        durations = [np.nan]\n",
    "        amplitudes = [np.nan]\n",
    "    else:\n",
    "        marks_id = np.arange(durations.size)\n",
    "        subject_ids_to_append = [subject_id] * durations.size\n",
    "    \n",
    "    # Save\n",
    "    table_byevent['duration'].append(durations)\n",
    "    table_byevent['amplitude'].append(amplitudes)\n",
    "    table_byevent['subject_id'].append(subject_ids_to_append)\n",
    "    table_byevent['mark_id'].append(marks_id)\n",
    "    table_byevent['sigma_to_beta'].append(sigma_to_beta)\n",
    "    table_byevent['entropy'].append(entropies)\n",
    "    \n",
    "    table_bysubject['subject_id'].append(subject_id)\n",
    "    table_bysubject['duration'].append(subject_mean_duration)\n",
    "    table_bysubject['amplitude'].append(subject_mean_amplitude)\n",
    "    table_bysubject['density'].append(subject_density)\n",
    "    table_bysubject['proba_event'].append(subject_mean_proba)\n",
    "    table_bysubject['n2_minutes'].append(n2_minutes)\n",
    "    table_bysubject['origin'].append(subdataset)\n",
    "    table_bysubject['age'].append(age)\n",
    "    table_bysubject['female'].append(female)\n",
    "    \n",
    "for key in table_byevent:\n",
    "    table_byevent[key] = np.concatenate(table_byevent[key])\n",
    "table_byevent = pd.DataFrame.from_dict(table_byevent)\n",
    "table_bysubject = pd.DataFrame.from_dict(table_bysubject)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bysubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by-event stuff\n",
    "table_byevent.hist(bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by-subject stuff\n",
    "table_bysubject.hist(bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check tendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (table_bysubject['n2_minutes'] * table_bysubject['density'])\n",
    "a[a <= 20].hist(bins=np.arange(0, 20 + 0.001, 1))\n",
    "plt.show()\n",
    "print(a.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_density = 0.5\n",
    "\n",
    "table_zoom = table_bysubject[table_bysubject.density < zoom_density]\n",
    "table_zoom.hist()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlations\n",
    "\n",
    "param_names = table_bysubject.select_dtypes(include=np.number).columns.tolist()\n",
    "n_params = len(param_names)\n",
    "n_plots = n_params * (n_params - 1) / 2\n",
    "n_cols = 5\n",
    "n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2.5, n_rows* 2.5), dpi=100)\n",
    "axes = axes.flatten()\n",
    "global_count = -1\n",
    "for i in range(n_params):\n",
    "    for j in range(i + 1, n_params):\n",
    "        global_count += 1\n",
    "        ax = axes[global_count]\n",
    "        \n",
    "        x_data = table_bysubject[param_names[i]].values\n",
    "        y_data = table_bysubject[param_names[j]].values\n",
    "        \n",
    "        ax.plot(x_data, y_data, linestyle=\"none\", marker='o', markersize=3, alpha=0.1)\n",
    "        ax.set_xlabel(param_names[i], fontsize=8)\n",
    "        ax.set_ylabel(param_names[j], fontsize=8)\n",
    "        \n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bysubject[table_bysubject.duration > 1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check by-event anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor spindles\n",
    "subtable = table_byevent[table_byevent.sigma_to_beta < 2]\n",
    "\n",
    "unique_subjects = np.unique(subtable.subject_id)\n",
    "print(\"Unique subjects:\", len(unique_subjects))\n",
    "subtable.sort_values(by=[\"sigma_to_beta\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable.sigma_to_beta.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too long spindles\n",
    "subtable = table_byevent[table_byevent.duration > 2.95]\n",
    "\n",
    "unique_subjects = np.unique(subtable.subject_id)\n",
    "print(\"Unique subjects:\", len(unique_subjects))\n",
    "subtable.sort_values(by=[\"amplitude\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable.duration.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird amplitudes\n",
    "cutoff = 0.1\n",
    "\n",
    "bottom_thr, upper_thr = np.percentile(table_byevent.amplitude.dropna().values, (cutoff, 100 - cutoff))\n",
    "print(\"Top %s%% PP amplitudes are larger than %1.4f uV\" % (cutoff, upper_thr))\n",
    "print(\"Bottom %s%% PP amplitudes are smaller than %1.4f uV\" % (cutoff, bottom_thr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too small spindles\n",
    "subtable = table_byevent[table_byevent.amplitude < 10]\n",
    "\n",
    "unique_subjects = np.unique(subtable.subject_id)\n",
    "print(\"Unique subjects:\", len(unique_subjects))\n",
    "subtable.sort_values(by=[\"amplitude\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable.amplitude.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too large spindles\n",
    "subtable = table_byevent[table_byevent.amplitude > 200]\n",
    "\n",
    "unique_subjects = np.unique(subtable.subject_id)\n",
    "print(\"Unique subjects:\", len(unique_subjects))\n",
    "subtable.sort_values(by=[\"amplitude\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable.amplitude.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too large spindles according to MODA stats (max encountered in entire MODA dataset)\n",
    "subtable = table_byevent[table_byevent.amplitude > 134.12087769782073]\n",
    "\n",
    "unique_subjects = np.unique(subtable.subject_id)\n",
    "print(\"Unique subjects:\", len(unique_subjects))\n",
    "subtable.sort_values(by=[\"amplitude\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of spindles in this situation:\n",
    "n_outliers = len(subtable)\n",
    "n_total = len(table_byevent)\n",
    "fraction_outliers = 100 * n_outliers / n_total\n",
    "print(\"Fraction of events affected: %1.4f%%\" % fraction_outliers)\n",
    "\n",
    "subtable.hist()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent.hist()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(8, 3), dpi=200)\n",
    "table_byevent.drop(columns=[\"mark_id\"]).hist(ax=axes[0, :], bins=30)\n",
    "subtable.drop(columns=[\"mark_id\"]).hist(ax=axes[1, :], bins=10)\n",
    "\n",
    "axes[0, 0].set_xlim([0, 300])\n",
    "axes[1, 0].set_xlim([0, 300])\n",
    "\n",
    "axes[0, 1].set_xlim([0, 3])\n",
    "axes[1, 1].set_xlim([0, 3])\n",
    "\n",
    "axes[0, 2].set_xlim([3.5, 5.5])\n",
    "axes[1, 2].set_xlim([3.5, 5.5])\n",
    "\n",
    "axes[0, 3].set_xlim([0, 30])\n",
    "axes[1, 3].set_xlim([0, 30])\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "axes[0, 0].set_title(\"Amplitud PP ($\\mu$V)\", fontsize=9)\n",
    "axes[1, 0].set_title(\"Amplitud PP ($\\mu$V)\\n(anómalos)\", fontsize=9)\n",
    "\n",
    "axes[0, 1].set_title(\"Duración (s)\", fontsize=9)\n",
    "axes[1, 1].set_title(\"Duración (s)\\n(anómalos)\", fontsize=9)\n",
    "\n",
    "axes[0, 2].set_title(\"Entropía 11-30 Hz\", fontsize=9)\n",
    "axes[1, 2].set_title(\"Entropía 11-30 Hz\\n(anómalos)\", fontsize=9)\n",
    "\n",
    "axes[0, 3].set_title(\"Razón sigma:beta\", fontsize=9)\n",
    "axes[1, 3].set_title(\"Razón sigma:beta\\n(anómalos)\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable = table_byevent[table_byevent.amplitude <= 134.12087769782073]\n",
    "subtable.hist(bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check stats of all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats = {\n",
    "    'scale': [],\n",
    "    'exponent': [],\n",
    "    'max_ratio': [],\n",
    "    'standard_deviation': [],\n",
    "    'r2': [],\n",
    "}\n",
    "\n",
    "for subject_id in tqdm(predictions.all_ids):\n",
    "    subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "    sub_signal = subject_data['signal']\n",
    "    sub_pages = predictions.data[subject_id]['n2_pages']\n",
    "    \n",
    "    n2_minutes = sub_pages.size * nsrr.original_page_duration / 60\n",
    "    if n2_minutes < 60:\n",
    "        continue\n",
    "    \n",
    "    sub_signal = sub_signal.reshape(-1, nsrr.fs * nsrr.original_page_duration)[sub_pages]  # [n_pages, n_samples]\n",
    "    freq, pages_spectrum = utils.compute_pagewise_fft(sub_signal, nsrr.fs, window_duration=2)\n",
    "    pages_scales, pages_exponents = utils.compute_pagewise_powerlaw(freq, pages_spectrum)  # (n_pages,)\n",
    "    \n",
    "    # Deviation from power law\n",
    "    f_min = 2\n",
    "    f_max = 30\n",
    "    valid_locs = np.where((freq >= f_min) & (freq <= f_max))[0]\n",
    "    dev_f = freq[valid_locs]\n",
    "    dev_x = pages_spectrum[:, valid_locs]\n",
    "    dev_x_law = [fit_s * (dev_f ** fit_e) for fit_s, fit_e in zip(pages_scales, pages_exponents)]\n",
    "    dev_x_law = np.stack(dev_x_law, axis=0)\n",
    "    error = dev_x / dev_x_law  # n_pages, n_freqs\n",
    "    max_error = np.max(error, axis=1)  # to detect weird peaks, shape (n_pages,)\n",
    "    \n",
    "    # for r2, we remove sigma\n",
    "    valid_locs = np.where((dev_f < 10) | (dev_f > 17))[0]\n",
    "    dev_f = dev_f[valid_locs]\n",
    "    log_dev_x = np.log(dev_x[:, valid_locs])\n",
    "    log_dev_x_law = np.log(dev_x_law[:, valid_locs])\n",
    "    squared_data = np.sum((log_dev_x - log_dev_x.mean(axis=1).reshape(-1, 1)) ** 2, axis=1)\n",
    "    squared_residuals = np.sum((log_dev_x - log_dev_x_law) ** 2, axis=1)\n",
    "    r2 = 1 - squared_residuals / squared_data  # (n_pages,)\n",
    "    \n",
    "    page_stats['scale'].append(pages_scales)\n",
    "    page_stats['exponent'].append(pages_exponents)\n",
    "    page_stats['max_ratio'].append(max_error)\n",
    "    page_stats['standard_deviation'].append(sub_signal.std(axis=1))\n",
    "    page_stats['r2'].append(r2)\n",
    "\n",
    "for key in page_stats.keys():\n",
    "    page_stats[key] = np.concatenate(page_stats[key])\n",
    "    print(key, page_stats[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(14, 4), dpi=120)\n",
    "for i_k, key in enumerate(page_stats.keys()):\n",
    "    ax = axes[i_k]\n",
    "    \n",
    "    value_vector = page_stats[key]\n",
    "    moda_value_vector = moda_page_stats[key]\n",
    "    \n",
    "    if key in ['scale', 'max_ratio', 'standard_deviation']:\n",
    "        value_vector = np.log(value_vector)\n",
    "        moda_value_vector = np.log(moda_value_vector)\n",
    "        name_str = 'log %s' % key\n",
    "    else:\n",
    "        name_str = key\n",
    "        \n",
    "    moda_min = moda_value_vector.min()\n",
    "    moda_max = moda_value_vector.max()\n",
    "    \n",
    "    # Count number of pages falling outside MODA ranges\n",
    "    n_inliers = np.sum((value_vector >= moda_min) * (value_vector <= moda_max))\n",
    "    n_total = value_vector.size\n",
    "    fraction_inliers = 100 * n_inliers / n_total\n",
    "    \n",
    "    ax.hist(value_vector, bins=50)\n",
    "    ax.set_title('NSRR\\n%s\\nMin %1.4f Max %1.4f\\n(MODA Min %1.4f Max %1.4f)\\nInliers: %1.2f%%' % (\n",
    "        name_str, value_vector.min(), value_vector.max(), moda_min, moda_max, fraction_inliers), fontsize=8)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.axvline(moda_min, color=\"k\", linestyle=\"--\", linewidth=0.8)\n",
    "    ax.axvline(moda_max, color=\"k\", linestyle=\"--\", linewidth=0.8)\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = []\n",
    "all_std = []\n",
    "large_subjects = []\n",
    "origin_subject_id = []\n",
    "origin_page_id = []\n",
    "for subject_id in predictions.all_ids:\n",
    "    signal = nsrr.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "    n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "    \n",
    "    if n2_minutes < 60:\n",
    "        continue\n",
    "    \n",
    "    x_pages = signal.reshape(-1, nsrr.original_page_duration * nsrr.fs)[n2_pages]\n",
    "    for i_n2, x in enumerate(x_pages):\n",
    "        freq, power = utils.power_spectrum_by_sliding_window(x, nsrr.fs, window_duration=2)\n",
    "        spectra.append(power)\n",
    "        this_std = x.std()\n",
    "        all_std.append(this_std)\n",
    "        origin_subject_id.append(subject_id)\n",
    "        origin_page_id.append(n2_pages[i_n2])\n",
    "        if this_std > 50:\n",
    "            large_subjects.append(subject_id)\n",
    "spectra = np.stack(spectra, axis=0)\n",
    "all_std = np.array(all_std)\n",
    "large_subjects = np.unique(large_subjects)\n",
    "origin_subject_id = np.array(origin_subject_id)\n",
    "origin_page_id = np.array(origin_page_id)\n",
    "print(\"Done\")\n",
    "print(spectra.shape, all_std.shape, large_subjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std hist\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), dpi=100)\n",
    "ax.hist(all_std, bins=np.arange(0, 130 + 0.001, 2.5))\n",
    "ax.set_xticks(np.arange(0, 130 + 0.001, 10))\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"STD -- Min %1.4f, Mean %1.4f, Median %1.4f, Max %1.4f\" % (all_std.min(), all_std.mean(), np.median(all_std), all_std.max()))\n",
    "\n",
    "# Central 95% of pages\n",
    "# STD between 7.5 and 36.\n",
    "# Central 99% of pages\n",
    "# STD between 6.15 and 73.34\n",
    "\n",
    "prctl = 0.5\n",
    "print(\"STD -- Percentile %s: %1.4f\" % (prctl, np.percentile(all_std, prctl)))\n",
    "\n",
    "prctl = 99.5\n",
    "print(\"STD -- Percentile %s: %1.4f\" % (prctl, np.percentile(all_std, prctl)))\n",
    "print(np.sum(all_std > 50), all_std.size, 100 * np.sum(all_std > 50)/all_std.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_locs = np.where((freq >= 0.3) & (freq <= 30))[0]\n",
    "freq_short = freq[valid_locs]\n",
    "spectra_short = spectra[:, valid_locs]\n",
    "\n",
    "# Compute power law\n",
    "locs_to_use = np.where(freq_short >= 4)[0]\n",
    "x_data = freq_short[locs_to_use]\n",
    "y_data = spectra_short[350, locs_to_use]\n",
    "\n",
    "locs_no_sigma = np.where((x_data < 10) | (x_data > 17))[0]\n",
    "x_data_no_sigma = x_data[locs_no_sigma]\n",
    "y_data_no_sigma = y_data[locs_no_sigma]\n",
    "\n",
    "log_x = np.log(x_data_no_sigma)\n",
    "log_y = np.log(y_data_no_sigma)\n",
    "pl_exponent, pl_intercept, _, _, _ = scipy.stats.linregress(log_x,log_y)\n",
    "def fitted_power_law(x):\n",
    "    return (x ** pl_exponent) * np.exp(pl_intercept)\n",
    "print(pl_exponent, pl_intercept)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=140)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x_data, y_data, linewidth=0.8)\n",
    "ax.plot(x_data_no_sigma, y_data_no_sigma, linewidth=0.8)\n",
    "ax.plot(x_data, fitted_power_law(x_data), linewidth=0.8)\n",
    "ax.set_xlim([3, 35])\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x_data, y_data, linewidth=0.8)\n",
    "ax.plot(x_data_no_sigma, y_data_no_sigma, linewidth=0.8)\n",
    "ax.plot(x_data, fitted_power_law(x_data), linewidth=0.8)\n",
    "ax.set_xlim([3, 35])\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_list = []\n",
    "for curve in spectra_short:\n",
    "    #Compute power law\n",
    "    locs_to_use = np.where(freq_short >= 4)[0]\n",
    "    x_data = freq_short[locs_to_use]\n",
    "    y_data = curve[locs_to_use]\n",
    "\n",
    "    locs_no_sigma = np.where((x_data < 10) | (x_data > 17))[0]\n",
    "    x_data_no_sigma = x_data[locs_no_sigma]\n",
    "    y_data_no_sigma = y_data[locs_no_sigma]\n",
    "    # Therefore we are considering frequencies 4-10 and 17-30 Hz for the power law.\n",
    "\n",
    "    log_x = np.log(x_data_no_sigma)\n",
    "    log_y = np.log(y_data_no_sigma)\n",
    "    pl_exponent, pl_intercept, _, _, _ = scipy.stats.linregress(log_x,log_y)\n",
    "    exponent_list.append(pl_exponent)\n",
    "exponent_list = np.array(exponent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = np.median(exponent_list)\n",
    "disp_val = scipy.stats.median_absolute_deviation(exponent_list)  # dispersion around median\n",
    "disp_width = 2.2\n",
    "\n",
    "lower_bound = mean_val - disp_width * disp_val\n",
    "upper_bound = mean_val + disp_width * disp_val\n",
    "\n",
    "n_inliers = np.sum((exponent_list >= lower_bound) * (exponent_list <= upper_bound))\n",
    "print(n_inliers, 100 * n_inliers / exponent_list.size)\n",
    "print(lower_bound, upper_bound)\n",
    "print(\"\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=120)\n",
    "ax.hist(exponent_list, bins=50)\n",
    "ax.axvline(mean_val, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "ax.axvline(upper_bound, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "ax.axvline(lower_bound, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "ax.set_title(\"Mean %1.4f\" % mean_val)\n",
    "plt.show()\n",
    "\n",
    "for prct in [0, 0.5, 5, 95, 99.5, 100]:\n",
    "    value = np.percentile(exponent_list, prct)\n",
    "    print(\"Percentile %s is %1.4f\" % (prct, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_locs = np.where((freq >= 0.3) & (freq <= 30))[0]\n",
    "freq_short = freq[valid_locs]\n",
    "spectra_short = spectra[:, valid_locs]\n",
    "\n",
    "# Compute avg power of 2Hz freq bands\n",
    "all_p_avg = []\n",
    "all_f_avg = []\n",
    "start_freq = 0\n",
    "width = 2\n",
    "n_bands = int(np.ceil((freq_short.max() - start_freq) / width))\n",
    "for i_band in range(n_bands):\n",
    "    end_freq = start_freq + width\n",
    "    locs = np.where((f >= start_freq) & (f < end_freq))[0]\n",
    "    center_freq = (start_freq + end_freq)/2\n",
    "    start_freq = end_freq\n",
    "    if center_freq < 2:\n",
    "        continue\n",
    "    this_power = spectra_short[:, locs].mean(axis=1)\n",
    "    all_p_avg.append(this_power)\n",
    "    all_f_avg.append(center_freq)\n",
    "    \n",
    "all_f_avg = np.array(all_f_avg)\n",
    "all_p_avg = np.stack(all_p_avg, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=200)\n",
    "\n",
    "ax = axes[0]\n",
    "prctl_results = np.percentile(all_p_avg, (0, 0.5, 25, 50, 75, 99.5, 100), axis=0)\n",
    "for curve in prctl_results:\n",
    "    ax.plot(all_f_avg, curve, linewidth=0.8, marker='o')\n",
    "ax.set_xlim([0.1, 35])\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "for curve in prctl_results:\n",
    "    ax.plot(all_f_avg, curve, linewidth=0.8, marker='o')\n",
    "ax.set_xlim([0.1, 35])\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_locs = np.where((freq >= 0.3) & (freq <= 30))[0]\n",
    "freq_short = freq[valid_locs]\n",
    "spectra_short = spectra[:, valid_locs]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=200)\n",
    "\n",
    "prctl_results = np.percentile(all_p_avg, (0.5, 99.5), axis=0)\n",
    "\n",
    "ax = axes[0]\n",
    "for curve in prctl_results:\n",
    "    ax.plot(all_f_avg, curve, linewidth=0.8, marker='o', markersize=3, color=\"b\")\n",
    "for curve in spectra_short[outliers_locs]:\n",
    "    ax.plot(freq_short, curve, linewidth=0.8, color=\"k\", alpha=0.1)\n",
    "ax.set_xlim([0.1, 35])\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "for curve in prctl_results:\n",
    "    ax.plot(all_f_avg, curve, linewidth=0.8, marker='o', markersize=3, color=\"b\")\n",
    "for curve in spectra_short[outliers_locs]:\n",
    "    ax.plot(freq_short, curve, linewidth=0.8, color=\"k\", alpha=0.1)\n",
    "ax.set_xlim([0.1, 35])\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 350\n",
    "\n",
    "p = spectra_short[loc]\n",
    "f = freq_short\n",
    "# Compute avg power of 2Hz freq bands\n",
    "p_avg = []\n",
    "f_avg = []\n",
    "start_freq = 0\n",
    "width = 2\n",
    "n_bands = int(np.ceil((f.max() - start_freq) / width))\n",
    "for i_band in range(n_bands):\n",
    "    end_freq = start_freq + width\n",
    "    locs = np.where((f >= start_freq) & (f < end_freq))[0]\n",
    "    f_avg.append((start_freq + end_freq)/2)\n",
    "    p_avg.append(p[locs].mean())\n",
    "    start_freq = end_freq\n",
    "f_avg = np.array(f_avg)\n",
    "p_avg = np.array(p_avg)\n",
    "\n",
    "plt.plot(f, p)\n",
    "plt.plot(f_avg, p_avg, marker='o')\n",
    "print(f_avg)\n",
    "plt.title(\"Subject %s, page %d\" % (origin_subject_id[loc], origin_page_id[loc]))\n",
    "plt.show()\n",
    "print(\"Subject %s, page %d\" % (origin_subject_id[loc], origin_page_id[loc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = np.percentile(all_p_avg, (0.5, 99.5), axis=0)\n",
    "print(\"Original: \", all_p_avg.shape)\n",
    "spectra_filt = []\n",
    "inliers_locs = []\n",
    "outliers_locs = []\n",
    "for i_page, spec_page in enumerate(all_p_avg):\n",
    "    if np.all(spec_page >= lower_bound) and np.all(spec_page <= upper_bound):\n",
    "        spectra_filt.append(spec_page)\n",
    "        inliers_locs.append(i_page)\n",
    "    else:\n",
    "        outliers_locs.append(i_page)\n",
    "spectra_filt = np.stack(spectra_filt, axis=0)\n",
    "print(\"Filtered:\", spectra_filt.shape)\n",
    "print(\"Percentage kept: %1.4f\" % (100 * spectra_filt.shape[0] / all_p_avg.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'shhs1-202398'  # 'shhs1-201480' #  'shhs1-200721'   # 'shhs1-201711'  # 'sof-visit-8-02332'\n",
    "\n",
    "print(\"subject %s\" % subject_id)\n",
    "subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "signal = subject_data['signal']\n",
    "n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "marks = predictions.get_subject_stamps(subject_id, pages_subset='n2')\n",
    "proba = predictions.get_subject_probabilities(subject_id)\n",
    "proba_up = np.repeat(proba, 8)\n",
    "durations = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "\n",
    "print(\"Marks\", marks.shape)\n",
    "print(\"N2 minutes\", n2_minutes)\n",
    "\n",
    "plt.plot(signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3221848/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_selected = None  # marks[17]\n",
    "\n",
    "center_sample = int((536 + 0.5) * 30 * 200) # mark_selected.mean()  # int((199 + 0.5) * 30 * 200) # mark_selected.mean()\n",
    "\n",
    "window_duration = 30\n",
    "window_size = nsrr.fs * window_duration\n",
    "start_sample = int(center_sample - window_size // 2)\n",
    "end_sample = int(start_sample + window_size)\n",
    "time_axis = np.arange(start_sample, end_sample) / nsrr.fs\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2.5), dpi=140)\n",
    "\n",
    "ax.plot(time_axis, signal[start_sample:end_sample], linewidth=.6)\n",
    "\n",
    "if mark_selected is not None:\n",
    "    ax.plot(mark_selected / nsrr.fs, [-100, -100], linewidth=4, color=viz.PALETTE['red'], alpha=0.7)\n",
    "\n",
    "ax.fill_between(\n",
    "    time_axis, \n",
    "    -300 - 50 * proba_up[start_sample:end_sample], \n",
    "    -300 + 50 * proba_up[start_sample:end_sample],\n",
    "    color=viz.PALETTE['red'], alpha=1.0\n",
    ")\n",
    "ax.axhline(-300 - 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 + 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 - 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 0, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "\n",
    "ax.set_ylim([-400, 200])\n",
    "ax.set_xlim([start_sample/nsrr.fs, end_sample/nsrr.fs])\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "ax.tick_params(labelsize=8)\n",
    "\n",
    "page_std = signal[start_sample:end_sample].std()\n",
    "\n",
    "ax.set_title(\"Subject %s (Age %1.4f, Sex %s). Page STD %1.4f\" % (\n",
    "    subject_id, subject_data['age'], subject_data['sex'], page_std\n",
    "))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Page STD: %1.4f\" % page_std)\n",
    "\n",
    "# Compute spectrum of page\n",
    "freq, power = utils.power_spectrum_by_sliding_window(signal[start_sample:end_sample], nsrr.fs, window_duration=2)\n",
    "valid_locs = np.where((freq >= 0.3) & (freq <= 30))[0]\n",
    "freq = freq[valid_locs]\n",
    "power = power[valid_locs]\n",
    "\n",
    "locs_to_use = np.where(freq >= 2)[0]\n",
    "x_data = freq[locs_to_use]\n",
    "y_data = power[locs_to_use]\n",
    "locs_no_sigma = np.where((x_data < 10) | (x_data > 17))[0]\n",
    "x_data_no_sigma = x_data[locs_no_sigma]\n",
    "y_data_no_sigma = y_data[locs_no_sigma]\n",
    "log_x = np.log(x_data_no_sigma)\n",
    "log_y = np.log(y_data_no_sigma)\n",
    "pl_exponent, pl_intercept, _, _, _ = scipy.stats.linregress(log_x,log_y)\n",
    "def fitted_power_law(x):\n",
    "    return (x ** pl_exponent) * np.exp(pl_intercept)\n",
    "print(pl_exponent, pl_intercept)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 2), dpi=100)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(freq, power)\n",
    "ax.set_xlim([0.1, 30])\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(freq, power)\n",
    "ax.plot(freq, fitted_power_law(freq))\n",
    "ax.set_xlim([0.1, 30])\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Frequency (Hz)\", fontsize=8)\n",
    "ax.set_title(\"Exponent %1.4f\" % (pl_exponent), fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if mark_selected is not None:\n",
    "    # Spindle spectral quality\n",
    "    spindle = signal[mark_selected[0]:mark_selected[1]+1]\n",
    "    f, y = fig_utils.get_fft_spectrum(spindle, nsrr.fs, pad_to_duration=10, f_min=1, f_max=30, apply_hann_window=False)\n",
    "    sigma_to_beta = np.mean(y[(f >= 11) * (f <= 16)]) / np.mean(y[(f >= 16) * (f <= 30)])\n",
    "    print(\"Sigma to beta\", sigma_to_beta)\n",
    "\n",
    "    spectrum_subset = y[(f >= 11) * (f <= 30)]\n",
    "    spectrum_subset = spectrum_subset / spectrum_subset.sum()\n",
    "    entropy = np.sum(-spectrum_subset * np.log(spectrum_subset + 1e-6))\n",
    "    print(\"Entropy:\", entropy)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3), dpi=80)\n",
    "    axes[0].plot(np.arange(spindle.size) / nsrr.fs, spindle)\n",
    "    axes[0].set_ylim([-100, 100])\n",
    "    axes[1].plot(f, y)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "diff_l = []\n",
    "for subject_id in predictions.all_ids:\n",
    "    n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "    if n2_pages.size == 0:\n",
    "        continue\n",
    "    subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "    signal = subject_data['signal']\n",
    "    last_n2 = n2_pages.max()\n",
    "    last_page = signal.size / (nsrr.original_page_duration * nsrr.fs) - 1\n",
    "    diff = last_page - last_n2\n",
    "    diff_l.append(diff)\n",
    "diff_l = np.array(diff_l)\n",
    "print(\"Min diff is\", diff_l.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
