{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants, viz, pkeys\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import det_utils\n",
    "from figs_thesis import fig_utils\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event\n",
    "\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')\n",
    "LETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(parts_to_load, dataset):\n",
    "    pred_objects = []\n",
    "    for part in parts_to_load:\n",
    "        filepath = os.path.join(\n",
    "            RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "            'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "            'v2_time',\n",
    "            'prediction_part%d.pkl' % part\n",
    "        )\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            pred_object = pickle.load(handle)\n",
    "        pred_object.set_parent_dataset(dataset)\n",
    "        pred_objects.append(pred_object)\n",
    "    return pred_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NSRR dataset and pre-computed predicted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_to_load = [0]  # 0 to 11\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True, params={pkeys.PAGE_DURATION: 30})\n",
    "pred_objects = load_predictions(parts_to_load, nsrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dataset de parámetros</h1>\n",
    "\n",
    "Parámetros no definidos son NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames of dataset checkpoints\n",
    "byevent_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_byevent.csv'\n",
    ")\n",
    "bysubject_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_bysubject.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_load_checkpoint = True\n",
    "\n",
    "# ############################\n",
    "\n",
    "if params_load_checkpoint:\n",
    "    print(\"Loading from checkpoint\")\n",
    "    table_byevent = pd.read_csv(byevent_ckpt_path)\n",
    "    table_bysubject = pd.read_csv(bysubject_ckpt_path)\n",
    "\n",
    "else:\n",
    "    # Perform computation and save checkpoint\n",
    "    table_byevent = {\n",
    "        'subject_id': [],\n",
    "        'mark_id': [],\n",
    "        'duration': [], \n",
    "        'amplitude': [],\n",
    "        'frequency': [],\n",
    "    }\n",
    "    table_bysubject = {\n",
    "        'subject_id': [],\n",
    "        'age': [], \n",
    "        'female': [],\n",
    "        'n2_minutes': [],\n",
    "        'n2_abs_sigma_power': [],\n",
    "        'n2_rel_sigma_power': [],\n",
    "        'n2_pl_exponent': [],\n",
    "        'density': [],\n",
    "        'duration': [], \n",
    "        'amplitude': [],\n",
    "        'frequency': [],\n",
    "    }\n",
    "\n",
    "    min_n2_minutes = 60\n",
    "    verbose_min_minutes = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Generating table of parameters\")\n",
    "    for part_id, predictions in enumerate(pred_objects):\n",
    "        print(\"Processing Part %d / %d\" % (part_id + 1, len(pred_objects)))\n",
    "        for subject_id in tqdm(predictions.all_ids):\n",
    "            n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "            n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "            if n2_minutes < min_n2_minutes:\n",
    "                if verbose_min_minutes:\n",
    "                    print(\"Skipped by N2 minutes: Subject %s with %d N2 minutes\" % (subject_id, n2_minutes))\n",
    "                continue\n",
    "\n",
    "            # Now compute parameters\n",
    "            subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "            signal = subject_data['signal']\n",
    "            age = float(subject_data['age'].item())\n",
    "            female = int(subject_data['sex'].item() == 'f')\n",
    "\n",
    "            # By-subject spectral parameters\n",
    "            signal_n2 = signal.reshape(-1, nsrr.fs * nsrr.original_page_duration)[n2_pages].flatten()\n",
    "            freq, power = utils.power_spectrum_by_sliding_window(signal_n2, nsrr.fs, window_duration=5)\n",
    "            # a) Absolute sigma power \n",
    "            n2_abs_sigma_power = power[(freq >= 11) & (freq <= 16)].mean()\n",
    "            # b) Relative sigma power (as in Lacourse)\n",
    "            n2_broad_power = power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "            n2_rel_sigma_power = n2_abs_sigma_power / n2_broad_power\n",
    "            # c) Power law exponent 2-30 Hz without sigma band (same as exclusion process)\n",
    "            locs_to_use = np.where(((freq >= 2) & (freq < 10)) | ((freq > 17) & (freq <= 30)))[0]\n",
    "            log_x = np.log(freq[locs_to_use])\n",
    "            log_y = np.log(power[locs_to_use])\n",
    "            n2_pl_exponent, _, _, _, _ = scipy.stats.linregress(log_x, log_y)\n",
    "\n",
    "            # Spindle parameters\n",
    "            marks = predictions.get_subject_stamps(subject_id, pages_subset='n2')\n",
    "            n_marks = marks.shape[0]\n",
    "            density = n_marks / n2_minutes  # epm\n",
    "            if n_marks == 0:\n",
    "                # Dummy entries\n",
    "                be_subject_id = [subject_id]\n",
    "                be_mark_id = [np.nan]\n",
    "                be_duration = [np.nan]\n",
    "                be_amplitude = [np.nan]\n",
    "                be_frequency = [np.nan]\n",
    "            else:\n",
    "                be_subject_id = [subject_id] * n_marks\n",
    "                be_mark_id = np.arange(n_marks)\n",
    "\n",
    "                be_duration = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "\n",
    "                filt_signal = param_filtering_fn(signal, nsrr.fs, constants.SPINDLE)\n",
    "                signal_events = [filt_signal[e[0]:(e[1]+1)] for e in marks]\n",
    "\n",
    "                be_amplitude = np.array([\n",
    "                    param_amplitude_fn(s, nsrr.fs, constants.SPINDLE) for s in signal_events\n",
    "                ])\n",
    "\n",
    "                be_frequency = np.array([\n",
    "                    param_frequency_fn(s, nsrr.fs) for s in signal_events\n",
    "                ])\n",
    "\n",
    "            # By-subject averages\n",
    "            bs_duration = np.mean(be_duration)\n",
    "            bs_amplitude = np.mean(be_amplitude)\n",
    "            bs_frequency = np.mean(be_frequency)\n",
    "\n",
    "            # Save parameters\n",
    "            table_byevent['subject_id'].append(be_subject_id)\n",
    "            table_byevent['mark_id'].append(be_mark_id)\n",
    "            table_byevent['duration'].append(be_duration)\n",
    "            table_byevent['amplitude'].append(be_amplitude)\n",
    "            table_byevent['frequency'].append(be_frequency)\n",
    "\n",
    "            table_bysubject['subject_id'].append(subject_id)\n",
    "            table_bysubject['age'].append(age)\n",
    "            table_bysubject['female'].append(female)\n",
    "            table_bysubject['n2_minutes'].append(n2_minutes)\n",
    "            table_bysubject['n2_abs_sigma_power'].append(n2_abs_sigma_power)\n",
    "            table_bysubject['n2_rel_sigma_power'].append(n2_rel_sigma_power)\n",
    "            table_bysubject['n2_pl_exponent'].append(n2_pl_exponent)\n",
    "            table_bysubject['density'].append(density)\n",
    "            table_bysubject['duration'].append(bs_duration)\n",
    "            table_bysubject['amplitude'].append(bs_amplitude)\n",
    "            table_bysubject['frequency'].append(bs_frequency) \n",
    "    end_time = time.time()\n",
    "    et_time = (end_time - start_time) / 60  # minutes\n",
    "    print(\"Elapsed time: %1.4f minutes\" % et_time)\n",
    "    \n",
    "    for key in table_byevent:\n",
    "        table_byevent[key] = np.concatenate(table_byevent[key])\n",
    "    table_byevent = pd.DataFrame.from_dict(table_byevent)\n",
    "    table_bysubject = pd.DataFrame.from_dict(table_bysubject)\n",
    "   \n",
    "    # Save checkpoint\n",
    "    print(\"Saving checkpoint\")\n",
    "    table_byevent.to_csv(byevent_ckpt_path, index=False)\n",
    "    table_bysubject.to_csv(bysubject_ckpt_path, index=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_byevent.shape)\n",
    "table_byevent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_bysubject.shape)\n",
    "table_bysubject.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final NSRR composition statistics\n",
    "\n",
    "Measure number of subjects and N2 hours of each subdataset (consider also zero marks subjects because this table should be independent from the detector). The idea is to say: the dataset was reduced from X subjects/hours to Y subjects/hours. On this dataset, there are Z total spindle detections to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bysubject['origin'] = [s.split(\"-\")[0] for s in table_bysubject.subject_id.values]\n",
    "table_bysubject.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_stats = table_bysubject[[\"subject_id\", \"origin\"]].groupby(by=\"origin\").count()\n",
    "origin_stats['hours'] = table_bysubject[[\"n2_minutes\", \"origin\"]].groupby(by=\"origin\").sum() / 60  # hours\n",
    "origin_stats['min_age'] = table_bysubject[[\"age\", \"origin\"]].groupby(by=\"origin\").min()\n",
    "origin_stats['max_age'] = table_bysubject[[\"age\", \"origin\"]].groupby(by=\"origin\").max()\n",
    "origin_stats['female'] = table_bysubject[[\"female\", \"origin\"]].groupby(by=\"origin\").mean() * 100\n",
    "print(origin_stats)\n",
    "print(\"\")\n",
    "print(\"Total:\")\n",
    "print(\"Number of subjects:\", len(table_bysubject))\n",
    "print(\"Number of hours: %1.6f\" % (table_bysubject[\"n2_minutes\"].sum() / 60))\n",
    "# print(origin_stats[[\"subject_id\", \"hours\"]].sum())\n",
    "print(\"Age:\", table_bysubject[\"age\"].min(), table_bysubject[\"age\"].max())\n",
    "print(\"Female:\", 100 * table_bysubject[\"female\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subjects without detections:\")\n",
    "table_bysubject[table_bysubject['density'] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_detections = 10\n",
    "\n",
    "event_counts = np.round(table_bysubject[\"n2_minutes\"] * table_bysubject[\"density\"])\n",
    "subjects_with_few_detections = np.sum(event_counts < min_detections)\n",
    "print(\"Subjects with less than %d detections: %d (%1.4f%%)\" % (\n",
    "    min_detections, subjects_with_few_detections, 100 * subjects_with_few_detections / len(table_bysubject)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos por evento (sin agrupar por demografía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "\n",
    "table_byevent_valid = table_byevent.dropna()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(8, 2), dpi=200)\n",
    "\n",
    "show_average = False\n",
    "\n",
    "factor_to_lim = 1.35 if show_average else 1.1\n",
    "\n",
    "ax = axes[0]\n",
    "n, _, _ = ax.hist(table_byevent_valid.duration.values, bins=np.arange(0.0, 3.0 + 0.001, 0.08), color=viz.PALETTE['blue'])\n",
    "max_count = np.max(n)\n",
    "ax.set_ylim([0, factor_to_lim * max_count])\n",
    "ax.set_xlabel(\"Duración (s)\", fontsize=8)\n",
    "mean_value = np.mean(table_byevent_valid.duration.values)\n",
    "if show_average:\n",
    "    ax.axvline(mean_value, linewidth=0.8, color=\"k\", linestyle=\"--\", label=\"Promedio %1.2f s\" % mean_value)\n",
    "ax.set_xticks(np.arange(0, 3 + 0.001, 0.5))\n",
    "ax.set_xlim([0, 3])\n",
    "ax.set_ylabel(\"Número de husos\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "n, _, _ = ax.hist(table_byevent_valid.amplitude.values, bins=np.arange(0, 135 + 0.001, 5), color=viz.PALETTE['blue'])\n",
    "max_count = np.max(n)\n",
    "ax.set_ylim([0, factor_to_lim * max_count])\n",
    "ax.set_xlabel(\"Amplitud PP ($\\mu$V)\", fontsize=8)\n",
    "mean_value = np.mean(table_byevent_valid.amplitude.values)\n",
    "if show_average:\n",
    "    ax.axvline(mean_value, linewidth=0.8, color=\"k\", linestyle=\"--\", label=\"Promedio %1.1f $\\mu$V\" % mean_value)\n",
    "ax.set_xticks(np.arange(0, 140 + 0.001, 20))\n",
    "ax.set_xlim([0, 140])\n",
    "ax.set_ylabel(\"Número de husos\", fontsize=8)\n",
    "\n",
    "ax = axes[2]\n",
    "n, _, _ = ax.hist(table_byevent_valid.frequency.values, bins=np.arange(8.5, 17.5 + 0.001, 0.5), color=viz.PALETTE['blue'])\n",
    "max_count = np.max(n)\n",
    "ax.set_ylim([0, factor_to_lim * max_count])\n",
    "ax.set_xlabel(\"Frecuencia (Hz)\", fontsize=8)\n",
    "mean_value = np.mean(table_byevent_valid.frequency.values)\n",
    "if show_average:\n",
    "    ax.axvline(mean_value, linewidth=0.8, color=\"k\", linestyle=\"--\", label=\"Promedio %1.1f Hz\" % mean_value)\n",
    "ax.set_xticks(np.arange(8, 18 + 0.001, 1))\n",
    "ax.set_xlim([8, 18])\n",
    "ax.set_ylabel(\"Número de husos\", fontsize=8)\n",
    "\n",
    "# oscilaciones\n",
    "#ax = axes[3]\n",
    "#oscils = table_byevent_valid.frequency.values * table_byevent_valid.duration.values\n",
    "#n, _, _ = ax.hist(oscils, color=viz.PALETTE['blue'], bins=np.arange(0, 50 + 0.001, 2.5))\n",
    "#max_count = np.max(n)\n",
    "#ax.set_ylim([0, factor_to_lim * max_count])\n",
    "#ax.set_xlabel(\"Oscilaciones\", fontsize=8)\n",
    "#mean_value = np.mean(oscils)\n",
    "#ax.axvline(mean_value, linewidth=0.8, color=\"k\", linestyle=\"--\", label=\"Promedio %1.1f\" % mean_value)\n",
    "#ax.set_xticks(np.arange(0, 50 + 0.001, 5))\n",
    "#ax.set_xlim([0, 50])\n",
    "\n",
    "# Densidad \n",
    "ax = axes[3]\n",
    "n, _, _ = ax.hist(table_bysubject.density.values, color=viz.PALETTE['blue'], bins=np.arange(0, 8 + 0.001, 0.5))\n",
    "max_count = np.max(n)\n",
    "ax.set_ylim([0, factor_to_lim * max_count])\n",
    "ax.set_xlabel(\"Densidad (epm)\", fontsize=8)\n",
    "mean_value = np.mean(table_bysubject.density.values)\n",
    "if show_average:\n",
    "    ax.axvline(mean_value, linewidth=0.8, color=\"k\", linestyle=\"--\", label=\"Promedio %1.1f epm\" % mean_value)\n",
    "ax.set_xticks(np.arange(0, 8 + 0.001, 1))\n",
    "ax.set_xlim([0, 8])\n",
    "ax.set_ylabel(\"Número de sujetos\", fontsize=8)\n",
    "\n",
    "\n",
    "for i_ax, ax in enumerate(axes.flatten()):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_yticks([])\n",
    "    ax.grid()\n",
    "    if show_average:\n",
    "        ax.legend(loc=\"upper right\", fontsize=8, frameon=True)\n",
    "    ax.text(\n",
    "        x=-0.01, y=1.05, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_hist\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos de densidad vs potencia en banda sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "density_thr_to_print = 0.1\n",
    "n_below = np.sum(table_bysubject.density < density_thr_to_print)\n",
    "n_total = len(table_bysubject)\n",
    "print(\"There are %d subjects (%1.1f %%) with density less than %1.2f epm\" % (n_below, 100 * n_below / n_total, density_thr_to_print))\n",
    "\n",
    "\n",
    "df = table_bysubject.copy()\n",
    "df['density_bin'] = pd.cut(table_bysubject.density, bins=[0, 0.1, 0.5, 1.0, 1.5, 2, 3, 4, 5, 6, 8])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3), dpi=200)\n",
    "df.boxplot(\n",
    "    column=[\"n2_abs_sigma_power\", \"n2_rel_sigma_power\"],\n",
    "    by=\"density_bin\", \n",
    "    ax=axes, fontsize=8, rot=90,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    boxprops=dict(color=viz.PALETTE['blue']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['blue']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    ")\n",
    "\n",
    "fig.suptitle('')\n",
    "for i_ax, ax in enumerate(axes):\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel(\"Densidad (epm)\", fontsize=8)\n",
    "    ax.text(\n",
    "        x=-0.07, y=1.05, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "axes[0].set_title(\"Potencia sigma absoluta en etapa N2\", fontsize=8, loc=\"left\")\n",
    "axes[0].set_ylim([0, 0.7])\n",
    "axes[1].set_title(\"Potencia sigma relativa en etapa N2\", fontsize=8, loc=\"left\")\n",
    "axes[1].set_ylim([0.6, 2.0])\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_density_power\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos by-subject con suficientes detecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "# Demographics of subjects with enough detections\n",
    "min_detections = 10\n",
    "event_counts = np.round(table_bysubject[\"n2_minutes\"] * table_bysubject[\"density\"])\n",
    "enough_dets = table_bysubject[event_counts >= min_detections]\n",
    "\n",
    "# Those with less than 5 years are three subjects with 4.5, 4.9 and 4.9 y.o.. For statistics purposes those\n",
    "# subjects will be within the bin containing 5 years old.\n",
    "enough_dets[\"age\"] = np.clip(enough_dets[\"age\"], a_min=5, a_max=None)\n",
    "\n",
    "ages_m = enough_dets[enough_dets.female == 0][\"age\"].values\n",
    "ages_f = enough_dets[enough_dets.female == 1][\"age\"].values\n",
    "\n",
    "bins = np.arange(5, 90 + 0.001, 5)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 2.5), dpi=200, sharey=True)\n",
    "\n",
    "ax = axes[0]\n",
    "n, _, _ = ax.hist(ages_m, bins=bins, color=viz.PALETTE['blue'])\n",
    "ax.set_title(\"Hombres (N = %d)\" % len(ages_m), fontsize=8, loc=\"left\")\n",
    "print(n)\n",
    "\n",
    "ax = axes[1]\n",
    "n, _, _ = ax.hist(ages_f, bins=bins, color=viz.PALETTE['blue'])\n",
    "ax.set_title(\"Mujeres (N = %d)\" % len(ages_f), fontsize=8, loc=\"left\")\n",
    "print(n)\n",
    "\n",
    "for i_ax, ax in enumerate(axes):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel(\"Edad (años)\", fontsize=8)\n",
    "    ax.set_xticks(bins)\n",
    "    ax.grid()\n",
    "    ax.set_xlim([bins[0], bins[-1]])\n",
    "    ax.text(\n",
    "        x=-0.07, y=1.02, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_composition\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by-subject params, age differences\n",
    "save_figure = True\n",
    "\n",
    "df = enough_dets.copy()\n",
    "df['age_bin'] = pd.cut(enough_dets.age, bins=np.arange(5, 90 + 0.001, 5).astype(np.int32))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5), dpi=200, sharex=True)\n",
    "df.boxplot(\n",
    "    column=[\"duration\", \"amplitude\", \"frequency\", \"density\"],\n",
    "    by=\"age_bin\", \n",
    "    ax=axes, fontsize=8, rot=90,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    boxprops=dict(color=viz.PALETTE['blue']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['blue']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    ")\n",
    "fig.suptitle('')\n",
    "for i_ax, ax in enumerate(axes.flatten()):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.text(\n",
    "        x=-0.06, y=1.04, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "axes[0, 0].set_title(\"Duración (s)\", fontsize=8, loc=\"left\")\n",
    "axes[0, 1].set_title(\"Amplitud PP ($\\mu$V)\", fontsize=8, loc=\"left\")\n",
    "axes[1, 0].set_title(\"Frecuencia (Hz)\", fontsize=8, loc=\"left\")\n",
    "axes[1, 1].set_title(\"Densidad (epm)\", fontsize=8, loc=\"left\")\n",
    "\n",
    "axes[1, 0].set_xlabel(\"Edad (años)\", fontsize=8)\n",
    "axes[1, 1].set_xlabel(\"Edad (años)\", fontsize=8)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_age\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_annotate_brackets(ax, num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p <= 0.05\n",
    "        # ** is p <= 0.01\n",
    "        # *** is p <= 0.001\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        while data < p:\n",
    "            text += '*'\n",
    "            p /= 10.\n",
    "\n",
    "            if maxasterix and len(text) == maxasterix:\n",
    "                break\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = ax.get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh/2)\n",
    "\n",
    "    ax.plot(barx, bary, c='black', linewidth=0.8)\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "\n",
    "    ax.annotate(text, mid, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by-subject params, sex differences\n",
    "save_figure = True\n",
    "\n",
    "df = enough_dets.copy()\n",
    "df['age_bin'] = pd.cut(enough_dets.age, bins=[5, 12, 50, 90])\n",
    "\n",
    "for param_name in [\"duration\", \"amplitude\", \"frequency\", \"density\"]:\n",
    "    print(\"\")\n",
    "    for a in df.groupby(by=\"age_bin\"):\n",
    "        mid_age = a[0].mid\n",
    "        data_l = []\n",
    "        sex_l = []\n",
    "        for b in a[1].groupby(by=\"female\"):\n",
    "            female = b[0]\n",
    "            table = b[1]\n",
    "            data_l.append(table[param_name].values)\n",
    "            sex_l.append(female)\n",
    "        # statistical test\n",
    "        pvalue = stats.ttest_ind(data_l[0], data_l[1], equal_var=False)[1]\n",
    "        print(\"Paramater %s, Age midpoint %1.1f. female=%d with mean %1.4f vs female=%d with mean %1.4f (Diff %1.4f): P-value %1.4f\" % (\n",
    "            param_name, mid_age,\n",
    "            sex_l[0], data_l[0].mean(),\n",
    "            sex_l[1], data_l[1].mean(),\n",
    "            data_l[0].mean() - data_l[1].mean(),\n",
    "            pvalue\n",
    "        ))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(8, 3), dpi=200, sharex=True)\n",
    "df[df.female==0].boxplot(\n",
    "    column=[\"duration\", \"amplitude\", \"frequency\", \"density\"],\n",
    "    by=[\"age_bin\"], \n",
    "    fontsize=8, rot=0, ax=axes,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    positions=[0.75, 2.25, 3.75], widths=0.4,\n",
    "    boxprops=dict(color=viz.PALETTE['blue']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['blue']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    ")\n",
    "df[df.female==1].boxplot(\n",
    "    column=[\"duration\", \"amplitude\", \"frequency\", \"density\"],\n",
    "    by=[\"age_bin\"], \n",
    "    fontsize=8, rot=0, ax=axes,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    positions=[1.25, 2.75, 4.25], widths=0.4, \n",
    "    boxprops=dict(color=viz.PALETTE['red']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['red']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    ")\n",
    "\n",
    "fig.suptitle('')\n",
    "for i_ax, ax in enumerate(axes.flatten()):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xlim([0.25, 4.75])\n",
    "    ax.set_xticks([1, 2.5, 4])\n",
    "    ax.set_xlabel(\"Edad (años)\", fontsize=8)\n",
    "    ax.text(\n",
    "        x=-0.15, y=1.02, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "axes[0].set_title(\"Duración (s)\", fontsize=8, loc=\"left\")\n",
    "axes[1].set_title(\"Amplitud PP ($\\mu$V)\", fontsize=8, loc=\"left\")\n",
    "axes[2].set_title(\"Frecuencia (Hz)\", fontsize=8, loc=\"left\")\n",
    "axes[3].set_title(\"Densidad (epm)\", fontsize=8, loc=\"left\")\n",
    "\n",
    "# P-values\n",
    "positions = np.array([0.75, 1.25, 2.25, 2.75, 3.75, 4.25])\n",
    "param_dict = {\n",
    "    'duration': dict(ax=axes[0], max_value=1.43),\n",
    "    'amplitude': dict(ax=axes[1], max_value=75),\n",
    "    'frequency': dict(ax=axes[2], max_value=15.5),\n",
    "    'density': dict(ax=axes[3], max_value=8),\n",
    "}\n",
    "for param_name in param_dict.keys():\n",
    "    plot_specs = param_dict[param_name]\n",
    "    for i, a in enumerate(df.groupby(by=\"age_bin\")):\n",
    "        data_l = []\n",
    "        for b in a[1].groupby(by=\"female\"):\n",
    "            data_l.append(b[1][param_name].values)\n",
    "        pvalue = stats.ttest_ind(data_l[0], data_l[1], equal_var=False)[1]\n",
    "        if pvalue <= 0.001:\n",
    "            p_text = '***'\n",
    "        elif pvalue <= 0.01:\n",
    "            p_text = '**'\n",
    "        elif pvalue <= 0.05:\n",
    "            p_text = '*'\n",
    "        else:\n",
    "            p_text = 'ns'\n",
    "            continue\n",
    "        barplot_annotate_brackets(plot_specs['ax'], 2 * i, 2 * i + 1, p_text, positions, len(positions) * [plot_specs['max_value']], fs=8, dh=0.0, barh=0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "        \n",
    "# Get legend\n",
    "custom_lines = [Line2D([0], [0], color=viz.PALETTE['blue'], lw=2),\n",
    "                Line2D([0], [0], color=viz.PALETTE['red'], lw=2)]\n",
    "labels = [\"Hombres\", \"Mujeres\"]\n",
    "lg1 = fig.legend(\n",
    "    custom_lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.02), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_sex\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscilación de 0.02 Hz medida por distancia entre clusters\n",
    "\n",
    "Seria interesarte desagregarlo por demografia en NSRR. Por ejemplo hombre, y mujer, y 5-18 años, 18-50 años, y 50-90 años.\n",
    "18 por la mayoria de edad y 50 porque es el corte de las fases en moda. In Science paper, human subjects had between 18 to 28 y.o. and were all men. Seria bueno mostrar tambien la distribucion de los tamaños de los clusters para mostrar que son normalmente pequeños. Podria probar la robustez del resultado al criterio de clustering, por ejemplo probar {5, 10, 15, 20}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('mass_ss', load_checkpoint=True, verbose=False)\n",
    "expert = 1\n",
    "\n",
    "subject_id = dataset.all_ids[0]\n",
    "marks = dataset.get_subject_stamps(subject_id, which_expert=expert, pages_subset=constants.N2_RECORD)\n",
    "n2_pages = dataset.get_subject_pages(subject_id, pages_subset=constants.N2_RECORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_l, clusters = get_distance_between_clusters(marks, n2_pages, dataset.page_size, dataset.fs, separation_to_join_seconds=10, return_clusters=True, shuffle=True, seed=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 2), dpi=100)\n",
    "axes[0].hist(distances_l, bins=30)\n",
    "axes[0].set_xlabel(\"Distancia (s)\")\n",
    "clusters_concat = np.concatenate(clusters, axis=0)\n",
    "sizes = (clusters_concat[:, 1] - clusters_concat[:, 0] + 1) / dataset.fs\n",
    "axes[1].hist(sizes, bins=30)\n",
    "axes[1].set_xlabel(\"Tamaño (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "s = [(c.max() - c.min() + 1) for c in clusters]\n",
    "max_length = np.max(s)\n",
    "n_blocks = len(clusters)\n",
    "matrix = np.zeros((n_blocks, max_length))\n",
    "for i_c, c in enumerate(clusters):\n",
    "    c = c - c.min()\n",
    "    v = utils.stamp2seq(c, 0, max_length-1)\n",
    "    matrix[i_c, :] = v\n",
    "\n",
    "max_time_minutes = max_length / dataset.fs / 60\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4), dpi=100)\n",
    "ax.imshow(matrix, aspect='auto', extent=(0, max_time_minutes, 1, n_blocks), cmap=\"binary\")\n",
    "ax.set_xticks(np.arange(0, max_time_minutes + 0.001, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_placement(durations, start_sample, end_sample, seed):\n",
    "    durations = np.random.RandomState(seed=seed).permutation(durations)\n",
    "    total_duration = np.sum(durations)\n",
    "    \n",
    "    n_gaps = len(durations) + 1\n",
    "    samples_available_for_gaps = end_sample - start_sample + 1 - total_duration\n",
    "    \n",
    "    break_points = np.random.RandomState(seed=seed).choice(samples_available_for_gaps, size=n_gaps-1, replace=False)\n",
    "    break_points = np.sort(break_points)\n",
    "    \n",
    "    gaps = np.diff(break_points, prepend=0, append=samples_available_for_gaps)\n",
    "\n",
    "    last_sample = start_sample\n",
    "    events = []\n",
    "    for g, d in zip(gaps[:-1], durations):\n",
    "        event_start = last_sample + g + 1\n",
    "        event_end = event_start + d - 1\n",
    "        events.append([event_start, event_end])\n",
    "        last_sample = event_end\n",
    "    events = np.stack(events, axis=0)\n",
    "    return events\n",
    "\n",
    "\n",
    "def split_into_blocks(pages_indices, min_block_length=1):\n",
    "    split_points = np.where(np.diff(pages_indices) > 1)[0]\n",
    "    split_points = split_points + 1\n",
    "    end_locs = np.concatenate([split_points, [pages_indices.size - 1]])\n",
    "    start_locs = np.concatenate([[0], split_points])\n",
    "    blocks = []\n",
    "    for start_loc, end_loc in zip(start_locs, end_locs):\n",
    "        block = pages_indices[start_loc:end_loc]\n",
    "        if len(block) >= min_block_length:\n",
    "            blocks.append(block)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def get_distance_between_clusters(marks, n2_pages, page_size, fs, separation_to_join_seconds=10, return_clusters=False, shuffle=False, seed=0):\n",
    "    # process blocks of contiguous N2 stages independently\n",
    "    blocks = split_into_blocks(n2_pages)\n",
    "    distances_l = []\n",
    "    clusters_l = []\n",
    "    for i_block, block in enumerate(blocks):\n",
    "        start_sample = int(block[0] * page_size)\n",
    "        end_sample = int((block[-1] + 1) * page_size)\n",
    "        block_marks = utils.filter_stamps(marks, start_sample, end_sample)\n",
    "        if block_marks.shape[0] < 2:\n",
    "            # not enough clusters to measure distances\n",
    "            continue\n",
    "        \n",
    "        if shuffle:\n",
    "            seed_block = int(1e3 * seed) + i_block\n",
    "            # ignore positions, only preserve lengths in n_samples\n",
    "            durations = block_marks[:, 1] - block_marks[:, 0] + 1\n",
    "            # relocate each duration\n",
    "            block_marks = get_random_placement(durations, start_sample, end_sample, seed_block)\n",
    "        \n",
    "        # cluster marks\n",
    "        clusters = stamp_correction.combine_close_stamps(block_marks, fs, separation_to_join_seconds)\n",
    "        # remove marks that did not cluster (clean space between clusters)\n",
    "        clusters = stamp_correction.filter_duration_stamps(clusters, fs, separation_to_join_seconds, None)\n",
    "        \n",
    "        if clusters.shape[0] < 2:\n",
    "            # not enough clusters to measure distances\n",
    "            continue\n",
    "        # Get centers\n",
    "        centers = clusters.mean(axis=1)\n",
    "        # Get distances between centers in seconds\n",
    "        distances_between_centers = np.diff(centers) / fs\n",
    "        distances_l.append(distances_between_centers)\n",
    "        clusters_l.append(clusters)\n",
    "    \n",
    "    if len(distances_l) == 0:\n",
    "        distances_l = np.zeros((0, ))\n",
    "    else:\n",
    "        distances_l = np.concatenate(distances_l)\n",
    "        \n",
    "    if return_clusters:\n",
    "        return distances_l, clusters_l\n",
    "    else:\n",
    "        return distances_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust clustering strategy most delicate parameter: the distance to cluster, using expert annotations\n",
    "save_figure = False\n",
    "\n",
    "print_dataset_map = {\n",
    "    (constants.MASS_SS_NAME, 1): 'MASS-SS2-E1SS',\n",
    "    (constants.MASS_SS_NAME, 2): 'MASS-SS2-E2SS',\n",
    "    (constants.MODA_SS_NAME, 1): 'MASS-MODA',\n",
    "    (constants.NSRR_SS_NAME, 1): 'NSRR',\n",
    "}\n",
    "\n",
    "configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1),\n",
    "    #dict(dataset_name=constants.MASS_SS_NAME, expert=2),\n",
    "    #dict(dataset_name=constants.MODA_SS_NAME, expert=1),\n",
    "]\n",
    "\n",
    "max_distance_minutes_for_pdf = 5  # minutes\n",
    "max_distance_seconds = 60 * max_distance_minutes_for_pdf\n",
    "distances_array = np.arange(0, max_distance_seconds + 1e-6, 0.01)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 4), dpi=120)\n",
    "for i_config, config in enumerate(configs):\n",
    "    dataset_str = print_dataset_map[(config['dataset_name'], config['expert'])]\n",
    "    \n",
    "    dataset = load_dataset(config['dataset_name'], load_checkpoint=True, verbose=False)\n",
    "    expert = config['expert']\n",
    "    \n",
    "    for separation_to_join_seconds in [5, 10, 15]: #[5, 7.5, 10, 12.5 ,15]:\n",
    "    \n",
    "        for i_s, shuffle in enumerate([False, True]):\n",
    "    \n",
    "            distances_l = []\n",
    "            for i_subject, subject_id in enumerate(dataset.all_ids):\n",
    "                #print(\"Sujeto\", subject_id, \"separacion\", separation_to_join_seconds)\n",
    "                marks = dataset.get_subject_stamps(subject_id, which_expert=expert, pages_subset=constants.N2_RECORD)\n",
    "                n2_pages = dataset.get_subject_pages(subject_id, pages_subset=constants.N2_RECORD)\n",
    "                subject_distances = get_distance_between_clusters(\n",
    "                    marks, n2_pages, dataset.page_size, dataset.fs, \n",
    "                    separation_to_join_seconds=separation_to_join_seconds, \n",
    "                    return_clusters=False,\n",
    "                    shuffle=shuffle,\n",
    "                    seed=i_subject)\n",
    "                distances_l.append(subject_distances)\n",
    "            distances_l = np.concatenate(distances_l)\n",
    "\n",
    "            # consider only those distances less than some meaningful value\n",
    "            distances_valid = distances_l[distances_l <= max_distance_seconds]\n",
    "            distance_pdf = stats.gaussian_kde(distances_valid)   \n",
    "            distances_probability = distance_pdf(distances_array)\n",
    "\n",
    "            axes[i_s, i_config].plot(distances_array, distances_probability, linewidth=1.0, label='%1.1f s' % separation_to_join_seconds)\n",
    "\n",
    "    axes[0, i_config].set_title(dataset_str, fontsize=8, loc=\"left\")\n",
    "    axes[1, i_config].set_title('%s (aleatorio)' % dataset_str, fontsize=8, loc=\"left\")\n",
    "    \n",
    "    \n",
    "    \n",
    "for i_ax, ax in enumerate(axes.flatten()):\n",
    "    ax.set_xticks(np.arange(0, max_distance_seconds + 1e-6, 25))\n",
    "    ax.grid(axis=\"x\")\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([0, 200])\n",
    "    ax.set_ylim([0, None])\n",
    "    ax.text(\n",
    "        x=-0.08, y=1.05, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    ax.set_xlabel(\"Distancia entre grupos (s)\", fontsize=8)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes.flatten():\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = list(labels_to_lines_dict.keys())\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.02), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_params_clustering_fit\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at MASS1 and MASS2, a clustering distance of 10s seems good to fit the known 50s oscillation.\n",
    "# When using 10s, in MODA the mode of the distribution is smaller, like 30 something.\n",
    "# In moda, there could be a problem because it is composed of 120s blocks, which is restrictive to capture the modal nature of the 50s gaps.\n",
    "# But nevertheless there is like a minor peak at around 50 in MODA  when using 50s, so that is a good sign.\n",
    "# Decision: 10s separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now measure the stuff in NSRR annotations and group by demographics\n",
    "separation_to_join_seconds = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_to_load = [0, 1, 2, 3]  # 0 to 11\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True, params={pkeys.PAGE_DURATION: 30})\n",
    "pred_objects = load_predictions(parts_to_load, nsrr)\n",
    "demography = table_bysubject[['subject_id', 'age', 'female']].set_index('subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n2_minutes = 60\n",
    "min_distance_to_cluster = 10  # s\n",
    "max_distance_minutes_to_consider = 5  # min\n",
    "\n",
    "page_size = int(nsrr.fs * nsrr.original_page_duration)\n",
    "\n",
    "distances_l = []\n",
    "for predictions in pred_objects:\n",
    "    for subject_id in predictions.all_ids:\n",
    "        n2_pages = predictions.data[subject_id]['n2_pages']\n",
    "        n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "        if n2_minutes < min_n2_minutes:\n",
    "            continue\n",
    "        marks = predictions.get_subject_stamps(subject_id, pages_subset='n2')\n",
    "        n_marks = marks.shape[0]\n",
    "        if n_marks == 0:\n",
    "            continue\n",
    "        \n",
    "        # cluster marks\n",
    "        marks = stamp_correction.combine_close_stamps(marks, nsrr.fs, min_distance_to_cluster)\n",
    "        # remove marks that did not clustered\n",
    "        marks = stamp_correction.filter_duration_stamps(marks, nsrr.fs, min_distance_to_cluster, None)\n",
    "        \n",
    "        # Process blocks independently\n",
    "        blocks = split_into_blocks(n2_pages)\n",
    "        for block in blocks:\n",
    "            start_sample = int(block[0] * page_size)\n",
    "            end_sample = int((block[-1] + 1) * page_size)\n",
    "            block_marks = utils.filter_stamps(marks, start_sample, end_sample)\n",
    "            if block_marks.shape[0] < 2:\n",
    "                continue\n",
    "            # Get centers\n",
    "            centers = block_marks.mean(axis=1)\n",
    "            # Get distances between centers in seconds\n",
    "            distances_between_centers = np.diff(centers) / nsrr.fs\n",
    "            distances_l.append(distances_between_centers)\n",
    "# concat all\n",
    "distances_l = np.concatenate(distances_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only those distances less than some meaningful value\n",
    "max_distance_seconds = 60 * max_distance_minutes_to_consider\n",
    "distances_valid = distances_l[distances_l <= max_distance_seconds]\n",
    "distance_pdf = stats.gaussian_kde(distances_valid)\n",
    "\n",
    "distances_array = np.arange(0, max_distance_seconds, 0.1)\n",
    "distances_probability = distance_pdf(distances_array)\n",
    "plt.plot(distances_array, distances_probability)\n",
    "plt.show()\n",
    "\n",
    "peak_at = distances_array[np.argmax(distances_probability)]\n",
    "print(peak_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice shuffling of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "durations = [10, 5, 20, 15]\n",
    "start_sample = 100\n",
    "end_sample = 300\n",
    "\n",
    "\n",
    "valid_centers = np.arange(start_sample, end_sample)\n",
    "\n",
    "intersection = True\n",
    "counter = 0\n",
    "seed_base = int(1e4 * seed)\n",
    "while intersection:\n",
    "    print(\"counter\", counter)\n",
    "    this_seed = seed_base + counter\n",
    "    centers = np.random.RandomState(seed=this_seed).choice(valid_centers, size=len(durations), replace=False)\n",
    "    events = []\n",
    "    for c, d in zip(centers, durations):\n",
    "        event_start = c - d // 2\n",
    "        event_end = event_start + d - 1\n",
    "        events.append([event_start, event_end])\n",
    "    events = np.stack(events, axis=0)\n",
    "    events = events[np.argsort(events[:, 0])]\n",
    "    # Check intersection\n",
    "    m = utils.get_overlap_matrix(events, events)\n",
    "    if np.all(m.sum(axis=1) == 1):\n",
    "        intersection = False\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(events)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), dpi=100)\n",
    "for e in events:\n",
    "    ax.fill_between(e, -1, 1, linewidth=4, alpha=0.3)\n",
    "ax.set_xlim([0, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "durations = [10, 5, 20, 15]\n",
    "start_sample = 100\n",
    "end_sample = 300\n",
    "\n",
    "\n",
    "\n",
    "durations = np.random.RandomState(seed=seed).permutation(durations)\n",
    "\n",
    "total_duration = np.sum(durations)\n",
    "samples_available_for_gaps = end_sample - start_sample + 1 - total_duration\n",
    "\n",
    "n_gaps = len(durations) + 1\n",
    "\n",
    "break_points = np.random.RandomState(seed=seed).choice(samples_available_for_gaps, size=n_gaps-1, replace=False)\n",
    "break_points = np.sort(break_points)\n",
    "gaps = np.diff(break_points, prepend=0, append=samples_available_for_gaps)\n",
    "\n",
    "last_sample = start_sample\n",
    "events = []\n",
    "for g, d in zip(gaps[:-1], durations):\n",
    "    event_start = last_sample + g + 1\n",
    "    event_end = event_start + d - 1\n",
    "    events.append([event_start, event_end])\n",
    "    last_sample = event_end\n",
    "events = np.stack(events, axis=0)\n",
    "\n",
    "print(events)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), dpi=100)\n",
    "for e in events:\n",
    "    ax.fill_between(e, -1, 1, linewidth=4, alpha=0.3)\n",
    "ax.set_xlim([0, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
