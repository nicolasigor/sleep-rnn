{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, hilbert\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.common import viz, constants\n",
    "from sleeprnn.helpers import reader, plotter, misc, performer\n",
    "from sleeprnn.detection import metrics\n",
    "from figs_thesis import fig_utils\n",
    "from baselines_scripts.butils import get_partitions\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sleeprnn.data import utils\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "BASELINES_PATH = os.path.join(project_root, 'resources', 'comparison_data', 'baselines_2021')\n",
    "\n",
    "%matplotlib inline\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación E1 y E2 en MASS-SS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marks(dataset_name, expert):\n",
    "    dataset = reader.load_dataset(dataset_name, verbose=False)\n",
    "    marks_list = dataset.get_stamps(pages_subset=constants.N2_RECORD, which_expert=expert)\n",
    "    return marks_list\n",
    "\n",
    "\n",
    "events_list = get_marks(constants.MASS_SS_NAME, 1)\n",
    "detections_list = get_marks(constants.MASS_SS_NAME, 2)\n",
    "performance = fig_utils.compute_fold_performance(events_list, detections_list, constants.MACRO_AVERAGE)\n",
    "for metric_name in performance.keys():\n",
    "    print('%s: %1.1f%%' % (metric_name.ljust(10), 100 * performance[metric_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de desempeño (by-fold)\n",
    "Comparación con la literatura en desempeño in-dataset.\n",
    "Todos los P-value de REDv2-CWT son mayores a 0.05.\n",
    "Todos los P-value de baselines son menor a 0.001 excepto en INTA-UCH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: '\\\\textbf{REDv2-Time}',\n",
    "    constants.V2_CWT1D: '\\\\textbf{REDv2-CWT}',\n",
    "    'dosed': 'DOSED \\cite{chambon2019dosed}',\n",
    "    'a7': 'A7 \\cite{lacourse2019sleep}',\n",
    "    'spinky': 'Spinky \\cite{lajnef2017meet}'\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='fixed', seeds=11),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='fixed', seeds=11),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='fixed', seeds=11),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Measure performance byfold\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {'Detector': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    for model_name in pred_dict.keys():\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "            table['Detector'].append(model_name)\n",
    "            table['F1-score'].append(performance['F1-score'])\n",
    "            table['Recall'].append(performance['Recall'])\n",
    "            table['Precision'].append(performance['Precision'])\n",
    "            table['mIoU'].append(performance['mIoU'])\n",
    "            table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    print(\"By-fold statistics\")\n",
    "    metric_mean = table.groupby(by=[\"Detector\"]).mean().drop(columns=[\"Fold\"])\n",
    "    metric_std = table.groupby(by=[\"Detector\"]).std(ddof=0).drop(columns=[\"Fold\"])\n",
    "    print(\"Detector & F1-score (\\%) & Recall (\\%) & Precision (\\%) & mIoU (\\%) \\\\\\\\\")\n",
    "    for model_name in pred_dict.keys():\n",
    "        print(\"%s & %s & %s & %s & %s \\\\\\\\\" % (\n",
    "            print_model_names[model_name].ljust(10),\n",
    "            fig_utils.format_metric(metric_mean.at[model_name, \"F1-score\"], metric_std.at[model_name, \"F1-score\"]),\n",
    "            fig_utils.format_metric(metric_mean.at[model_name, \"Recall\"], metric_std.at[model_name, \"Recall\"]),\n",
    "            fig_utils.format_metric(metric_mean.at[model_name, \"Precision\"], metric_std.at[model_name, \"Precision\"]),\n",
    "            fig_utils.format_metric(metric_mean.at[model_name, \"mIoU\"], metric_std.at[model_name, \"mIoU\"]),\n",
    "        ))\n",
    "    # Statistical tests\n",
    "    reference_model_name = constants.V2_TIME\n",
    "    print(\"P-value test against %s\" % reference_model_name)\n",
    "    for model_name in pred_dict.keys():\n",
    "        model_metrics = table[table[\"Detector\"] == model_name][\"F1-score\"].values\n",
    "        reference_metrics = table[table[\"Detector\"] == reference_model_name][\"F1-score\"].values\n",
    "        pvalue = stats.ttest_ind(model_metrics, reference_metrics, equal_var=False)[1]\n",
    "        print(\"%s: P %1.4f\" % (print_model_names[model_name].ljust(10), pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dispersion by-subject\n",
    "5CV solamente por brevedad, ya que ya se vio que es similar el desempeño y permite tener todos los sujetos en MASS-SS2.\n",
    "\n",
    "Datos cuantitativos de dispersión entre sujetos (print esperado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "dispersions_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # Measure performance by subject\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {'Detector': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Subject': [], 'Fold': []}\n",
    "    if config[\"dataset_name\"] == constants.MODA_SS_NAME:\n",
    "        valid_subjects = [sub_id for sub_id in dataset.all_ids if dataset.data[sub_id]['n_blocks'] == 10 and sub_id not in ['01-01-0012', '01-01-0022']]\n",
    "        print(\"moda, using n=\", len(valid_subjects))\n",
    "    else:\n",
    "        valid_subjects = dataset.all_ids\n",
    "    for model_name in pred_dict.keys():\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_subject_performance(events_list, detections_list)\n",
    "            for i, subject_id in enumerate(subject_ids):\n",
    "                if subject_id in valid_subjects:\n",
    "                    table['Detector'].append(model_name)\n",
    "                    table['F1-score'].append(performance['F1-score'][i])\n",
    "                    table['Recall'].append(performance['Recall'][i])\n",
    "                    table['Precision'].append(performance['Precision'][i])\n",
    "                    table['mIoU'].append(performance['mIoU'][i])\n",
    "                    table['Subject'].append(subject_id)\n",
    "                    table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    bysubject_dispersions = 100 * table.groupby([\"Detector\", \"Subject\"]).mean().drop(columns=[\"Fold\"]).groupby(\"Detector\").std(ddof=0)\n",
    "    dispersions_list.append(bysubject_dispersions)\n",
    "print(\"Dispersions computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, config in enumerate(eval_configs):\n",
    "    print(\"\\nDataset: %s\" % print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])])\n",
    "    print(dispersions_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "fig, axes = plt.subplots(1, 5, figsize=(8, 3), dpi=200, sharex=True)\n",
    "for i, config in enumerate(eval_configs):\n",
    "    ax = axes[i]\n",
    "    disp_table = dispersions_list[i]\n",
    "    if config[\"dataset_name\"] == constants.MASS_KC_NAME:\n",
    "        extra_row = pd.DataFrame([[\"a7\", 0, 0, 0, 0]], columns=[\"Detector\", \"F1-score\", \"Recall\", \"Precision\", \"mIoU\"])\n",
    "        extra_row = extra_row.set_index(\"Detector\")\n",
    "        disp_table_mod = disp_table.append(extra_row).reindex([\"spinky\", \"a7\", \"dosed\", constants.V2_CWT1D, constants.V2_TIME])\n",
    "    else:\n",
    "        extra_row = pd.DataFrame([[\"spinky\", 0, 0, 0, 0]], columns=[\"Detector\", \"F1-score\", \"Recall\", \"Precision\", \"mIoU\"])\n",
    "        extra_row = extra_row.set_index(\"Detector\")\n",
    "        disp_table_mod = disp_table.append(extra_row).reindex([\"spinky\", \"a7\", \"dosed\", constants.V2_CWT1D, constants.V2_TIME])\n",
    "    \n",
    "    ax = disp_table_mod.plot.barh(y=[\"F1-score\", \"Recall\", \"Precision\"], ax=ax, fontsize=8, legend=False)\n",
    "    ax.set_title(print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])], loc=\"left\", fontsize=8)\n",
    "    \n",
    "    yticklabels = ax.get_yticklabels()\n",
    "    ax.set_yticklabels([print_model_names[yt.get_text()] for yt in yticklabels])\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"$\\sigma_\\mathrm{subjects}$ (%)\", fontsize=8)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    if i > 0:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlim([0, 16])\n",
    "    ax.set_xticks([0, 4, 8, 12, 16])\n",
    "    ax.set_xticks(np.arange(0, 16+0.001, 2), minor=True)\n",
    "    ax.grid(axis=\"x\", which=\"minor\")\n",
    "    ax.text(\n",
    "        x=-0.01, y=1.15, fontsize=16, s=r\"$\\bf{%s}$\" % letters[i],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend\n",
    "lines_labels = [axes[0].get_legend_handles_labels()]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "# plt.subplots_adjust(bottom=0.9)\n",
    "lg = fig.legend(\n",
    "    lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.02), ncol=3, frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_bysubject_std\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg,), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efecto umbral probabilidad: curva PR y métricas vs umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Measure performance byfold\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {'Detector': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    for model_name in pred_dict.keys():\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "            table['Detector'].append(model_name)\n",
    "            table['F1-score'].append(performance['F1-score'])\n",
    "            table['Recall'].append(performance['Recall'])\n",
    "            table['Precision'].append(performance['Precision'])\n",
    "            table['mIoU'].append(performance['mIoU'])\n",
    "            table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    print(\"By-fold statistics\")\n",
    "    metric_mean = table.groupby(by=[\"Detector\"]).mean().drop(columns=[\"Fold\"])\n",
    "    metrics_list.append(metric_mean)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint of curve of thresholds\n",
    "adjusted_thr_list = np.arange(0.05, 0.95 + 0.001, 0.05)\n",
    "with open('pr_curve_ckpt.pkl', 'rb') as handle:\n",
    "    metrics_curve_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_largest_sequence(binary_sequence):\n",
    "    locs_seq = utils.seq2stamp(binary_sequence)\n",
    "    locs_sizes = [(l[1]-l[0]) for l in locs_seq]\n",
    "    valid_interval = locs_seq[np.argmax(locs_sizes)]\n",
    "    binary_sequence = np.zeros(binary_sequence.shape, dtype=np.int32)\n",
    "    binary_sequence[valid_interval[0]:valid_interval[1]+1] = 1\n",
    "    return binary_sequence\n",
    "\n",
    "\n",
    "def fix_single_pr_curve(single_recall_curve, single_precision_curve):\n",
    "    # ignore \"come back\"\n",
    "    d_recall = np.diff(single_recall_curve, prepend=single_recall_curve[0])\n",
    "    d_precision = np.diff(single_precision_curve, prepend=single_precision_curve[0])\n",
    "    d_recall = (d_recall <= 0).astype(np.int32)\n",
    "    d_precision = (d_precision >= 0).astype(np.int32)\n",
    "    d_recall = keep_largest_sequence(d_recall)\n",
    "    d_precision = keep_largest_sequence(d_precision)\n",
    "    valid_locs = np.where(d_recall & d_precision)[0]\n",
    "    single_recall_curve = single_recall_curve[valid_locs]\n",
    "    single_precision_curve= single_precision_curve[valid_locs]\n",
    "    return single_recall_curve, single_precision_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "markersize = 5\n",
    "red_alpha = 0.6\n",
    "pr_alpha_curve = 0.5\n",
    "thr_alpha_curve = 1.0\n",
    "baseline_color = viz.GREY_COLORS[9]\n",
    "letters = ['A', 'B', 'C', 'D', 'E']\n",
    "letters2 = ['F', 'G', 'H', 'I', 'J']\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "spindle_net = dict(metrics={\"F1-score\": .83, \"Recall\": .852, \"Precision\": .81}, marker='<', color=baseline_color)\n",
    "dkl_kc = dict(metrics={\"F1-score\": .78, \"Recall\": .80, \"Precision\": .77}, marker='>', color=baseline_color)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 4), dpi=200)\n",
    "for i, config in enumerate(eval_configs):\n",
    "    # PR PLOT\n",
    "    ax = axes[0, i]\n",
    "    metric_dict = metrics_list[i].to_dict('index')\n",
    "    for model_name in metric_dict.keys():\n",
    "        ax.plot(\n",
    "            metric_dict[model_name][\"Recall\"], \n",
    "            metric_dict[model_name][\"Precision\"],\n",
    "            linestyle=\"None\",\n",
    "            alpha=red_alpha,\n",
    "            marker=model_specs[model_name][\"marker\"],\n",
    "            markersize=markersize,\n",
    "            markeredgewidth=0.0, zorder=20,\n",
    "            color=model_specs[model_name][\"color\"],\n",
    "            label=print_model_names[model_name])\n",
    "    ax.set_title(print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])], loc=\"left\", fontsize=8)\n",
    "    plotter.format_precision_recall_plot_simple(\n",
    "        ax, axis_range=(0.5, 1), show_quadrants=False, show_grid=True,\n",
    "        axis_markers=np.arange(0.5, 1 + 0.001, 0.5), minor_axis_markers=np.arange(0.5, 1 + 0.001, 0.1))\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel(\"Recall\", fontsize=8)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Precision\", fontsize=8) \n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    # Get labels closer to axis\n",
    "    ax.xaxis.labelpad = -8\n",
    "    ax.yaxis.labelpad = -8\n",
    "    # Add PR curve\n",
    "    # [loc in config][model_name][fold_id][metric_name][loc in thr]\n",
    "    pr_curve_data = metrics_curve_list[i]\n",
    "    for model_name in pr_curve_data.keys():\n",
    "        n_folds = len(pr_curve_data[model_name].keys())\n",
    "        seeds_recall = [pr_curve_data[model_name][k][\"Recall\"] for k in range(n_folds)]\n",
    "        seeds_precision = [pr_curve_data[model_name][k][\"Precision\"] for k in range(n_folds)]\n",
    "        # Ignore the \"come-back\" of each individual curve before averaging\n",
    "        new_seeds_recall = []\n",
    "        new_seeds_precision = []\n",
    "        for k in range(n_folds):\n",
    "            fold_recall, fold_precision = fix_single_pr_curve(seeds_recall[k], seeds_precision[k])\n",
    "            new_seeds_recall.append(fold_recall)\n",
    "            new_seeds_precision.append(fold_precision)\n",
    "        mean_recall_curve, mean_precision_curve = plotter.average_curves(new_seeds_recall, new_seeds_precision)\n",
    "        ax.plot(\n",
    "            mean_recall_curve, mean_precision_curve,\n",
    "            linewidth=1.0, color=model_specs[model_name][\"color\"], zorder=10, alpha=pr_alpha_curve)\n",
    "\n",
    "    if print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])] == print_dataset_names[(constants.MASS_SS_NAME, 2)]:\n",
    "        ax.plot(\n",
    "            spindle_net[\"metrics\"][\"Recall\"], \n",
    "            spindle_net[\"metrics\"][\"Precision\"],\n",
    "            linestyle=\"None\",\n",
    "            alpha=red_alpha,\n",
    "            marker=spindle_net[\"marker\"],\n",
    "            markersize=markersize,\n",
    "            markeredgewidth=0.0, zorder=20,\n",
    "            color=spindle_net[\"color\"],\n",
    "            label=\"SpindleNet\")\n",
    "    if print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])] == print_dataset_names[(constants.MASS_KC_NAME, 1)]:\n",
    "        ax.plot(\n",
    "            dkl_kc[\"metrics\"][\"Recall\"], \n",
    "            dkl_kc[\"metrics\"][\"Precision\"],\n",
    "            linestyle=\"None\",\n",
    "            alpha=red_alpha,\n",
    "            marker=dkl_kc[\"marker\"],\n",
    "            markersize=markersize,\n",
    "            markeredgewidth=0.0, zorder=20,\n",
    "            color=dkl_kc[\"color\"],\n",
    "            label=\"DKL-KC\")\n",
    "    \n",
    "    ax.text(\n",
    "        x=-0.01, y=1.2, fontsize=16, s=r\"$\\bf{%s}$\" % letters[i],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "    # CHANGE DUE TO THR\n",
    "    ax = axes[1, i]\n",
    "    model_name = \"v2_time\"\n",
    "    # for model_name in pr_curve_data.keys():\n",
    "    \n",
    "    n_folds = len(pr_curve_data[model_name].keys())\n",
    "    seeds_metric = {\n",
    "        metric_name: np.stack([pr_curve_data[model_name][k][metric_name] for k in range(n_folds)], axis=0)\n",
    "        for metric_name in [\"F1-score\", \"Recall\", \"Precision\", \"mIoU\"]\n",
    "    }\n",
    "    for metric_name in [\"F1-score\", \"Recall\", \"Precision\", \"mIoU\"]:\n",
    "        ax.plot(\n",
    "            adjusted_thr_list, seeds_metric[metric_name].mean(axis=0), \n",
    "            linewidth=1.0, zorder=10, alpha=thr_alpha_curve, label=metric_name)\n",
    "        ax.fill_between(\n",
    "            adjusted_thr_list, \n",
    "            seeds_metric[metric_name].mean(axis=0) + seeds_metric[metric_name].std(axis=0), \n",
    "            seeds_metric[metric_name].mean(axis=0) - seeds_metric[metric_name].std(axis=0), \n",
    "            linewidth=1.0, zorder=10, alpha=0.2)\n",
    "    ax.axvline(0.5, color=\"k\", linewidth=1.5, zorder=30)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel(\"Umbral prob.\", fontsize=8)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    ax.set_xticks([0, 1.0])\n",
    "    ax.set_xticks(np.arange(0, 1 + 0.001, 0.1), minor=True)\n",
    "    ax.set_yticks([0.5, 1.0])\n",
    "    ax.set_yticks(np.arange(0.5, 1 + 0.001, 0.1), minor=True)\n",
    "    ax.grid(which=\"minor\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Métrica\", fontsize=8) \n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    ax.xaxis.labelpad = -8\n",
    "    ax.yaxis.labelpad = -8\n",
    "    ax.set_aspect(2)\n",
    "    \n",
    "    ax.text(\n",
    "        x=-0.01, y=1.05, fontsize=16, s=r\"$\\bf{%s}$\" % letters2[i],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes[0, :]:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"REDv2-Time\", \"REDv2-CWT\", \"DOSED\", \"A7\", \"Spinky\", \"SpindleNet\", \"DKL-KC\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=7, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.49), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "# Get legend metrics\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes[1, :]:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"F1-score\", \"Recall\", \"Precision\", \"mIoU\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg2 = fig.legend(\n",
    "    lines, labels, fontsize=7, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.02), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_pr_thr\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1, lg2), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1, lg2), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1, lg2), bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efecto umbral IoU: F1 vs IoU, Histograma IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "\n",
    "iou_curve_axis = np.arange(0.05, 0.95 + 0.001, 0.05)\n",
    "iou_hist_bins = np.linspace(0, 1, 21)\n",
    "\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Measure performance byfold\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {'Detector': [], 'F1-score_vs_iou': [], 'IoU_hist': [], 'mIoU': [], 'Fold': []}\n",
    "    for model_name in pred_dict.keys():\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_fold_performance_vs_iou(\n",
    "                events_list, detections_list, average_mode, iou_curve_axis)\n",
    "            iou_mean, iou_hist_values = fig_utils.compute_iou_histogram(\n",
    "                performance['nonzero_IoU'], average_mode, iou_hist_bins)\n",
    "            table['Detector'].append(model_name)\n",
    "            table['F1-score_vs_iou'].append(performance['F1-score_vs_iou'])\n",
    "            table['IoU_hist'].append(iou_hist_values)\n",
    "            table['mIoU'].append(iou_mean)\n",
    "            table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    print(\"By-fold statistics\")\n",
    "    metric_mean = table.groupby(by=[\"Detector\"]).apply(np.mean).drop(columns=[\"Fold\"])\n",
    "    metrics_list.append(metric_mean)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "markersize = 4\n",
    "f1_alpha_curve = 1.0\n",
    "iou_thr_reported = 0.2\n",
    "f1_markers_iou = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "idx_markers_iou = [\n",
    "    misc.closest_index(single_marker, iou_curve_axis) \n",
    "    for single_marker in f1_markers_iou]\n",
    "\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "letters = ['A', 'B', 'C', 'D', 'E']\n",
    "letters2 = ['F', 'G', 'H', 'I', 'J']\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "spindle_net = dict(metrics={\"F1-score\": .83, \"Recall\": .852, \"Precision\": .81}, marker='<', color=baseline_color)\n",
    "dkl_kc = dict(metrics={\"F1-score\": .78, \"Recall\": .80, \"Precision\": .77}, marker='>', color=baseline_color)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 4.5), dpi=200)\n",
    "for i, config in enumerate(eval_configs):\n",
    "    # F1-score vs IoU\n",
    "    ax = axes[0, i]\n",
    "    metric_dict = metrics_list[i].to_dict('index')\n",
    "    for model_name in metric_dict.keys():\n",
    "        ax.plot(\n",
    "            iou_curve_axis, \n",
    "            metric_dict[model_name][\"F1-score_vs_iou\"],\n",
    "            linewidth=1,\n",
    "            marker=model_specs[model_name][\"marker\"],\n",
    "            markersize=markersize,\n",
    "            alpha=f1_alpha_curve,\n",
    "            markeredgewidth=0.0, zorder=20,\n",
    "            color=model_specs[model_name][\"color\"],\n",
    "            label=print_model_names[model_name],\n",
    "            markevery=idx_markers_iou)\n",
    "    ax.set_title(print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])], loc=\"left\", fontsize=8)\n",
    "    # ax.axvline(iou_thr_reported, color=\"k\", linewidth=1.5, zorder=5, alpha=0.5)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel(\"Umbral IoU\", fontsize=8)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xticks([0, 1.0])\n",
    "    ax.set_xticks(np.arange(0, 1 + 0.001, 0.1), minor=True)\n",
    "    ax.set_yticks([0, 1.0])\n",
    "    ax.set_yticks(np.arange(0, 1 + 0.001, 0.1), minor=True)\n",
    "    ax.grid(which=\"minor\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"F1-score\", fontsize=8) \n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    ax.xaxis.labelpad = -8\n",
    "    ax.yaxis.labelpad = -8\n",
    "    ax.set_aspect(\"equal\")\n",
    "    \n",
    "    ax.text(\n",
    "        x=-0.01, y=1.2, fontsize=16, s=r\"$\\bf{%s}$\" % letters[i],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "    # IoU Hist\n",
    "    ax = axes[1, i]\n",
    "    max_value = 0\n",
    "    for model_name in metric_dict.keys():\n",
    "        max_value = max(metric_dict[model_name][\"IoU_hist\"].max(), max_value)\n",
    "    max_value = max_value * 1.3\n",
    "    n_cases = len(metric_dict.keys())\n",
    "    y_sep = 1 / n_cases\n",
    "    this_center = 1 - y_sep\n",
    "    \n",
    "    model_names = list(metric_dict.keys())\n",
    "    reference_order = [constants.V2_TIME, constants.V2_CWT1D, \"dosed\", \"a7\", \"spinky\"]\n",
    "    model_names_sorted = [n for n in reference_order if n in model_names]\n",
    "    \n",
    "    for i_offset, model_name in enumerate(model_names_sorted):\n",
    "        x, y = plotter.piecewise_constant_histogram(\n",
    "            iou_hist_bins, metric_dict[model_name][\"IoU_hist\"])\n",
    "        y = y_sep * y / max_value\n",
    "        ax.plot(\n",
    "            [metric_dict[model_name][\"mIoU\"], metric_dict[model_name][\"mIoU\"]], \n",
    "            [this_center, this_center + 0.8*y_sep],\n",
    "            linewidth=1.5, color=\"k\", zorder=25, label='mIoU')\n",
    "        ax.fill_between(\n",
    "            x, this_center + y, this_center,\n",
    "            edgecolor=model_specs[model_name][\"color\"], linewidth=1,\n",
    "            facecolor=viz.GREY_COLORS[3], zorder=20)\n",
    "        ax.plot(\n",
    "            0.05, this_center + 0.2*y_sep, \n",
    "            markersize=markersize, c=model_specs[model_name][\"color\"], zorder=15, \n",
    "            marker=model_specs[model_name][\"marker\"], linestyle=\"None\")\n",
    "        this_center = this_center - y_sep\n",
    "        if i_offset == 0:\n",
    "            lg = ax.legend(loc='upper left', fontsize=8, frameon=False, bbox_to_anchor=(0, 1.05))\n",
    "            \n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel(\"IoU de par\", fontsize=8)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_xticks([0, 1.0])\n",
    "    ax.set_xticks(np.arange(0, 1 + 0.001, 0.1), minor=True)\n",
    "    ax.xaxis.labelpad = -8\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Densidad\", fontsize=8)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(axis=\"x\", which=\"minor\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    \n",
    "    ax.text(\n",
    "        x=-0.01, y=1.05, fontsize=16, s=r\"$\\bf{%s}$\" % letters2[i],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes[0, :]:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"REDv2-Time\", \"REDv2-CWT\", \"DOSED\", \"A7\", \"Spinky\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=7, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.49), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_f1_iou\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters: By-event overlap\n",
    "[5CV only, MODA y MASS-KC, by-event all-in] matches individuales: duracion real vs predicha (ajuste lineal y R2), duracion real vs IoU  ¿scatter o hist2D?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations(events_list, detections_list):\n",
    "    # iou_matching = []  # Array for IoU for every true event (gs)\n",
    "    # idx_matching = []  # Array for the index associated with the true event.\n",
    "    _, idx_matching_list = metrics.matching_with_list(events_list, detections_list)\n",
    "    durations_real_list = []\n",
    "    durations_pred_list = []\n",
    "    for i in range(len(events_list)):\n",
    "        events = events_list[i]\n",
    "        detections = detections_list[i]\n",
    "        if events.size == 0 or detections.size == 0:\n",
    "            continue\n",
    "        idx_matching = idx_matching_list[i]\n",
    "        valid_event_locs = np.where(idx_matching != -1)[0]\n",
    "        if valid_event_locs.size == 0:\n",
    "            continue\n",
    "        events_m = events[valid_event_locs]\n",
    "        detections_m = detections[idx_matching[valid_event_locs]]\n",
    "        durations_real = events_m[:, 1] - events_m[:, 0] + 1\n",
    "        durations_pred = detections_m[:, 1] - detections_m[:, 0] + 1\n",
    "        durations_real_list.append(durations_real)\n",
    "        durations_pred_list.append(durations_pred)\n",
    "    durations_real_list = np.concatenate(durations_real_list)\n",
    "    durations_pred_list = np.concatenate(durations_pred_list)\n",
    "    return durations_real_list, durations_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Retrieve matchings (it does not matter macro or micro because all events are grouped together)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {'Detector': [], 'Duration_real': [], 'Duration_pred': []}\n",
    "    for model_name in pred_dict.keys():\n",
    "        durations_real_list = []\n",
    "        durations_pred_list = []\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            durations_real, durations_pred = get_durations(events_list, detections_list)\n",
    "            durations_real_list.append(durations_real)\n",
    "            durations_pred_list.append(durations_pred)\n",
    "        durations_real_list = np.concatenate(durations_real_list).astype(np.float32) / dataset.fs\n",
    "        durations_pred_list = np.concatenate(durations_pred_list).astype(np.float32) / dataset.fs\n",
    "        table['Detector'].append(model_name)\n",
    "        table['Duration_real'].append(durations_real_list)\n",
    "        table['Duration_pred'].append(durations_pred_list)\n",
    "    metrics_list.append(table)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "use_hist = True\n",
    "hist_temp_res = 0.05\n",
    "scatter_alpha = 0.01\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "letters = ['A', 'B', 'C', 'D']\n",
    "letters2 = ['E', 'F', 'G', 'H']\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(8, 4.7), dpi=200, sharex=True, sharey=True)\n",
    "for i, config in enumerate(eval_configs):\n",
    "    \n",
    "    max_dur = 2 #3 if 'moda' in config[\"dataset_name\"] else 3\n",
    "    \n",
    "    x_bins = np.arange(0, max_dur + 0.001, hist_temp_res)\n",
    "    y_bins = np.arange(0, max_dur + 0.001, hist_temp_res)\n",
    "    x_centers = x_bins[:-1] + x_bins[1]/2 - x_bins[0]/2\n",
    "    y_centers = y_bins[:-1] + y_bins[1]/2 - y_bins[0]/2\n",
    "    xv, yv = np.meshgrid(x_centers, y_centers)\n",
    "    \n",
    "    # Duration scatter\n",
    "    axx = axes[i, :]\n",
    "    metric_dict = metrics_list[i]\n",
    "    n_models = len(metric_dict['Detector'])\n",
    "    print(\"\\nDatos & Detector & R2 & Mean Error & Std Error \\\\\\\\\")\n",
    "    for j in range(n_models):\n",
    "        model_name = metric_dict['Detector'][j]\n",
    "        x_data = metric_dict['Duration_real'][j]\n",
    "        y_data = metric_dict['Duration_pred'][j]\n",
    "        ax = axx[j]\n",
    "        if use_hist:\n",
    "            hist, _, _ = np.histogram2d(\n",
    "                x_data, y_data, \n",
    "                bins=[x_bins, y_bins], density=True)\n",
    "            ax.hist2d(\n",
    "                xv.flatten(), yv.flatten(), bins=[x_bins, y_bins], weights=np.transpose(hist).flatten(), cmap='Blues')\n",
    "        else:\n",
    "            ax.plot(\n",
    "                x_data, y_data, \n",
    "                linestyle='None', marker='o', markersize=3, alpha=scatter_alpha, markeredgewidth=0.0)\n",
    "        dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "        model_str = print_model_names[model_name]\n",
    "        ax.set_title('%s, %s' % (model_str, dataset_str), loc=\"left\", fontsize=8)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xlim([0, max_dur])\n",
    "        ax.set_ylim([0, max_dur])\n",
    "        ax.set_xticks([0, max_dur])\n",
    "        ax.set_yticks([0, max_dur])\n",
    "        ax.set_xticks(np.arange(0, max_dur + 0.001, 0.5), minor=True)\n",
    "        ax.set_yticks(np.arange(0, max_dur + 0.001, 0.5), minor=True)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.xaxis.labelpad = -7\n",
    "        ax.yaxis.labelpad = -7\n",
    "        ax.grid(which=\"minor\")\n",
    "        ax.plot([0, max_dur], [0, max_dur], color=viz.GREY_COLORS[4], linewidth=0.7, zorder=5)\n",
    "        # fig_utils.linear_regression(metric_dict['Duration_real'][j], metric_dict['Duration_pred'][j], 0.3, 1.7, ax)\n",
    "        r2_score = fig_utils.linear_regression(\n",
    "            x_data, y_data, \n",
    "            0.3, 1.7, ax,\n",
    "            frameon=False, fontsize=8, loc=\"lower right\", bbox_to_anchor=(1.05, -0.05)\n",
    "        )\n",
    "        ax.plot(\n",
    "            np.mean(x_data), \n",
    "            np.mean(y_data), \n",
    "            linestyle=\"None\", marker='o', zorder=30,\n",
    "            markersize=6, markeredgewidth=0.0, color=viz.PALETTE['red'])\n",
    "        # print(dataset_str, \"max\",metric_dict['Duration_real'][j].max(), \"prct\", np.percentile(metric_dict['Duration_real'][j], 98))\n",
    "        if i == 1:\n",
    "            axes[i, j].set_xlabel(\"Duración real (s)\", fontsize=8)\n",
    "        letters_selected = letters if i == 0 else letters2\n",
    "        ax.text(\n",
    "            x=-0.01, y=1.15, fontsize=16, s=r\"$\\bf{%s}$\" % letters_selected[j],\n",
    "            ha=\"left\", transform=ax.transAxes)\n",
    "        # Print quantitative data\n",
    "        error_data = y_data - x_data  #  pred - real\n",
    "        print(\"%s & %s & %1.2f & %1.4f (s) & %1.4f (s) \\\\\\\\\" % (\n",
    "            dataset_str, model_str, r2_score, np.mean(error_data), np.std(error_data)\n",
    "        ))\n",
    "            \n",
    "    axes[i, 0].set_ylabel(\"Duración predicha (s)\", fontsize=8)\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_durations\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters: By-event parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Retrieve parameters, all events together\n",
    "    stat_spindle = (config[\"dataset_name\"] == constants.MODA_SS_NAME)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {\n",
    "        'Detector': [],\n",
    "        'Duration': [], \n",
    "        'AmplitudePP': [],\n",
    "        'Frequency': [],\n",
    "    }\n",
    "    # Insert GS first\n",
    "    tmp_table = {\n",
    "        'Duration': [], \n",
    "        'AmplitudePP': [],\n",
    "        'Frequency': [],\n",
    "    }\n",
    "    for k in range(n_folds):\n",
    "        subject_ids = test_ids_list[k]\n",
    "        feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "        events_list = feed_d.get_stamps()\n",
    "        for i_sub, subject_id in enumerate(subject_ids):\n",
    "            events = events_list[i_sub]\n",
    "            if events.size == 0:\n",
    "                continue\n",
    "            # Duration\n",
    "            duration = (events[:, 1] - events[:, 0] + 1) / dataset.fs\n",
    "            tmp_table['Duration'].append(duration)\n",
    "            # Amplitude\n",
    "            signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "            event_name = 'spindle' if stat_spindle else 'kcomplex'\n",
    "            filt_signal = param_filtering_fn(signal, dataset.fs, event_name)\n",
    "            signal_events = [filt_signal[e[0]:e[1]+1] for e in events]\n",
    "            amplitude = np.array([param_amplitude_fn(s, dataset.fs, event_name) for s in signal_events])\n",
    "            tmp_table['AmplitudePP'].append(amplitude)\n",
    "            # Frequency\n",
    "            if stat_spindle:\n",
    "                freq_central = np.array([param_frequency_fn(s, dataset.fs) for s in signal_events])\n",
    "            else:\n",
    "                freq_central = np.array([1] * len(signal_events))\n",
    "            tmp_table['Frequency'].append(freq_central)\n",
    "    table['Detector'].append('Expert')\n",
    "    for key in tmp_table.keys():\n",
    "        table[key].append(np.concatenate(tmp_table[key]))\n",
    "        \n",
    "    # Now detectors\n",
    "    for model_name in pred_dict.keys():\n",
    "        tmp_table = {\n",
    "            'Duration': [], \n",
    "            'AmplitudePP': [],\n",
    "            'Frequency': [],\n",
    "        }\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            for i_sub, subject_id in enumerate(subject_ids):\n",
    "                detections = detections_list[i_sub]\n",
    "                if detections.size == 0:\n",
    "                    continue\n",
    "                # Duration\n",
    "                duration = (detections[:, 1] - detections[:, 0] + 1) / dataset.fs\n",
    "                tmp_table['Duration'].append(duration)\n",
    "                # Amplitude\n",
    "                signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "                event_name = 'spindle' if stat_spindle else 'kcomplex'\n",
    "                filt_signal = param_filtering_fn(signal, dataset.fs, event_name)\n",
    "                signal_events = [filt_signal[e[0]:e[1]+1] for e in detections]\n",
    "                amplitude = np.array([param_amplitude_fn(s, dataset.fs, event_name) for s in signal_events])\n",
    "                tmp_table['AmplitudePP'].append(amplitude)\n",
    "                # Frequency\n",
    "                if stat_spindle:\n",
    "                    freq_central = np.array([param_frequency_fn(s, dataset.fs) for s in signal_events])\n",
    "                else:\n",
    "                    freq_central = np.array([1] * len(signal_events))\n",
    "                tmp_table['Frequency'].append(freq_central) \n",
    "        table['Detector'].append(model_name)\n",
    "        for key in tmp_table.keys():\n",
    "            table[key].append(np.concatenate(tmp_table[key]))\n",
    "    metrics_list.append(table)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky',\n",
    "    'Expert': 'Anotación'\n",
    "}\n",
    "print_parameter = {\n",
    "    'Duration': 'Duración (s)', 'AmplitudePP': 'Amplitud PP ($\\mu$V)', 'Frequency': 'Frecuencia (Hz)'\n",
    "}\n",
    "\n",
    "min_values_forced = [0, 0, 9, 0, 0]\n",
    "max_values_forced = [3, 150, 17, 2, 600]\n",
    "major_ticks_forced = [[0, 1, 2, 3], [0, 50, 100, 150], [9, 11, 13, 15, 17], [0, 1, 2], [0, 200, 400, 600]]\n",
    "minor_ticks_step_forced = [0.5, 25, 1, 0.5, 100]\n",
    "\n",
    "markersize = 8\n",
    "baseline_color = viz.GREY_COLORS[6]\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "    'Expert': dict(marker='p', color=\"k\"),  # \"#F9A825\"\n",
    "}\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D', 'E']\n",
    "fig, axes = plt.subplots(1, 5, figsize=(8, 3), dpi=200)\n",
    "j = 0\n",
    "order_models_reference = ['Expert', constants.V2_TIME, constants.V2_CWT1D, 'dosed', 'a7', 'spinky']\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    metric_dict = metrics_list[i_config]\n",
    "    n_models = len(metric_dict['Detector'])\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    print(\"Processing\", dataset_str)\n",
    "    dpi = 200\n",
    "    if config[\"dataset_name\"] == constants.MODA_SS_NAME:\n",
    "        param_names = ['Duration', 'AmplitudePP', 'Frequency']\n",
    "    else:\n",
    "        param_names = ['Duration', 'AmplitudePP']\n",
    "    for param_name in param_names:\n",
    "        ax = axes[j]\n",
    "        model_to_data = {m: d for m, d in zip(metric_dict['Detector'], metric_dict[param_name])}\n",
    "        this_order = [m for m in order_models_reference if m in metric_dict['Detector']]\n",
    "        this_order = this_order[::-1]\n",
    "        x_data_list = [model_to_data[model_name] for model_name in this_order]\n",
    "        positions = np.arange(len(x_data_list))\n",
    "        parts = ax.violinplot(\n",
    "            x_data_list, vert=False, showextrema=False, positions=positions, widths=0.8\n",
    "        )\n",
    "        for i_pc, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(viz.GREY_COLORS[3])\n",
    "            pc.set_edgecolor(model_specs[this_order[i_pc]][\"color\"])\n",
    "            pc.set_linewidth(0.7)\n",
    "            pc.set_alpha(1.0)\n",
    "            pc.set_zorder(20)\n",
    "        for i_data, x_data in enumerate(x_data_list):\n",
    "            mean_val = x_data.mean()\n",
    "            model_name = this_order[i_data]\n",
    "            ax.plot(\n",
    "                mean_val, i_data, marker=\"x\",#model_specs[model_name][\"marker\"], \n",
    "                color=model_specs[model_name][\"color\"], linestyle=\"None\", zorder=30,\n",
    "                markersize=4)\n",
    "            low_iqr, high_iqr = np.percentile(x_data, (25, 75))\n",
    "            ax.plot(\n",
    "                [low_iqr, high_iqr], [i_data, i_data], linewidth=0.8, zorder=30,\n",
    "                color=model_specs[model_name][\"color\"])\n",
    "            # ax.plot([mean_val, mean_val], [i_data + 0.4, i_data - 0.4], linewidth=1, color=model_specs[this_order[i_data]][\"color\"])\n",
    "            if this_order[i_data] == 'Expert':\n",
    "                ax.axvline(mean_val, linewidth=0.8, alpha=1.0, color=model_specs[model_name][\"color\"], zorder=35, linestyle=\"--\")\n",
    "                print(\"Param %s, Detector %s, Min %1.4f, Prctl33 %1.4f, prctl50 %1.4f, Prctl66 %1.4f, Max %1.4f\" % (\n",
    "                    param_name, model_name, x_data.min(), np.percentile(x_data, 33), np.percentile(x_data, 50), np.percentile(x_data, 66), x_data.max()\n",
    "                ))\n",
    "\n",
    "        min_pos_true = min_values_forced[j]\n",
    "        max_pos_true = max_values_forced[j]\n",
    "        for i_model, model_name in enumerate(this_order):\n",
    "            ax.plot(\n",
    "                min_pos_true - 0.1 * (max_pos_true - min_pos_true), i_model, \n",
    "                markersize=markersize, color=model_specs[model_name][\"color\"], zorder=30, markeredgewidth=0.0,\n",
    "                label=print_model_names[model_name],\n",
    "                marker=model_specs[model_name][\"marker\"], linestyle=\"None\")\n",
    "        min_pos_lim = min_pos_true - 0.2 * (max_pos_true - min_pos_true)\n",
    "        ax.set_xlim([min_pos_lim, max_pos_true])\n",
    "        ax.set_xticks(major_ticks_forced[j])\n",
    "        ax.set_xticks(\n",
    "            np.arange(\n",
    "                major_ticks_forced[j][0], \n",
    "                major_ticks_forced[j][-1]+0.001, \n",
    "                minor_ticks_step_forced[j]\n",
    "            ), minor=True)\n",
    "        ax.set_title(dataset_str, fontsize=8, loc=\"left\")\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(print_parameter[param_name], fontsize=8)\n",
    "        ax.text(\n",
    "            x=-0.01, y=1.12, fontsize=16, s=r\"$\\bf{%s}$\" % letters[j],\n",
    "            ha=\"left\", transform=ax.transAxes)\n",
    "        ax.xaxis.set_label_coords(0.6, -0.12)\n",
    "        j = j + 1\n",
    "        ax.grid(axis=\"x\", which=\"minor\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"Anotación\", \"REDv2-Time\", \"REDv2-CWT\", \"DOSED\", \"A7\", \"Spinky\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=7, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.05), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_byevent_params\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters: By-subject parameters\n",
    "[5CV only, MODA y MASS-KC, by-subject, pintar por fase] parámetro experto vs modelo, mostrando ajuste lineal y R2: duracion promedio, densidad promedio, amplitud PP promedio, y PR (maxSigma/broadNoDelta) promedio (only SS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Retrieve by subject parameters (MODA only if 10 blocks)\n",
    "    if config[\"dataset_name\"] == constants.MODA_SS_NAME:\n",
    "        valid_subjects = [sub_id for sub_id in dataset.all_ids if dataset.data[sub_id]['n_blocks'] == 10]\n",
    "        phase_subjects = {sub_id: dataset.data[sub_id]['phase'] for sub_id in valid_subjects}\n",
    "        stat_spindle = True\n",
    "    else:\n",
    "        valid_subjects = dataset.all_ids\n",
    "        phase_subjects = {sub_id: 1 for sub_id in valid_subjects}\n",
    "        stat_spindle = False\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {\n",
    "        'Detector': [], \n",
    "        'Phase': [],\n",
    "        'Duration_mean_real': [], \n",
    "        'Duration_mean_pred': [], \n",
    "        'Density_real': [],\n",
    "        'Density_pred': [],\n",
    "        'AmplitudePP_mean_real': [],\n",
    "        'AmplitudePP_mean_pred': [],\n",
    "        'Frequency_mean_real': [],\n",
    "        'Frequency_mean_pred': [],\n",
    "    }\n",
    "    for model_name in pred_dict.keys():\n",
    "        tmp_table = {\n",
    "            'Phase': [],\n",
    "            'Duration_mean_real': [], \n",
    "            'Duration_mean_pred': [], \n",
    "            'Density_real': [],\n",
    "            'Density_pred': [],\n",
    "            'AmplitudePP_mean_real': [],\n",
    "            'AmplitudePP_mean_pred': [],\n",
    "            'Frequency_mean_real': [],\n",
    "            'Frequency_mean_pred': [],\n",
    "        }\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            for i_sub, subject_id in enumerate(subject_ids):\n",
    "                if subject_id not in valid_subjects:\n",
    "                    continue\n",
    "                events = events_list[i_sub]\n",
    "                detections = detections_list[i_sub]\n",
    "                if events.size * detections.size == 0:\n",
    "                    print(\"Skipped subject,\", config[\"dataset_name\"], model_name, \"Events shape\", events.shape, \"Detections shape\", detections.shape)\n",
    "                    continue\n",
    "                if events.shape[0] < 10:\n",
    "                    print(\"Skipped subject\", subject_id, \"due to too few spindles, events:\", events.shape)\n",
    "                    continue\n",
    "                    \n",
    "                tmp_table['Phase'].append(phase_subjects[subject_id])\n",
    "                    \n",
    "                # Duration\n",
    "                duration_real = np.mean((events[:, 1] - events[:, 0] + 1) / dataset.fs)\n",
    "                duration_pred = np.mean((detections[:, 1] - detections[:, 0] + 1) / dataset.fs)\n",
    "                tmp_table['Duration_mean_real'].append(duration_real)\n",
    "                tmp_table['Duration_mean_pred'].append(duration_pred)\n",
    "                \n",
    "                # Density\n",
    "                n2_pages = dataset.get_subject_pages(subject_id, pages_subset=constants.N2_RECORD)\n",
    "                n2_minutes = n2_pages.size * dataset.page_duration / 60\n",
    "                density_real = events.shape[0] / n2_minutes\n",
    "                density_pred = detections.shape[0] / n2_minutes\n",
    "                tmp_table['Density_real'].append(density_real)\n",
    "                tmp_table['Density_pred'].append(density_pred)\n",
    "                \n",
    "                # Amplitude\n",
    "                signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "                event_name = 'spindle' if stat_spindle else 'kcomplex'\n",
    "                filt_signal = param_filtering_fn(signal, dataset.fs, event_name)\n",
    "                signal_events = [filt_signal[e[0]:e[1]+1] for e in events]\n",
    "                signal_detections = [filt_signal[e[0]:e[1]+1] for e in detections]\n",
    "                amplitude_real = np.mean([param_amplitude_fn(s, dataset.fs, event_name) for s in signal_events])\n",
    "                amplitude_pred = np.mean([param_amplitude_fn(s, dataset.fs, event_name) for s in signal_detections])\n",
    "                tmp_table['AmplitudePP_mean_real'].append(amplitude_real)\n",
    "                tmp_table['AmplitudePP_mean_pred'].append(amplitude_pred)\n",
    "                \n",
    "                # Frequency\n",
    "                if config[\"dataset_name\"] == constants.MODA_SS_NAME:\n",
    "                    freq_real = np.array([param_frequency_fn(s, dataset.fs) for s in signal_events]).mean()\n",
    "                    freq_pred = np.array([param_frequency_fn(s, dataset.fs) for s in signal_detections]).mean()\n",
    "                else:\n",
    "                    freq_real = 1\n",
    "                    freq_pred = 1\n",
    "                #if np.abs(freq_real - freq_pred) > 0.5:\n",
    "                    #print(config[\"dataset_name\"], model_name, subject_id, phase_subjects[subject_id], events.shape, freq_real, detections.shape, freq_pred)\n",
    "                tmp_table['Frequency_mean_real'].append(freq_real)\n",
    "                tmp_table['Frequency_mean_pred'].append(freq_pred)\n",
    "        table['Detector'].append(model_name)\n",
    "        for key in tmp_table.keys():\n",
    "            table[key].append(tmp_table[key])\n",
    "    metrics_list.append(table)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "scatter_alpha = 0.5\n",
    "markersize = 3\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "letters = ['A', 'B', 'C', 'D']\n",
    "letters2 = ['E', 'F', 'G', 'H']\n",
    "letters3 = ['I', 'J', 'K', 'L']\n",
    "letters4 = ['M', 'N', 'O', 'P']\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "print_name = {\n",
    "    'Duration_mean': 'Duración', 'Density': 'Densidad', 'AmplitudePP_mean': 'Amplitud PP', 'Frequency_mean': 'Frecuencia'\n",
    "}\n",
    "units = {\n",
    "    'Duration_mean': 's', 'Density': 'epm', 'AmplitudePP_mean': '$\\mu$V', 'Frequency_mean': 'Hz'\n",
    "}\n",
    "decimals = {\n",
    "    'Duration_mean': 1, 'Density': 0, 'AmplitudePP_mean': -1, 'Frequency_mean': 0\n",
    "}\n",
    "resolutions = {\n",
    "    'Duration_mean': 0.1, 'Density': 1, 'AmplitudePP_mean': 10, 'Frequency_mean': 0.5\n",
    "}\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    metric_dict = metrics_list[i_config]\n",
    "    n_models = len(metric_dict['Detector'])\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    print(\"Processing\", dataset_str)\n",
    "    dpi = 200\n",
    "    if config[\"dataset_name\"] == constants.MODA_SS_NAME:\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(8, 9), dpi=dpi)\n",
    "        param_names = ['Duration_mean', 'Density', 'AmplitudePP_mean', 'Frequency_mean']\n",
    "    else:\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(8, 7), dpi=dpi)\n",
    "        param_names = ['Duration_mean', 'Density', 'AmplitudePP_mean']\n",
    "    for i, param_name in enumerate(param_names):\n",
    "        # Find range\n",
    "        min_val = 1000\n",
    "        max_val = 0\n",
    "        for j in range(n_models):\n",
    "            x_data = np.array(metric_dict['%s_real' % param_name][j])\n",
    "            y_data = np.array(metric_dict['%s_pred' % param_name][j])\n",
    "            joint_data = np.concatenate([x_data, y_data])\n",
    "            min_val = min(min_val, joint_data.min())\n",
    "            max_val = max(max_val, joint_data.max())\n",
    "        range_width = max_val - min_val\n",
    "        min_val = max(0, min_val - 0.1 * range_width)\n",
    "        max_val = max_val + 0.1 * range_width\n",
    "        min_val = np.around(min_val, decimals=decimals[param_name])\n",
    "        max_val = np.around(max_val, decimals=decimals[param_name])\n",
    "        if param_name == 'Frequency_mean':\n",
    "            min_val = 11\n",
    "            max_val = 15\n",
    "        # print(param_name, min_val, max_val)\n",
    "        if config[\"dataset_name\"] == constants.MODA_SS_NAME and param_name == 'AmplitudePP_mean':\n",
    "            this_resolution = resolutions[param_name] / 2\n",
    "        elif config[\"dataset_name\"] == constants.MASS_KC_NAME and param_name == 'Density':\n",
    "            this_resolution = resolutions[param_name] / 2\n",
    "        else:\n",
    "            this_resolution = resolutions[param_name]\n",
    "        minor_ticks = np.arange(min_val, max_val + 0.001, this_resolution)\n",
    "        major_ticks = [min_val, max_val]\n",
    "        \n",
    "        for j in range(n_models):\n",
    "            ax = axes[i, j]\n",
    "            model_name = metric_dict['Detector'][j]\n",
    "            x_data = np.array(metric_dict['%s_real' % param_name][j])\n",
    "            y_data = np.array(metric_dict['%s_pred' % param_name][j])\n",
    "            ax.plot(\n",
    "                x_data, y_data, linestyle=\"None\", marker='o', \n",
    "                markersize=markersize, alpha=scatter_alpha, markeredgewidth=0.0, color=viz.PALETTE['blue'])\n",
    "            ax.plot(\n",
    "                np.mean(x_data), np.mean(y_data), linestyle=\"None\", marker='o', zorder=30,\n",
    "                markersize=2*markersize, markeredgewidth=0.0, color=viz.PALETTE['red'])\n",
    "            model_str = print_model_names[model_name]\n",
    "            ax.set_title('%s' % model_str, loc=\"left\", fontsize=8)\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xlim([min_val, max_val])\n",
    "            ax.set_ylim([min_val, max_val])\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], color=viz.GREY_COLORS[4], linewidth=0.7, zorder=5)\n",
    "            ax.set_xlabel(\"%s real (%s)\" % (print_name[param_name], units[param_name]), fontsize=8)\n",
    "            ax.set_xticks(major_ticks)\n",
    "            ax.set_xticks(minor_ticks, minor=True)\n",
    "            ax.set_yticks(major_ticks)\n",
    "            ax.set_yticks(minor_ticks, minor=True)\n",
    "            ax.grid(which=\"minor\")\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(\"%s pred. (%s)\" % (print_name[param_name], units[param_name]), fontsize=8)\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            ax.xaxis.labelpad = -7\n",
    "            if config[\"dataset_name\"] == constants.MODA_SS_NAME and param_name == 'Duration_mean':\n",
    "                ax.yaxis.labelpad = -9.5\n",
    "            elif config[\"dataset_name\"] == constants.MODA_SS_NAME and param_name == 'AmplitudePP_mean':\n",
    "                ax.yaxis.labelpad = -9\n",
    "            elif config[\"dataset_name\"] == constants.MASS_KC_NAME and param_name == 'AmplitudePP_mean':\n",
    "                ax.yaxis.labelpad = -12\n",
    "            elif config[\"dataset_name\"] == constants.MASS_KC_NAME and param_name == 'Density':\n",
    "                ax.yaxis.labelpad = -1\n",
    "            elif config[\"dataset_name\"] == constants.MASS_KC_NAME and param_name == 'Duration_mean':\n",
    "                ax.yaxis.labelpad = -8.5\n",
    "            else:\n",
    "                ax.yaxis.labelpad = -7\n",
    "            new_range = max_val - min_val\n",
    "            r2_score = fig_utils.linear_regression(\n",
    "                x_data, y_data, min_val + 0.1*new_range, max_val-0.1*new_range, ax,\n",
    "                frameon=False, fontsize=8, loc=\"lower right\", bbox_to_anchor=(1.05, -0.05)\n",
    "            )\n",
    "            letters_selected = [letters, letters2, letters3, letters4][i]\n",
    "            ax.text(\n",
    "                x=-0.01, y=1.15, fontsize=16, s=r\"$\\bf{%s}$\" % letters_selected[j],\n",
    "                ha=\"left\", transform=ax.transAxes)\n",
    "            # Print quantitative data\n",
    "            error_data = y_data - x_data  # pred - real\n",
    "            print(\"%s & %s & %s & %1.2f & %1.4f & %1.2f \\\\\\\\\" % (\n",
    "                dataset_str, print_name[param_name], model_str,\n",
    "                r2_score,\n",
    "                np.mean(error_data),\n",
    "                np.std(error_data),\n",
    "            ))\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figure:\n",
    "        # Save figure\n",
    "        fname_prefix = \"result_comparison_bysubject_%s\" % config[\"dataset_name\"]\n",
    "        plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "        plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "        plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgroup performance analysis\n",
    "[5CV only, MODA y MASS-KC, by-fold ambos micro] Desempeño en subgrupos: \n",
    "\n",
    "Husos (MODA)\n",
    "- Duración: <0.6, 0.6-0.9, >0.9 (s)\n",
    "- Amplitud PP: <30, 30-40, >40 (muV)\n",
    "- Fase: 1 o 2.\n",
    "- Frecuencia: <13, >13 (Hz).\n",
    "- ¿PR? (sigma/broadNoDelta)\n",
    "\n",
    "Complejos K (MASS-KC):\n",
    "- Duración: <0.65, 0.65-0.80, >0.80 (s) (Ojo aqui con spinky, ya que todos sus FP quedarian en el ultimo grupo)\n",
    "- Amplitud PP: <110, 110-160, >160 (muV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_mark():\n",
    "    return np.zeros((0, 2), dtype=np.int32)\n",
    "\n",
    "\n",
    "def get_mark_rows(marks, indices):\n",
    "    if len(indices) == 0:\n",
    "        return empty_mark()\n",
    "    else:\n",
    "        return marks[indices]\n",
    "    \n",
    "    \n",
    "def sort_by_group(subgroup_dict, feature, group_bins, group_names, e=None, d=None):\n",
    "    n_groups = len(group_bins) - 1\n",
    "    for i in range(n_groups):\n",
    "        locs = np.where((feature > group_bins[i]) & (feature <= group_bins[i + 1]))[0]\n",
    "        if e is None:\n",
    "            group_e = empty_mark()\n",
    "        else:\n",
    "            group_e = get_mark_rows(e, locs)\n",
    "        if d is None:\n",
    "            group_d = empty_mark()\n",
    "        else:\n",
    "            group_d = get_mark_rows(d, locs)\n",
    "        group_name = group_names[i]\n",
    "        subgroup_dict[group_name]['events_list'].append(group_e)\n",
    "        subgroup_dict[group_name]['detections_list'].append(group_d)\n",
    "    return subgroup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines_ss = ['dosed', 'a7']\n",
    "baselines_kc = ['dosed', 'spinky']\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    baselines = baselines_ss if dataset.event_name == constants.SPINDLE else baselines_kc\n",
    "    \n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {}\n",
    "        for k in tmp_dict.keys():\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    for baseline_name in baselines:\n",
    "        pred_dict[baseline_name] = fig_utils.get_baseline_predictions(\n",
    "            baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"])\n",
    "    # print(\"Loaded models:\", pred_dict.keys())\n",
    "    \n",
    "    # Compute subgroup metrics, always using micro-average\n",
    "    average_mode = constants.MICRO_AVERAGE\n",
    "    stat_spindle = (config[\"dataset_name\"] == constants.MODA_SS_NAME)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    table = {\n",
    "        'Detector': [],\n",
    "        'Subgroup': [],\n",
    "        'F1-score': [], \n",
    "        'Recall': [], \n",
    "        'Precision': [], \n",
    "        'mIoU': [], \n",
    "        'Fold': []\n",
    "    }\n",
    "    for model_name in pred_dict.keys():\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "            events_list = feed_d.get_stamps()\n",
    "            detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "            _, idx_matching_list = metrics.matching_with_list(events_list, detections_list)\n",
    "            sg_names = [\n",
    "                'Duration_1', \n",
    "                'Duration_2', \n",
    "                'Duration_3',\n",
    "                'Amplitude_1',\n",
    "                'Amplitude_2',\n",
    "                'Amplitude_3',\n",
    "            ]\n",
    "            if stat_spindle:\n",
    "                sg_names.extend([\n",
    "                    'Frequency_1',\n",
    "                    'Frequency_2',\n",
    "                    'Frequency_3',\n",
    "                    'Phase_1',\n",
    "                    'Phase_2'\n",
    "                ])\n",
    "            subgroups = {\n",
    "                s_name: {'events_list': [], 'detections_list': []}\n",
    "                for s_name in sg_names}\n",
    "            for i in range(len(subject_ids)):\n",
    "                events = events_list[i]\n",
    "                detections = detections_list[i]\n",
    "                idx_matching = idx_matching_list[i]\n",
    "                subject_id = subject_ids[i]\n",
    "                # Locs matched pairs: to be assigned to the same subgroup (the one of the event)\n",
    "                events_tp_loc = np.where(idx_matching != -1)[0]\n",
    "                detections_tp_loc = idx_matching[events_tp_loc]\n",
    "                # Locs unmatched events/detections\n",
    "                events_fn_loc = np.array([\n",
    "                    loc for loc in range(events.shape[0]) \n",
    "                    if loc not in events_tp_loc])\n",
    "                detections_fp_loc = np.array([\n",
    "                    loc for loc in range(detections.shape[0]) \n",
    "                    if loc not in detections_tp_loc])\n",
    "                # Retrieve marks\n",
    "                events_tp = get_mark_rows(events, events_tp_loc)\n",
    "                detections_tp = get_mark_rows(detections, detections_tp_loc)\n",
    "                events_fn = get_mark_rows(events, events_fn_loc)\n",
    "                detections_fp = get_mark_rows(detections, detections_fp_loc)\n",
    "                \n",
    "                # Duration subgroups\n",
    "                duration_bins = [0, 0.6, 0.9, 10] if stat_spindle else [0, 0.65, 0.80, 10]\n",
    "                if events_tp.size > 0:\n",
    "                    durations = (events_tp[:, 1] - events_tp[:, 0] + 1) / dataset.fs\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, durations, duration_bins, ['Duration_1', 'Duration_2', 'Duration_3'], \n",
    "                        e=events_tp, d=detections_tp)\n",
    "                if events_fn.size > 0:\n",
    "                    durations = (events_fn[:, 1] - events_fn[:, 0] + 1) / dataset.fs\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, durations, duration_bins, ['Duration_1', 'Duration_2', 'Duration_3'], \n",
    "                        e=events_fn, d=None)\n",
    "                if detections_fp.size > 0:\n",
    "                    durations = (detections_fp[:, 1] - detections_fp[:, 0] + 1) / dataset.fs\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, durations, duration_bins, ['Duration_1', 'Duration_2', 'Duration_3'], \n",
    "                        e=None, d=detections_fp)\n",
    "                \n",
    "                # Amplitude subgroups\n",
    "                amplitude_bins = [0, 30, 40, 1000] if stat_spindle else [0, 110, 160, 1000]\n",
    "                signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "                event_name = 'spindle' if stat_spindle else 'kcomplex'\n",
    "                filt_signal = param_filtering_fn(signal, dataset.fs, event_name)\n",
    "                if events_tp.size > 0:\n",
    "                    tmp_segments = [filt_signal[e[0]:e[1]+1] for e in events_tp]\n",
    "                    amplitudes = np.array([param_amplitude_fn(s, dataset.fs, event_name) for s in tmp_segments])\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, amplitudes, amplitude_bins, ['Amplitude_1', 'Amplitude_2', 'Amplitude_3'], \n",
    "                        e=events_tp, d=detections_tp)\n",
    "                if events_fn.size > 0:\n",
    "                    tmp_segments = [filt_signal[e[0]:e[1]+1] for e in events_fn]\n",
    "                    amplitudes = np.array([param_amplitude_fn(s, dataset.fs, event_name) for s in tmp_segments])\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, amplitudes, amplitude_bins, ['Amplitude_1', 'Amplitude_2', 'Amplitude_3'], \n",
    "                        e=events_fn, d=None)\n",
    "                if detections_fp.size > 0:\n",
    "                    tmp_segments = [filt_signal[e[0]:e[1]+1] for e in detections_fp]\n",
    "                    amplitudes = np.array([param_amplitude_fn(s, dataset.fs, event_name) for s in tmp_segments])\n",
    "                    subgroups = sort_by_group(\n",
    "                        subgroups, amplitudes, amplitude_bins, ['Amplitude_1', 'Amplitude_2', 'Amplitude_3'], \n",
    "                        e=None, d=detections_fp)\n",
    "                \n",
    "                # SS specific subgroups\n",
    "                if stat_spindle:\n",
    "                    \n",
    "                    # Frequency subgroups\n",
    "                    frequency_bins = [0, 12.8, 13.5, 100]\n",
    "                    if events_tp.size > 0:\n",
    "                        tmp_segments = [filt_signal[e[0]:e[1]+1] for e in events_tp]\n",
    "                        frequencies = np.array([param_frequency_fn(s, dataset.fs) for s in tmp_segments])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, frequencies, frequency_bins, ['Frequency_1', 'Frequency_2', 'Frequency_3'], \n",
    "                            e=events_tp, d=detections_tp)\n",
    "                    if events_fn.size > 0:\n",
    "                        tmp_segments = [filt_signal[e[0]:e[1]+1] for e in events_fn]\n",
    "                        frequencies = np.array([param_frequency_fn(s, dataset.fs) for s in tmp_segments])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, frequencies, frequency_bins, ['Frequency_1', 'Frequency_2', 'Frequency_3'], \n",
    "                            e=events_fn, d=None)\n",
    "                    if detections_fp.size > 0:\n",
    "                        tmp_segments = [filt_signal[e[0]:e[1]+1] for e in detections_fp]\n",
    "                        frequencies = np.array([param_frequency_fn(s, dataset.fs) for s in tmp_segments])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, frequencies, frequency_bins, ['Frequency_1', 'Frequency_2', 'Frequency_3'], \n",
    "                            e=None, d=detections_fp)\n",
    "                \n",
    "                    # Phase subgroups\n",
    "                    phase_bins = [0, 1.5, 3]\n",
    "                    phase_subject = dataset.data[subject_id]['phase']\n",
    "                    if events_tp.size > 0:\n",
    "                        phases = np.array([phase_subject] * events_tp.shape[0])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, phases, phase_bins, ['Phase_1', 'Phase_2'], \n",
    "                            e=events_tp, d=detections_tp)\n",
    "                    if events_fn.size > 0:\n",
    "                        phases = np.array([phase_subject] * events_fn.shape[0])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, phases, phase_bins, ['Phase_1', 'Phase_2'], \n",
    "                            e=events_fn, d=None)\n",
    "                    if detections_fp.size > 0:\n",
    "                        phases = np.array([phase_subject] * detections_fp.shape[0])\n",
    "                        subgroups = sort_by_group(\n",
    "                            subgroups, phases, phase_bins, ['Phase_1', 'Phase_2'], \n",
    "                            e=None, d=detections_fp)\n",
    "                \n",
    "            # Compute fold performance\n",
    "            for subgroup_name in subgroups.keys():\n",
    "                s_events_list = subgroups[subgroup_name]['events_list']\n",
    "                s_detections_list = subgroups[subgroup_name]['detections_list']\n",
    "                performance = fig_utils.compute_fold_performance(s_events_list, s_detections_list, average_mode)\n",
    "                table['Detector'].append(model_name)\n",
    "                table['Subgroup'].append(subgroup_name)\n",
    "                table['F1-score'].append(performance['F1-score'])\n",
    "                table['Recall'].append(performance['Recall'])\n",
    "                table['Precision'].append(performance['Precision'])\n",
    "                table['mIoU'].append(performance['mIoU'])\n",
    "                table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    mean_table = table.groupby(by=['Detector', 'Subgroup']).mean().drop(columns=[\"Fold\"]).add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=['Detector', 'Subgroup']).std(ddof=0).drop(columns=[\"Fold\"]).add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate table for latex\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D, 'dosed', 'a7', 'spinky']\n",
    "pretty_sg_names_ss = {\n",
    "    # durations\n",
    "    'Duration_1': '($-\\infty$, 0.6]',\n",
    "    'Duration_2': '(0.6, 0.9]',\n",
    "    'Duration_3': '(0.9, $+\\infty$)',\n",
    "    # amplitudes\n",
    "    'Amplitude_1': '($-\\infty$, 30]',\n",
    "    'Amplitude_2': '(30, 40]',\n",
    "    'Amplitude_3': '(40, $+\\infty$)',\n",
    "    # frequencies\n",
    "    'Frequency_1': '($-\\infty$, 12.8]',\n",
    "    'Frequency_2': '(12.8, 13.5]',\n",
    "    'Frequency_3': '(13.5, $+\\infty$)',\n",
    "    # phases\n",
    "    'Phase_1': 'Jóvenes',\n",
    "    'Phase_2': 'Viejos'\n",
    "}\n",
    "pretty_sg_names_kc ={\n",
    "    # durations\n",
    "    'Duration_1': '($-\\infty$, 0.65]',\n",
    "    'Duration_2': '(0.65, 0.8]',\n",
    "    'Duration_3': '(0.8, $+\\infty$)',\n",
    "    # amplitudes\n",
    "    'Amplitude_1': '($-\\infty$, 110]',\n",
    "    'Amplitude_2': '(110, 160]',\n",
    "    'Amplitude_3': '(160, $+\\infty$)',\n",
    "}\n",
    "sg_prefix_order_ss = ['Duration', 'Amplitude', 'Frequency', 'Phase']\n",
    "sg_prefix_order_kc = ['Duration', 'Amplitude']\n",
    "sg_labels = {\n",
    "    'Duration': 'Duración (s)',\n",
    "    'Amplitude': 'Amplitud PP ($\\mu$V)',\n",
    "    'Frequency': 'Frecuencia (Hz)',\n",
    "    'Phase': 'Edad'\n",
    "}\n",
    "\n",
    "mode_2 = True\n",
    "\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]\n",
    "    stat_spindle = config['dataset_name'] == constants.MODA_SS_NAME \n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    n_models = len(model_names)\n",
    "    pretty_sg_names = pretty_sg_names_ss if stat_spindle else pretty_sg_names_kc\n",
    "    sg_prefix_order = sg_prefix_order_ss if stat_spindle else sg_prefix_order_kc\n",
    "    table = table[['Detector', 'Subgroup', 'F1-score_mean', 'F1-score_std']]\n",
    "    for sg_prefix in sg_prefix_order:\n",
    "        sg_names = [n for n in pretty_sg_names.keys() if sg_prefix in n]\n",
    "        sg_names.sort()\n",
    "        \n",
    "        if not mode_2:\n",
    "            # Start mini table\n",
    "            print(\"\\n%s, %s\" % (dataset_str, sg_labels[sg_prefix]))\n",
    "            header = \" & \".join([pretty_sg_names[sg_name] for sg_name in sg_names])\n",
    "            header = \"           & %s \\\\\\\\\" % header\n",
    "            print(header)\n",
    "            for model_name in model_names:\n",
    "                model_table = table[(table['Detector'] == model_name) & (np.isin(table['Subgroup'], sg_names))].drop(columns=['Detector'])\n",
    "                model_table = model_table.set_index('Subgroup')\n",
    "                metrics_dict = model_table.to_dict('index')\n",
    "                metric_str_list = []\n",
    "                for sg_name in sg_names:\n",
    "                    metric_str = '$%1.1f\\pm %1.1f$' % (\n",
    "                        100 * metrics_dict[sg_name]['F1-score_mean'], 100 * metrics_dict[sg_name]['F1-score_std'])\n",
    "                    metric_str_list.append(metric_str)\n",
    "                metric_str = \" & \".join(metric_str_list)\n",
    "                metric_str = \"%s & %s \\\\\\\\\" % (print_model_names[model_name].ljust(10), metric_str)\n",
    "                print(metric_str)\n",
    "        else:\n",
    "            print(\"\\n%s\" % dataset_str)\n",
    "            header = \" & \".join([print_model_names[model_name] for model_name in model_names])\n",
    "            header = \"Parámetro & Subconjunto & %s \\\\\\\\\" % header\n",
    "            print(header)\n",
    "            for sg_name in sg_names:\n",
    "                metric_str_list = []\n",
    "                for model_name in model_names:\n",
    "                    model_table = table[(table['Detector'] == model_name) & (np.isin(table['Subgroup'], sg_names))].drop(columns=['Detector'])\n",
    "                    model_table = model_table.set_index('Subgroup')\n",
    "                    metrics_dict = model_table.to_dict('index')\n",
    "                    metric_str = '$%1.1f\\pm %1.1f$' % (\n",
    "                        100 * metrics_dict[sg_name]['F1-score_mean'], 100 * metrics_dict[sg_name]['F1-score_std'])\n",
    "                    metric_str_list.append(metric_str)\n",
    "                metric_str = \" & \".join(metric_str_list)\n",
    "                metric_str = \"%s & %s & %s \\\\\\\\\" % (\n",
    "                    sg_labels[sg_prefix], pretty_sg_names[sg_name].ljust(20), metric_str)\n",
    "                print(metric_str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "markersize = 5\n",
    "number_of_std = 1\n",
    "groups_total_width = 0.5\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D, 'dosed', 'a7', 'spinky']\n",
    "pretty_sg_names_ss = {\n",
    "    # durations\n",
    "    'Duration_1': '($-\\infty$, 0.6]',\n",
    "    'Duration_2': '(0.6, 0.9]',\n",
    "    'Duration_3': '(0.9, $+\\infty$)',\n",
    "    # amplitudes\n",
    "    'Amplitude_1': '($-\\infty$, 30]',\n",
    "    'Amplitude_2': '(30, 40]',\n",
    "    'Amplitude_3': '(40, $+\\infty$)',\n",
    "    # frequencies\n",
    "    'Frequency_1': '($-\\infty$, 12.8]',\n",
    "    'Frequency_2': '(12.8, 13.5]',\n",
    "    'Frequency_3': '(13.5, $+\\infty$)',\n",
    "    # phases\n",
    "    'Phase_1': 'Jóvenes',\n",
    "    'Phase_2': 'Viejos'\n",
    "}\n",
    "pretty_sg_names_kc ={\n",
    "    # durations\n",
    "    'Duration_1': '($-\\infty$, 0.65]',\n",
    "    'Duration_2': '(0.65, 0.8]',\n",
    "    'Duration_3': '(0.8, $+\\infty$)',\n",
    "    # amplitudes\n",
    "    'Amplitude_1': '($-\\infty$, 110]',\n",
    "    'Amplitude_2': '(110, 160]',\n",
    "    'Amplitude_3': '(160, $+\\infty$)',\n",
    "}\n",
    "sg_prefix_order_ss = ['Duration', 'Amplitude', 'Frequency', 'Phase']\n",
    "sg_prefix_order_kc = ['Duration', 'Amplitude']\n",
    "sg_labels = {\n",
    "    'Duration': 'Duración (s)',\n",
    "    'Amplitude': 'Amplitud PP ($\\mu$V)',\n",
    "    'Frequency': 'Frecuencia (Hz)',\n",
    "    'Phase': 'Edad'\n",
    "}\n",
    "metrics_sorted = ['F1-score', 'Recall', 'Precision', 'mIoU']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 9), dpi=200)\n",
    "axes = np.concatenate([axes[:1, :], axes[1:, :]], axis=1) \n",
    "axes = axes.flatten()\n",
    "\n",
    "ax_loc = -1  # global loc\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    stat_spindle = config['dataset_name'] == constants.MODA_SS_NAME \n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    n_models = len(model_names)\n",
    "    pretty_sg_names = pretty_sg_names_ss if stat_spindle else pretty_sg_names_kc\n",
    "    sg_prefix_order = sg_prefix_order_ss if stat_spindle else sg_prefix_order_kc\n",
    "    for sg_prefix in sg_prefix_order:\n",
    "        ax_loc += 1\n",
    "        ax = axes[ax_loc]\n",
    "        sg_names = [n for n in pretty_sg_names.keys() if sg_prefix in n]\n",
    "        sg_names.sort()\n",
    "    \n",
    "        n_groups = len(sg_names)\n",
    "        positions = np.arange(n_groups)\n",
    "        groups_width = groups_total_width / n_models\n",
    "        initial_group_center_offset = groups_width * (1 - n_models) / 2\n",
    "        offsets = initial_group_center_offset + np.arange(n_models) * groups_width\n",
    "        distance_from_edge = groups_total_width / 2 + 0.05\n",
    "\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            if model_name == 'spinky' and sg_prefix == 'Duration':\n",
    "                continue\n",
    "            model_table = table[(table['Detector'] == model_name) & (np.isin(table['Subgroup'], sg_names))].drop(columns=['Detector'])\n",
    "            model_table = model_table.set_index('Subgroup')\n",
    "            metrics_dict = model_table.to_dict('index')\n",
    "            \n",
    "            metric_lims = []\n",
    "            for i, metric_name in enumerate(metrics_sorted):\n",
    "                offset = - 1.2 * i\n",
    "                metric_lims.append([offset, offset + 1])\n",
    "                # Margins\n",
    "                ax.axhline(offset, linewidth=0.8, color=\"k\")\n",
    "                ax.axhline(offset + 1, linewidth=0.8, color=\"k\")\n",
    "                ax.plot(\n",
    "                    [positions[0]-distance_from_edge, positions[0]-distance_from_edge], \n",
    "                    [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "                ax.plot(\n",
    "                    [positions[-1]+distance_from_edge, positions[-1]+distance_from_edge], \n",
    "                    [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "                # Data\n",
    "                mean_data = offset + np.array([metrics_dict[sg_name]['%s_mean' % metric_name] for sg_name in sg_names])\n",
    "                std_data = np.array([metrics_dict[sg_name]['%s_std' % metric_name] for sg_name in sg_names])\n",
    "                this_positions = positions + offsets[j]\n",
    "                ax.plot(\n",
    "                    this_positions, mean_data, label=print_model_names[model_name], linestyle=\"None\",\n",
    "                    marker=model_specs[model_name][\"marker\"], markersize=markersize, markeredgewidth=0.0,\n",
    "                    color=model_specs[model_name][\"color\"])\n",
    "                for i_sg in range(n_groups):\n",
    "                    ax.plot(\n",
    "                        [this_positions[i_sg], this_positions[i_sg]],\n",
    "                        [\n",
    "                            mean_data[i_sg] - number_of_std*std_data[i_sg], \n",
    "                            mean_data[i_sg] + number_of_std*std_data[i_sg]\n",
    "                        ],\n",
    "                        linewidth=1, color=model_specs[model_name][\"color\"]\n",
    "                    )\n",
    "                    \n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlim([positions[0] - distance_from_edge, positions[-1] + distance_from_edge])\n",
    "        ax.set_ylim([offset, 1])\n",
    "        yticks = np.concatenate([[m[0], (m[0]+m[1])/2, m[1]] for m in metric_lims])\n",
    "        yticklabels = np.concatenate([[0, n, 1] for n in metrics_sorted])\n",
    "        yticks_minor = np.concatenate([np.arange(m[0], m[1]+0.001, 0.1) for m in metric_lims])\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "        ax.set_yticks(yticks_minor, minor=True)\n",
    "        for i_tick, t in enumerate(ax.get_yticklabels()):\n",
    "            if i_tick % 3 == 1:\n",
    "                t.set_rotation('vertical')\n",
    "                t.set_verticalalignment('center')\n",
    "        ax.grid(axis=\"y\", which=\"minor\")\n",
    "        ax.yaxis.labelpad = -8\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xticks([i for i in range(n_groups)])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xticklabels([pretty_sg_names[sg_name] for sg_name in sg_names])\n",
    "        ax.set_xlabel(sg_labels[sg_prefix], fontsize=8)\n",
    "        dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]\n",
    "        ax.set_title(\"Desempeño en %s\" % dataset_str, loc=\"center\", fontsize=8)   \n",
    "        ax.text(\n",
    "            x=-0.01, y=1.02, fontsize=16, s=r\"$\\bf{%s}$\" % letters[ax_loc],\n",
    "            ha=\"left\", transform=ax.transAxes)\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"REDv2-Time\", \"REDv2-CWT\", \"DOSED\", \"A7\", \"Spinky\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.01), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_subgroups\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de datasets directa\n",
    "5CV only, las 4 bases de SS. Basarme en la forma de mostrar los resultados de subgroup analisis, en donde en vez de un subgrupo ahora es un dataset de origen, e incluir el mismo dataset como origen para tener ahi mismo la referencia del mejor desempeño posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "baselines = ['dosed', 'a7']\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),  \n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "for source_config in eval_configs:\n",
    "    source_str = print_dataset_names[(source_config[\"dataset_name\"], source_config[\"expert\"])]\n",
    "    print(\"\\nSource:\", source_str)\n",
    "    \n",
    "    table = {\n",
    "        'Detector': [], \n",
    "        'Target': [],\n",
    "        'F1-score': [], \n",
    "        'Recall': [], \n",
    "        'Precision': [], \n",
    "        'mIoU': [], \n",
    "        'Fold': []\n",
    "    }\n",
    "    \n",
    "    for config in eval_configs:\n",
    "        target_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "        print(\"Target:\", target_str)\n",
    "        dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "        \n",
    "        # Collect predictions\n",
    "        pred_dict = {}\n",
    "        for model_version in models:\n",
    "            tmp_dict = fig_utils.get_red_predictions(\n",
    "                model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False,\n",
    "                source_dataset_name=source_config[\"dataset_name\"], source_expert=source_config[\"expert\"]\n",
    "            )\n",
    "            # Retrieve only predictions, same format as baselines\n",
    "            pred_dict[model_version] = {}\n",
    "            for k in tmp_dict.keys():\n",
    "                fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "                fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "                pred_dict[model_version][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "        for baseline_name in baselines:\n",
    "            pred_dict[baseline_name] = fig_utils.get_baseline_predictions(\n",
    "                baseline_name, config[\"strategy\"], config[\"dataset_name\"], config[\"expert\"],\n",
    "                source_dataset_name=source_config[\"dataset_name\"], source_expert=source_config[\"expert\"]\n",
    "            )\n",
    "\n",
    "        # Measure performance byfold\n",
    "        average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "        _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "        n_folds = len(test_ids_list)\n",
    "        for model_name in pred_dict.keys():\n",
    "            for k in range(n_folds):\n",
    "                subject_ids = test_ids_list[k]\n",
    "                feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "                events_list = feed_d.get_stamps()\n",
    "                detections_list = [pred_dict[model_name][k][subject_id] for subject_id in subject_ids]\n",
    "                performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                table['Detector'].append(model_name)\n",
    "                table['Target'].append(target_str)\n",
    "                table['F1-score'].append(performance['F1-score'])\n",
    "                table['Recall'].append(performance['Recall'])\n",
    "                table['Precision'].append(performance['Precision'])\n",
    "                table['mIoU'].append(performance['mIoU'])\n",
    "                table['Fold'].append(k)\n",
    "    \n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    print(\"By-fold statistics\")\n",
    "    mean_table = table.groupby(by=['Detector', 'Target']).mean().drop(columns=[\"Fold\"]).add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=['Detector', 'Target']).std(ddof=0).drop(columns=[\"Fold\"]).add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)\n",
    "print(\"Metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate table for latex\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D, 'dosed', 'a7', 'spinky']\n",
    "datasets_order = ['MASS-SS2-E1SS', 'MASS-SS2-E2SS', 'MASS-MODA', 'INTA-UCH']\n",
    "\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]\n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    n_models = len(model_names)\n",
    "    table = table[['Detector', 'Target', 'F1-score_mean', 'F1-score_std']]\n",
    "    \n",
    "    print(\"\\nSource: %s\" % dataset_str)\n",
    "    header = \" & \".join([print_model_names[model_name] for model_name in model_names])\n",
    "    header = \"Entrenamiento & Evaluación & %s \\\\\\\\\" % header\n",
    "    print(header)\n",
    "    \n",
    "    for target_dataset_str in datasets_order:\n",
    "        metric_str_list = []\n",
    "        for model_name in model_names:\n",
    "            model_table = table[\n",
    "                (table['Detector'] == model_name) & (table['Target'] == target_dataset_str)\n",
    "            ].drop(columns=['Detector'])\n",
    "            model_table = model_table.set_index('Target')\n",
    "            metrics_dict = model_table.to_dict('index')\n",
    "            metric_str = '$%1.1f\\pm %1.1f$' % (\n",
    "                100 * metrics_dict[target_dataset_str]['F1-score_mean'], 100 * metrics_dict[target_dataset_str]['F1-score_std'])\n",
    "            metric_str_list.append(metric_str)\n",
    "        metric_str = \" & \".join(metric_str_list)\n",
    "        metric_str = \"%s & %s & %s \\\\\\\\\" % (\n",
    "            dataset_str, target_dataset_str, metric_str)\n",
    "        print(metric_str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura\n",
    "save_figure = True\n",
    "markersize = 5\n",
    "number_of_std = 1\n",
    "groups_total_width = 0.5\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D']\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "    'dosed': dict(marker='s', color=baseline_color),\n",
    "    'a7': dict(marker='^', color=baseline_color),\n",
    "    'spinky': dict(marker='v', color=baseline_color),\n",
    "}\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "    'dosed': 'DOSED',\n",
    "    'a7': 'A7',\n",
    "    'spinky': 'Spinky'\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D, 'dosed', 'a7', 'spinky']\n",
    "datasets_order = ['MASS-SS2-E1SS', 'MASS-SS2-E2SS', 'MASS-MODA', 'INTA-UCH']\n",
    "metrics_sorted = ['F1-score', 'Recall', 'Precision', 'mIoU']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 9), dpi=200)\n",
    "axes = np.concatenate([axes[:1, :], axes[1:, :]], axis=1) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    \n",
    "    ax = axes[i_config]\n",
    "    \n",
    "    dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]    \n",
    "    \n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    n_models = len(model_names)\n",
    "    \n",
    "    n_groups = len(datasets_order)\n",
    "    positions = np.arange(n_groups)\n",
    "    groups_width = groups_total_width / n_models\n",
    "    initial_group_center_offset = groups_width * (1 - n_models) / 2\n",
    "    offsets = initial_group_center_offset + np.arange(n_models) * groups_width\n",
    "    distance_from_edge = groups_total_width / 2 + 0.1\n",
    "\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        model_table = table[table['Detector'] == model_name].drop(columns=['Detector'])\n",
    "        model_table = model_table.set_index('Target')\n",
    "        metrics_dict = model_table.to_dict('index')\n",
    "\n",
    "        metric_lims = []\n",
    "        for i, metric_name in enumerate(metrics_sorted):\n",
    "            offset = - 1.2 * i\n",
    "            metric_lims.append([offset, offset + 1])\n",
    "            # Margins\n",
    "            ax.axhline(offset, linewidth=0.8, color=\"k\")\n",
    "            ax.axhline(offset + 1, linewidth=0.8, color=\"k\")\n",
    "            ax.plot(\n",
    "                [positions[0]-distance_from_edge, positions[0]-distance_from_edge], \n",
    "                [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "            ax.plot(\n",
    "                [positions[-1]+distance_from_edge, positions[-1]+distance_from_edge], \n",
    "                [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "            # Data\n",
    "            mean_data = offset + np.array([metrics_dict[target_str]['%s_mean' % metric_name] for target_str in datasets_order])\n",
    "            std_data = np.array([metrics_dict[target_str]['%s_std' % metric_name] for target_str in datasets_order])\n",
    "            this_positions = positions + offsets[j]\n",
    "            ax.plot(\n",
    "                this_positions, mean_data, label=print_model_names[model_name], linestyle=\"None\",\n",
    "                marker=model_specs[model_name][\"marker\"], markersize=markersize, markeredgewidth=0.0,\n",
    "                color=model_specs[model_name][\"color\"])\n",
    "            for i_sg in range(n_groups):\n",
    "                ax.plot(\n",
    "                    [this_positions[i_sg], this_positions[i_sg]],\n",
    "                    [\n",
    "                        mean_data[i_sg] - number_of_std*std_data[i_sg], \n",
    "                        mean_data[i_sg] + number_of_std*std_data[i_sg]\n",
    "                    ],\n",
    "                    linewidth=1, color=model_specs[model_name][\"color\"]\n",
    "                )\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim([positions[0] - distance_from_edge, positions[-1] + distance_from_edge])\n",
    "    ax.set_ylim([offset, 1])\n",
    "    yticks = np.concatenate([[m[0], (m[0]+m[1])/2, m[1]] for m in metric_lims])\n",
    "    yticklabels = np.concatenate([[0, n, 1] for n in metrics_sorted])\n",
    "    yticks_minor = np.concatenate([np.arange(m[0], m[1]+0.001, 0.1) for m in metric_lims])\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_yticks(yticks_minor, minor=True)\n",
    "    for i_tick, t in enumerate(ax.get_yticklabels()):\n",
    "        if i_tick % 3 == 1:\n",
    "            t.set_rotation('vertical')\n",
    "            t.set_verticalalignment('center')\n",
    "    ax.grid(axis=\"y\", which=\"minor\")\n",
    "    ax.yaxis.labelpad = -8\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xticks([i for i in range(n_groups)])\n",
    "    ax.set_xticklabels(datasets_order)\n",
    "    ax.set_xlabel(\"Evaluación\", fontsize=8)\n",
    "    ax.set_title(\"Entrenamiento en %s\" % dataset_str, loc=\"center\", fontsize=8)   \n",
    "    ax.text(\n",
    "        x=-0.01, y=1.02, fontsize=16, s=r\"$\\bf{%s}$\" % letters[i_config],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"REDv2-Time\", \"REDv2-CWT\", \"DOSED\", \"A7\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.01), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_comparison_crossdataset\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
