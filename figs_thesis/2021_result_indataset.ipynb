{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777296f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, hilbert\n",
    "import pickle\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.common import viz, constants\n",
    "from sleeprnn.helpers import reader, plotter, misc, performer\n",
    "from sleeprnn.detection import metrics, det_utils\n",
    "from figs_thesis import fig_utils\n",
    "from baselines_scripts.butils import get_partitions\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sleeprnn.data import utils\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "BASELINES_PATH = os.path.join(project_root, 'resources', 'comparison_data', 'baselines_2021')\n",
    "\n",
    "%matplotlib inline\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c357c",
   "metadata": {},
   "source": [
    "# Comparación REDv2-Time y REDv2-CWT\n",
    "\n",
    "Métricas en cada base de datos (5CV): F1-score y mIoU entre ambos REDv2 y entre ellos mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "metrics_list = []\n",
    "metrics_raw_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {s: {} for s in dataset.all_ids}\n",
    "        for k in range(n_folds):\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            for s, pred in zip(fold_subjects, fold_predictions):\n",
    "                pred_dict[model_version][s][k] = pred\n",
    "    # Generate typical dict\n",
    "    pred_dict_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_original[model_version][k] = {s: pred_dict[model_version][s][k] for s in test_ids_list[k]}\n",
    "    # Generate surrogate model\n",
    "    # Random permutation of fold assignments of predictions\n",
    "    pred_dict_permuted = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted[model_version] = {}\n",
    "        for i_sub, subject_id in enumerate(dataset.all_ids):\n",
    "            byfold_preds = pred_dict[model_version][subject_id]\n",
    "            subject_folds = list(byfold_preds.keys())\n",
    "            subject_preds = [byfold_preds[k] for k in subject_folds]\n",
    "            subject_folds = np.random.RandomState(seed=i_sub).permutation(subject_folds)\n",
    "            pred_dict_permuted[model_version][subject_id] = {k: pred for k, pred in zip(subject_folds, subject_preds)}\n",
    "    pred_dict_permuted_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_permuted_original[model_version][k] = {s: pred_dict_permuted[model_version][s][k] for s in test_ids_list[k]}\n",
    "    \n",
    "    # Performance\n",
    "    table = {'Comparison': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    # Measure performance of model with itself\n",
    "    for model_version in models:\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            events_list = [pred_dict_original[model_version][k][subject_id] for subject_id in subject_ids]\n",
    "            detections_list = [pred_dict_permuted_original[model_version][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "            table['Comparison'].append('%s_vs_%s' % (model_version, model_version))\n",
    "            table['F1-score'].append(performance['F1-score'])\n",
    "            table['Recall'].append(performance['Recall'])\n",
    "            table['Precision'].append(performance['Precision'])\n",
    "            table['mIoU'].append(performance['mIoU'])\n",
    "            table['Fold'].append(k)\n",
    "    # Measure performance of time vs cwt\n",
    "    for k in range(n_folds):\n",
    "        subject_ids = test_ids_list[k]\n",
    "        events_list = [pred_dict_original[models[0]][k][subject_id] for subject_id in subject_ids]\n",
    "        detections_list = [pred_dict_original[models[1]][k][subject_id] for subject_id in subject_ids]\n",
    "        performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "        table['Comparison'].append('%s_vs_%s' % (models[0], models[1]))\n",
    "        table['F1-score'].append(performance['F1-score'])\n",
    "        table['Recall'].append(performance['Recall'])\n",
    "        table['Precision'].append(performance['Precision'])\n",
    "        table['mIoU'].append(performance['mIoU'])\n",
    "        table['Fold'].append(k)\n",
    "    \n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    metrics_raw_list.append(table)\n",
    "    mean_table = table.groupby(by=[\"Comparison\"]).mean()[[\"F1-score\", \"mIoU\"]].add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=[\"Comparison\"]).std(ddof=0)[[\"F1-score\", \"mIoU\"]].add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla Latex\n",
    "comparisons_order = [\"v2_time_vs_v2_time\", \"v2_cwt1d_vs_v2_cwt1d\", \"v2_time_vs_v2_cwt1d\"]\n",
    "comparisons_print_name = {\n",
    "    \"v2_time_vs_v2_time\": \"REDv2-Time vs REDv2-Time\", \n",
    "    \"v2_cwt1d_vs_v2_cwt1d\": \"REDv2-CWT vs REDv2-CWT\", \n",
    "    \"v2_time_vs_v2_cwt1d\": \"REDv2-Time vs REDv2-CWT\"\n",
    "}\n",
    "\n",
    "print(\"Datos & Comparación & F1-score (%) & mIoU (%) \\\\\\\\\")\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    print(\"\")\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    metrics_dict = metrics_list[i_config].set_index('Comparison').to_dict('index')\n",
    "    for comparison in comparisons_order:\n",
    "        mets = metrics_dict[comparison]\n",
    "        metric_str = \"$%1.1f\\pm %1.1f$ & $%1.1f\\pm %1.1f$\" % (\n",
    "            100 * mets['F1-score_mean'], 100 * mets['F1-score_std'],\n",
    "            100 * mets['mIoU_mean'], 100 * mets['mIoU_std'])\n",
    "        row_str = \"%s & %s & %s \\\\\\\\\" % (dataset_str, comparisons_print_name[comparison], metric_str)\n",
    "        print(row_str)\n",
    "        \n",
    "    # Statistical tests\n",
    "    reference_comparison = comparisons_order[-1]\n",
    "    print(\"P-value test against %s\" % reference_comparison)\n",
    "    table = metrics_raw_list[i_config]\n",
    "    for comparison in comparisons_order[:-1]:\n",
    "        print(\"%s:\" % comparison.ljust(30), end='')\n",
    "        for metric_name in [\"F1-score\", \"mIoU\"]:\n",
    "            model_metrics = table[table[\"Comparison\"] == comparison][metric_name].values\n",
    "            reference_metrics = table[table[\"Comparison\"] == reference_comparison][metric_name].values\n",
    "            pvalue = stats.ttest_ind(model_metrics, reference_metrics, equal_var=False)[1]\n",
    "            print(\" P(%s) %1.4f\" % (metric_name, pvalue), end='')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de45a4f",
   "metadata": {},
   "source": [
    "# Ensamble de RED\n",
    "\n",
    "Desempeño al ensamblar las detecciones de ambos modelos (y ensambles de un modelo consigo mismo haciendo el truco de la permutacion) con un AND, con un OR, o al promediar las probabilidades ajustadas antes de aplicar el umbral. Esto para ver si las pequeñas diferencias que tienen ayudan o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563a118",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "ensemble_criterion_list = [\n",
    "    'and', \n",
    "    # 'or', \n",
    "    'avg'\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "metrics_raw_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(\n",
    "            model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve probas and stamps\n",
    "        pred_dict[model_version] = {s: {} for s in dataset.all_ids}\n",
    "        for k in range(n_folds):\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_probas = tmp_dict[k][constants.TEST_SUBSET].get_probabilities(return_adjusted=True)\n",
    "            fold_stamps = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            for s, proba, stamp in zip(fold_subjects, fold_probas, fold_stamps):\n",
    "                pred_dict[model_version][s][k] = {'probability': proba, 'stamp': stamp} \n",
    "    # Generate typical dict\n",
    "    pred_dict_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_original[model_version][k] = {s: pred_dict[model_version][s][k] for s in test_ids_list[k]}\n",
    "    # Generate surrogate model\n",
    "    # Random permutation of fold assignments of predictions\n",
    "    pred_dict_permuted = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted[model_version] = {}\n",
    "        for i_sub, subject_id in enumerate(dataset.all_ids):\n",
    "            byfold_preds = pred_dict[model_version][subject_id]\n",
    "            subject_folds = list(byfold_preds.keys())\n",
    "            subject_preds = [byfold_preds[k] for k in subject_folds]\n",
    "            subject_folds = np.random.RandomState(seed=i_sub).permutation(subject_folds)\n",
    "            pred_dict_permuted[model_version][subject_id] = {k: pred for k, pred in zip(subject_folds, subject_preds)}\n",
    "    pred_dict_permuted_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_permuted_original[model_version][k] = {s: pred_dict_permuted[model_version][s][k] for s in test_ids_list[k]}\n",
    "            \n",
    "    # Performance\n",
    "    # AND: ensemble of stamps -> thr = 1.0\n",
    "    # OR: ensemble of stamps -> thr = 1 / n_models\n",
    "    # AVG: ensemble of probas with thr of 0.5\n",
    "    table = {'Ensemble': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    pred_sources = {\n",
    "        'v2_time': pred_dict_original['v2_time'],\n",
    "        'v2_time_perm': pred_dict_permuted_original['v2_time'],\n",
    "        'v2_cwt1d': pred_dict_original['v2_cwt1d'],\n",
    "        'v2_cwt1d_perm': pred_dict_permuted_original['v2_cwt1d'],\n",
    "    }\n",
    "    pairs = [\n",
    "        ('v2_time', 'v2_time'),\n",
    "        ('v2_cwt1d', 'v2_cwt1d'),\n",
    "        ('v2_time', 'v2_time_perm'),\n",
    "        ('v2_cwt1d', 'v2_cwt1d_perm'),\n",
    "        ('v2_time', 'v2_cwt1d'),\n",
    "    ]\n",
    "    for k in range(n_folds):\n",
    "        subject_ids = test_ids_list[k]\n",
    "        print(\"Fold %s\" % (k))\n",
    "        reference_feeder_dataset = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "        events_list = reference_feeder_dataset.get_stamps()\n",
    "        for pred_source_name_1, pred_source_name_2 in pairs:\n",
    "            pred_source_1 = pred_sources[pred_source_name_1]\n",
    "            pred_source_2 = pred_sources[pred_source_name_2]\n",
    "            if pred_source_name_1 == pred_source_name_2:\n",
    "                detections_list = [pred_source_1[k][subject_id]['stamp'] for subject_id in subject_ids]\n",
    "                performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                ensemble_str = pred_source_name_1\n",
    "                table['Ensemble'].append(ensemble_str)\n",
    "                table['F1-score'].append(performance['F1-score'])\n",
    "                table['Recall'].append(performance['Recall'])\n",
    "                table['Precision'].append(performance['Precision'])\n",
    "                table['mIoU'].append(performance['mIoU'])\n",
    "                table['Fold'].append(k)\n",
    "            else:\n",
    "                for criterion in ensemble_criterion_list:\n",
    "                    if criterion == 'and':\n",
    "                        dict_of_stamps = {s: [pred_source_1[k][s]['stamp'], pred_source_2[k][s]['stamp']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = det_utils.generate_ensemble_from_stamps(dict_of_stamps, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(1.0)\n",
    "                    elif criterion == 'or':\n",
    "                        dict_of_stamps = {s: [pred_source_1[k][s]['stamp'], pred_source_2[k][s]['stamp']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = det_utils.generate_ensemble_from_stamps(dict_of_stamps, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(0.5)\n",
    "                    elif criterion == 'avg':\n",
    "                        dict_of_probas = {s: [pred_source_1[k][s]['probability'], pred_source_2[k][s]['probability']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = det_utils.generate_ensemble_from_probabilities(dict_of_probas, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(0.5)\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "                    detections_list = ensemble_pred_obj.get_stamps()\n",
    "                    performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                    ensemble_str = '%s(%s,%s)' % (criterion, pred_source_name_1, pred_source_name_2)\n",
    "                    table['Ensemble'].append(ensemble_str)\n",
    "                    table['F1-score'].append(performance['F1-score'])\n",
    "                    table['Recall'].append(performance['Recall'])\n",
    "                    table['Precision'].append(performance['Precision'])\n",
    "                    table['mIoU'].append(performance['mIoU'])\n",
    "                    table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    metrics_raw_list.append(table)\n",
    "    mean_table = table.groupby(by=[\"Ensemble\"]).mean().drop(columns=[\"Fold\"]).add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=[\"Ensemble\"]).std(ddof=0).drop(columns=[\"Fold\"]).add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15193f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc958851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla Latex\n",
    "ensembles_order = [\n",
    "    \"v2_time\", \n",
    "    \"v2_cwt1d\",\n",
    "    \"and(v2_time,v2_time_perm)\",\n",
    "    \"and(v2_cwt1d,v2_cwt1d_perm)\",\n",
    "    \"and(v2_time,v2_cwt1d)\",\n",
    "    \"avg(v2_time,v2_time_perm)\",\n",
    "    \"avg(v2_cwt1d,v2_cwt1d_perm)\",\n",
    "    \"avg(v2_time,v2_cwt1d)\",\n",
    "]\n",
    "ensembles_print_name = {\n",
    "    \"v2_time\": \"REDv2-Time\", \n",
    "    \"v2_cwt1d\": \"REDv2-CWT\",\n",
    "    \"and(v2_time,v2_time_perm)\": \"AND(REDv2-Time, REDv2-Time)\",\n",
    "    \"and(v2_cwt1d,v2_cwt1d_perm)\": \"AND(REDv2-CWT, REDv2-CWT)\",\n",
    "    \"and(v2_time,v2_cwt1d)\": \"AND(REDv2-Time, REDv2-CWT)\",\n",
    "    \"avg(v2_time,v2_time_perm)\": \"AVG(REDv2-Time, REDv2-Time)\",\n",
    "    \"avg(v2_cwt1d,v2_cwt1d_perm)\": \"AVG(REDv2-CWT, REDv2-CWT)\",\n",
    "    \"avg(v2_time,v2_cwt1d)\": \"AVG(REDv2-Time, REDv2-CWT)\",\n",
    "}\n",
    "print(\"\\\\toprule\")\n",
    "print(\"Datos & Detector & F1-score (\\%) & mIoU (\\%) \\\\\\\\\")\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    print(\"\\midrule\")\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    metrics_dict = metrics_list[i_config].set_index('Ensemble').to_dict('index')\n",
    "    for i_ens, ensemble in enumerate(ensembles_order):\n",
    "        mets = metrics_dict[ensemble]\n",
    "        metric_str = \"$%1.1f\\pm %1.1f$ & $%1.1f\\pm %1.1f$\" % (\n",
    "            100 * mets['F1-score_mean'], 100 * mets['F1-score_std'],\n",
    "            100 * mets['mIoU_mean'], 100 * mets['mIoU_std'])\n",
    "        dataset_to_print = '%s\\n' % dataset_str if (i_ens == 0) else ''\n",
    "        row_str = \"%s & %s & %s \\\\\\\\\" % (dataset_to_print, ensembles_print_name[ensemble].ljust(30), metric_str)\n",
    "        print(row_str)\n",
    "        \n",
    "    # Statistical tests\n",
    "    #reference_ensemble = ensembles_order[0]\n",
    "    #print(\"P-value test against %s\" % reference_ensemble)\n",
    "    #table = metrics_raw_list[i_config]\n",
    "    #for ensemble in ensembles_order[1:]:\n",
    "    #    print(\"%s:\" % ensemble.ljust(50), end='')\n",
    "    #    for metric_name in [\"F1-score\", \"mIoU\"]:\n",
    "    #        model_metrics = table[table[\"Ensemble\"] == ensemble][metric_name].values\n",
    "    #        reference_metrics = table[table[\"Ensemble\"] == reference_ensemble][metric_name].values\n",
    "    #        pvalue = stats.ttest_ind(model_metrics, reference_metrics, equal_var=False)[1]\n",
    "    #        print(\" P(%s) %1.4f\" % (metric_name, pvalue), end='')\n",
    "    #    print(\"\")\n",
    "print(\"\\\\bottomrule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325a090",
   "metadata": {},
   "source": [
    "# Perturbaciones en la entrada\n",
    "Medir cambios en SubsetMetric vs perturbacion, donde subset metric es la metrica mean+-std del 5CV del dataset, y puede ser f1-score, recall, precision y mIoU (lo bueno es que las 4 están en el rango 0-1).\n",
    "- Escalar la señal de entrada. Curva continua\n",
    "- Inversión de voltaje (multiplicar por -1) y de tiempo (flipped signal y labels, para implementarlo se podria hacer flip de la entrada, y el vector de probabilidad de salida volver a hacerle un flip). Discreto (barras).\n",
    "- Filtrar (quitar) bandas de potencia (modelado ya entrenado). Delta separarlo en delta lenta y rapida. Discreto (barras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66974e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
