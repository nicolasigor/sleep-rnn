{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777296f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, hilbert\n",
    "import pickle\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.common import viz, constants\n",
    "from sleeprnn.helpers import reader, plotter, misc, performer\n",
    "from sleeprnn.detection import metrics, det_utils, ensemble\n",
    "from figs_thesis import fig_utils\n",
    "from baselines_scripts.butils import get_partitions\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sleeprnn.data import utils\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "BASELINES_PATH = os.path.join(project_root, 'resources', 'comparison_data', 'baselines_2021')\n",
    "\n",
    "%matplotlib inline\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c357c",
   "metadata": {},
   "source": [
    "# Comparación REDv2-Time y REDv2-CWT\n",
    "\n",
    "Métricas en cada base de datos (5CV): F1-score y mIoU entre ambos REDv2 y entre ellos mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "metrics_list = []\n",
    "metrics_raw_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve only predictions, same format as baselines\n",
    "        pred_dict[model_version] = {s: {} for s in dataset.all_ids}\n",
    "        for k in range(n_folds):\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            for s, pred in zip(fold_subjects, fold_predictions):\n",
    "                pred_dict[model_version][s][k] = pred\n",
    "    # Generate typical dict\n",
    "    pred_dict_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_original[model_version][k] = {s: pred_dict[model_version][s][k] for s in test_ids_list[k]}\n",
    "    # Generate surrogate model\n",
    "    # Random permutation of fold assignments of predictions\n",
    "    pred_dict_permuted = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted[model_version] = {}\n",
    "        for i_sub, subject_id in enumerate(dataset.all_ids):\n",
    "            byfold_preds = pred_dict[model_version][subject_id]\n",
    "            subject_folds = list(byfold_preds.keys())\n",
    "            subject_preds = [byfold_preds[k] for k in subject_folds]\n",
    "            subject_folds = np.random.RandomState(seed=i_sub).permutation(subject_folds)\n",
    "            pred_dict_permuted[model_version][subject_id] = {k: pred for k, pred in zip(subject_folds, subject_preds)}\n",
    "    pred_dict_permuted_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_permuted_original[model_version][k] = {s: pred_dict_permuted[model_version][s][k] for s in test_ids_list[k]}\n",
    "    \n",
    "    # Performance\n",
    "    table = {'Comparison': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    # Measure performance of model with itself\n",
    "    for model_version in models:\n",
    "        for k in range(n_folds):\n",
    "            subject_ids = test_ids_list[k]\n",
    "            events_list = [pred_dict_original[model_version][k][subject_id] for subject_id in subject_ids]\n",
    "            detections_list = [pred_dict_permuted_original[model_version][k][subject_id] for subject_id in subject_ids]\n",
    "            performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "            table['Comparison'].append('%s_vs_%s' % (model_version, model_version))\n",
    "            table['F1-score'].append(performance['F1-score'])\n",
    "            table['Recall'].append(performance['Recall'])\n",
    "            table['Precision'].append(performance['Precision'])\n",
    "            table['mIoU'].append(performance['mIoU'])\n",
    "            table['Fold'].append(k)\n",
    "    # Measure performance of time vs cwt\n",
    "    for k in range(n_folds):\n",
    "        subject_ids = test_ids_list[k]\n",
    "        events_list = [pred_dict_original[models[0]][k][subject_id] for subject_id in subject_ids]\n",
    "        detections_list = [pred_dict_original[models[1]][k][subject_id] for subject_id in subject_ids]\n",
    "        performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "        table['Comparison'].append('%s_vs_%s' % (models[0], models[1]))\n",
    "        table['F1-score'].append(performance['F1-score'])\n",
    "        table['Recall'].append(performance['Recall'])\n",
    "        table['Precision'].append(performance['Precision'])\n",
    "        table['mIoU'].append(performance['mIoU'])\n",
    "        table['Fold'].append(k)\n",
    "    \n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    metrics_raw_list.append(table)\n",
    "    mean_table = table.groupby(by=[\"Comparison\"]).mean()[[\"F1-score\", \"mIoU\"]].add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=[\"Comparison\"]).std(ddof=0)[[\"F1-score\", \"mIoU\"]].add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla Latex\n",
    "comparisons_order = [\"v2_time_vs_v2_time\", \"v2_cwt1d_vs_v2_cwt1d\", \"v2_time_vs_v2_cwt1d\"]\n",
    "comparisons_print_name = {\n",
    "    \"v2_time_vs_v2_time\": \"REDv2-Time vs REDv2-Time\", \n",
    "    \"v2_cwt1d_vs_v2_cwt1d\": \"REDv2-CWT vs REDv2-CWT\", \n",
    "    \"v2_time_vs_v2_cwt1d\": \"REDv2-Time vs REDv2-CWT\"\n",
    "}\n",
    "\n",
    "print(\"Datos & Comparación & F1-score (%) & mIoU (%) \\\\\\\\\")\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    print(\"\")\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    metrics_dict = metrics_list[i_config].set_index('Comparison').to_dict('index')\n",
    "    for comparison in comparisons_order:\n",
    "        mets = metrics_dict[comparison]\n",
    "        metric_str = \"$%1.1f\\pm %1.1f$ & $%1.1f\\pm %1.1f$\" % (\n",
    "            100 * mets['F1-score_mean'], 100 * mets['F1-score_std'],\n",
    "            100 * mets['mIoU_mean'], 100 * mets['mIoU_std'])\n",
    "        row_str = \"%s & %s & %s \\\\\\\\\" % (dataset_str, comparisons_print_name[comparison], metric_str)\n",
    "        print(row_str)\n",
    "        \n",
    "    # Statistical tests\n",
    "    reference_comparison = comparisons_order[-1]\n",
    "    print(\"P-value test against %s\" % reference_comparison)\n",
    "    table = metrics_raw_list[i_config]\n",
    "    for comparison in comparisons_order[:-1]:\n",
    "        print(\"%s:\" % comparison.ljust(30), end='')\n",
    "        for metric_name in [\"F1-score\", \"mIoU\"]:\n",
    "            model_metrics = table[table[\"Comparison\"] == comparison][metric_name].values\n",
    "            reference_metrics = table[table[\"Comparison\"] == reference_comparison][metric_name].values\n",
    "            pvalue = stats.ttest_ind(model_metrics, reference_metrics, equal_var=False)[1]\n",
    "            print(\" P(%s) %1.4f\" % (metric_name, pvalue), end='')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de45a4f",
   "metadata": {},
   "source": [
    "# Ensamble de RED\n",
    "\n",
    "Desempeño al ensamblar las detecciones de ambos modelos (y ensambles de un modelo consigo mismo haciendo el truco de la permutacion) con un AND, con un OR, o al promediar las probabilidades ajustadas antes de aplicar el umbral. Esto para ver si las pequeñas diferencias que tienen ayudan o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563a118",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, expert=2, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.INTA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "ensemble_criterion_list = [\n",
    "    'and', \n",
    "    # 'or', \n",
    "    'avg'\n",
    "]\n",
    "\n",
    "metrics_list = []\n",
    "metrics_raw_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        tmp_dict = fig_utils.get_red_predictions(\n",
    "            model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        # Retrieve probas and stamps\n",
    "        pred_dict[model_version] = {s: {} for s in dataset.all_ids}\n",
    "        for k in range(n_folds):\n",
    "            fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "            fold_probas = tmp_dict[k][constants.TEST_SUBSET].get_probabilities(return_adjusted=True)\n",
    "            fold_stamps = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "            for s, proba, stamp in zip(fold_subjects, fold_probas, fold_stamps):\n",
    "                pred_dict[model_version][s][k] = {'probability': proba, 'stamp': stamp} \n",
    "    # Generate typical dict\n",
    "    pred_dict_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_original[model_version][k] = {s: pred_dict[model_version][s][k] for s in test_ids_list[k]}\n",
    "    # Generate surrogate model\n",
    "    # Random permutation of fold assignments of predictions\n",
    "    pred_dict_permuted = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted[model_version] = {}\n",
    "        for i_sub, subject_id in enumerate(dataset.all_ids):\n",
    "            byfold_preds = pred_dict[model_version][subject_id]\n",
    "            subject_folds = list(byfold_preds.keys())\n",
    "            subject_preds = [byfold_preds[k] for k in subject_folds]\n",
    "            subject_folds = np.random.RandomState(seed=i_sub).permutation(subject_folds)\n",
    "            pred_dict_permuted[model_version][subject_id] = {k: pred for k, pred in zip(subject_folds, subject_preds)}\n",
    "    pred_dict_permuted_original = {}\n",
    "    for model_version in models:\n",
    "        pred_dict_permuted_original[model_version] = {}\n",
    "        for k in range(n_folds):\n",
    "            pred_dict_permuted_original[model_version][k] = {s: pred_dict_permuted[model_version][s][k] for s in test_ids_list[k]}\n",
    "            \n",
    "    # Performance\n",
    "    # AND: ensemble of stamps -> thr = 1.0\n",
    "    # OR: ensemble of stamps -> thr = 1 / n_models\n",
    "    # AVG: ensemble of probas with thr of 0.5\n",
    "    table = {'Ensemble': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    pred_sources = {\n",
    "        'v2_time': pred_dict_original['v2_time'],\n",
    "        'v2_time_perm': pred_dict_permuted_original['v2_time'],\n",
    "        'v2_cwt1d': pred_dict_original['v2_cwt1d'],\n",
    "        'v2_cwt1d_perm': pred_dict_permuted_original['v2_cwt1d'],\n",
    "    }\n",
    "    pairs = [\n",
    "        ('v2_time', 'v2_time'),\n",
    "        ('v2_cwt1d', 'v2_cwt1d'),\n",
    "        ('v2_time', 'v2_time_perm'),\n",
    "        ('v2_cwt1d', 'v2_cwt1d_perm'),\n",
    "        ('v2_time', 'v2_cwt1d'),\n",
    "    ]\n",
    "    for k in range(n_folds):\n",
    "        subject_ids = test_ids_list[k]\n",
    "        print(\"Fold %s\" % (k))\n",
    "        reference_feeder_dataset = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "        events_list = reference_feeder_dataset.get_stamps()\n",
    "        for pred_source_name_1, pred_source_name_2 in pairs:\n",
    "            pred_source_1 = pred_sources[pred_source_name_1]\n",
    "            pred_source_2 = pred_sources[pred_source_name_2]\n",
    "            if pred_source_name_1 == pred_source_name_2:\n",
    "                detections_list = [pred_source_1[k][subject_id]['stamp'] for subject_id in subject_ids]\n",
    "                performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                ensemble_str = pred_source_name_1\n",
    "                table['Ensemble'].append(ensemble_str)\n",
    "                table['F1-score'].append(performance['F1-score'])\n",
    "                table['Recall'].append(performance['Recall'])\n",
    "                table['Precision'].append(performance['Precision'])\n",
    "                table['mIoU'].append(performance['mIoU'])\n",
    "                table['Fold'].append(k)\n",
    "            else:\n",
    "                for criterion in ensemble_criterion_list:\n",
    "                    if criterion == 'and':\n",
    "                        dict_of_stamps = {s: [pred_source_1[k][s]['stamp'], pred_source_2[k][s]['stamp']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = ensemble.generate_ensemble_from_stamps(dict_of_stamps, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(1.0)\n",
    "                    elif criterion == 'or':\n",
    "                        dict_of_stamps = {s: [pred_source_1[k][s]['stamp'], pred_source_2[k][s]['stamp']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = ensemble.generate_ensemble_from_stamps(dict_of_stamps, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(0.5)\n",
    "                    elif criterion == 'avg':\n",
    "                        dict_of_probas = {s: [pred_source_1[k][s]['probability'], pred_source_2[k][s]['probability']] for s in subject_ids}\n",
    "                        ensemble_pred_obj = ensemble.generate_ensemble_from_probabilities(dict_of_probas, reference_feeder_dataset, skip_setting_threshold=True)\n",
    "                        ensemble_pred_obj.set_parent_dataset(dataset)\n",
    "                        ensemble_pred_obj.set_probability_threshold(0.5)\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "                    detections_list = ensemble_pred_obj.get_stamps()\n",
    "                    performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                    ensemble_str = '%s(%s,%s)' % (criterion, pred_source_name_1, pred_source_name_2)\n",
    "                    table['Ensemble'].append(ensemble_str)\n",
    "                    table['F1-score'].append(performance['F1-score'])\n",
    "                    table['Recall'].append(performance['Recall'])\n",
    "                    table['Precision'].append(performance['Precision'])\n",
    "                    table['mIoU'].append(performance['mIoU'])\n",
    "                    table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    metrics_raw_list.append(table)\n",
    "    mean_table = table.groupby(by=[\"Ensemble\"]).mean().drop(columns=[\"Fold\"]).add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=[\"Ensemble\"]).std(ddof=0).drop(columns=[\"Fold\"]).add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc958851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla Latex\n",
    "ensembles_order = [\n",
    "    \"v2_time\", \n",
    "    \"v2_cwt1d\",\n",
    "    \"and(v2_time,v2_time_perm)\",\n",
    "    \"and(v2_cwt1d,v2_cwt1d_perm)\",\n",
    "    \"and(v2_time,v2_cwt1d)\",\n",
    "    \"avg(v2_time,v2_time_perm)\",\n",
    "    \"avg(v2_cwt1d,v2_cwt1d_perm)\",\n",
    "    \"avg(v2_time,v2_cwt1d)\",\n",
    "]\n",
    "ensembles_print_name = {\n",
    "    \"v2_time\": \"REDv2-Time\", \n",
    "    \"v2_cwt1d\": \"REDv2-CWT\",\n",
    "    \"and(v2_time,v2_time_perm)\": \"AND(REDv2-Time, REDv2-Time)\",\n",
    "    \"and(v2_cwt1d,v2_cwt1d_perm)\": \"AND(REDv2-CWT, REDv2-CWT)\",\n",
    "    \"and(v2_time,v2_cwt1d)\": \"AND(REDv2-Time, REDv2-CWT)\",\n",
    "    \"avg(v2_time,v2_time_perm)\": \"AVG(REDv2-Time, REDv2-Time)\",\n",
    "    \"avg(v2_cwt1d,v2_cwt1d_perm)\": \"AVG(REDv2-CWT, REDv2-CWT)\",\n",
    "    \"avg(v2_time,v2_cwt1d)\": \"AVG(REDv2-Time, REDv2-CWT)\",\n",
    "}\n",
    "print(\"\\\\toprule\")\n",
    "print(\"Datos & Detector & F1-score (\\%) & mIoU (\\%) \\\\\\\\\")\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    print(\"\\midrule\")\n",
    "    dataset_str = print_dataset_names[(config[\"dataset_name\"], config[\"expert\"])]\n",
    "    metrics_dict = metrics_list[i_config].set_index('Ensemble').to_dict('index')\n",
    "    for i_ens, ensemble in enumerate(ensembles_order):\n",
    "        mets = metrics_dict[ensemble]\n",
    "        metric_str = \"$%1.1f\\pm %1.1f$ & $%1.1f\\pm %1.1f$\" % (\n",
    "            100 * mets['F1-score_mean'], 100 * mets['F1-score_std'],\n",
    "            100 * mets['mIoU_mean'], 100 * mets['mIoU_std'])\n",
    "        dataset_to_print = '%s\\n' % dataset_str if (i_ens == 0) else ''\n",
    "        row_str = \"%s & %s & %s \\\\\\\\\" % (dataset_to_print, ensembles_print_name[ensemble].ljust(30), metric_str)\n",
    "        print(row_str)\n",
    "        \n",
    "    # Statistical tests\n",
    "    #reference_ensemble = ensembles_order[0]\n",
    "    #print(\"P-value test against %s\" % reference_ensemble)\n",
    "    #table = metrics_raw_list[i_config]\n",
    "    #for ensemble in ensembles_order[1:]:\n",
    "    #    print(\"%s:\" % ensemble.ljust(50), end='')\n",
    "    #    for metric_name in [\"F1-score\", \"mIoU\"]:\n",
    "    #        model_metrics = table[table[\"Ensemble\"] == ensemble][metric_name].values\n",
    "    #        reference_metrics = table[table[\"Ensemble\"] == reference_ensemble][metric_name].values\n",
    "    #        pvalue = stats.ttest_ind(model_metrics, reference_metrics, equal_var=False)[1]\n",
    "    #        print(\" P(%s) %1.4f\" % (metric_name, pvalue), end='')\n",
    "    #    print(\"\")\n",
    "print(\"\\\\bottomrule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325a090",
   "metadata": {},
   "source": [
    "# Perturbaciones en la entrada\n",
    "Medir cambios en SubsetMetric vs perturbacion, donde subset metric es la metrica mean+-std del 5CV del dataset, y puede ser f1-score, recall, precision y mIoU (lo bueno es que las 4 están en el rango 0-1).\n",
    "- Escalar la señal de entrada. Curva continua\n",
    "- Inversión de voltaje (multiplicar por -1) y de tiempo (flipped signal y labels, para implementarlo se podria hacer flip de la entrada, y el vector de probabilidad de salida volver a hacerle un flip). Discreto (barras).\n",
    "- Filtrar (quitar) bandas de potencia (modelado ya entrenado). Delta separarlo en delta lenta y rapida. Discreto (barras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66974e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "metrics_list = []\n",
    "metrics_raw_list = []\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    average_mode = constants.MICRO_AVERAGE if (config[\"dataset_name\"] == constants.MODA_SS_NAME) else constants.MACRO_AVERAGE\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        pred_dict[model_version] = {}\n",
    "        tmp_pert_dict = fig_utils.get_red_predictions_for_perturbations(model_version, config[\"strategy\"], dataset, config[\"expert\"], verbose=False)\n",
    "        perturbations = list(tmp_pert_dict.keys())\n",
    "        for perturbation_name in perturbations:\n",
    "            pred_dict[model_version][perturbation_name] = {}\n",
    "            tmp_dict = tmp_pert_dict[perturbation_name]\n",
    "            for k in tmp_dict.keys():\n",
    "                fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "                fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "                pred_dict[model_version][perturbation_name][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "\n",
    "    # Performance\n",
    "    table = {'Detector': [], 'Perturbación': [], 'F1-score': [], 'Recall': [], 'Precision': [], 'mIoU': [], 'Fold': []}\n",
    "    # Measure performance of model with itself\n",
    "    for model_version in models:\n",
    "        for perturbation_name in perturbations:\n",
    "            for k in range(n_folds):\n",
    "                subject_ids = test_ids_list[k]\n",
    "                feed_d = FeederDataset(dataset, subject_ids, constants.N2_RECORD, which_expert=config[\"expert\"])\n",
    "                events_list = feed_d.get_stamps()\n",
    "                detections_list = [pred_dict[model_version][perturbation_name][k][subject_id] for subject_id in subject_ids]\n",
    "                performance = fig_utils.compute_fold_performance(events_list, detections_list, average_mode)\n",
    "                table['Detector'].append(model_version)\n",
    "                table['Perturbación'].append(perturbation_name)\n",
    "                table['F1-score'].append(performance['F1-score'])\n",
    "                table['Recall'].append(performance['Recall'])\n",
    "                table['Precision'].append(performance['Precision'])\n",
    "                table['mIoU'].append(performance['mIoU'])\n",
    "                table['Fold'].append(k)\n",
    "    table = pd.DataFrame.from_dict(table)\n",
    "    print(\"By-fold statistics\")\n",
    "    metrics_raw_list.append(table)\n",
    "    mean_table = table.groupby(by=[\"Detector\", \"Perturbación\"]).mean().drop(columns=[\"Fold\"]).add_suffix(\"_mean\")\n",
    "    std_table = table.groupby(by=[\"Detector\", \"Perturbación\"]).std(ddof=0).drop(columns=[\"Fold\"]).add_suffix(\"_std\")\n",
    "    subgroup_stats_table = mean_table.join(std_table)\n",
    "    subgroup_stats_table = subgroup_stats_table.reindex(sorted(subgroup_stats_table.columns), axis=1)\n",
    "    subgroup_stats_table = subgroup_stats_table.reset_index()\n",
    "    metrics_list.append(subgroup_stats_table)  \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc963194",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate table for latex\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "pert_prefix_order = ['scale', 'invert', 'filter']\n",
    "pert_labels = {\n",
    "    'scale': 'Factor de escala',\n",
    "    'invert': 'Inversión',\n",
    "    'filter': 'Filtro rechaza-banda',\n",
    "}\n",
    "pert_sorted_values = {\n",
    "    'scale': np.round(np.arange(0.5, 1.5 + 0.001, 0.1), 1),\n",
    "    'invert': ['value', 'time'],\n",
    "    'filter': [(0, 2), (2, 4), (4, 8), (8, 11), (10, 16), (16, 30)]\n",
    "}\n",
    "pert_names_table = {\n",
    "    'scale': {x: '%1.1f' % x for x in pert_sorted_values['scale']},\n",
    "    'invert': {x: x for x in pert_sorted_values['invert']},\n",
    "    'filter': {b: '%d-%d' % (b[0], b[1]) for b in pert_sorted_values['filter']}\n",
    "}\n",
    "pert_names_print = {\n",
    "    'scale': {x: '%1.1f' % x for x in pert_sorted_values['scale']},\n",
    "    'invert': {'time': 'Tiempo', 'value': 'Amplitud'},\n",
    "    'filter': {b: '%d-%d Hz' % (b[0], b[1]) for b in pert_sorted_values['filter']}\n",
    "}\n",
    "print(\"Datos & Perturbación & Valor & %s \\\\\\\\\" % \" & \".join([print_model_names[model_name] for model_name in ref_order]))\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]\n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    n_models = len(model_names)\n",
    "    table = table[['Detector', 'Perturbación', 'F1-score_mean', 'F1-score_std']]\n",
    "    perturbations = np.unique(table['Perturbación'])\n",
    "    print(\"\\n%s\" % dataset_str)\n",
    "    for pert_prefix in pert_prefix_order:      \n",
    "        print(\"& %s\" % pert_labels[pert_prefix])\n",
    "        for pert_value in pert_sorted_values[pert_prefix]:\n",
    "            metric_str_list = []\n",
    "            pert_name_table = '%s-%s' % (pert_prefix, pert_names_table[pert_prefix][pert_value])\n",
    "            for model_name in model_names:\n",
    "                model_table = table[(table['Detector'] == model_name) & (np.isin(table['Perturbación'], pert_name_table))].drop(columns=['Detector'])\n",
    "                model_table = model_table.set_index('Perturbación')\n",
    "                metrics_dict = model_table.to_dict('index')\n",
    "                metric_str = '$%1.1f\\pm %1.1f$' % (\n",
    "                    100 * metrics_dict[pert_name_table]['F1-score_mean'], 100 * metrics_dict[pert_name_table]['F1-score_std'])\n",
    "                metric_str_list.append(metric_str)\n",
    "            metric_str = \" & \".join(metric_str_list)\n",
    "            pert_name_to_print = pert_names_print[pert_prefix][pert_value]\n",
    "            metric_str = \" & & %s & %s \\\\\\\\\" % (pert_name_to_print, metric_str)\n",
    "            print(metric_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "\n",
    "save_figure = True\n",
    "markersize = 4\n",
    "number_of_std = 1\n",
    "groups_total_width = 0.5\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "baseline_color = viz.GREY_COLORS[8]\n",
    "model_specs = {\n",
    "    constants.V2_TIME: dict(marker='o', color=viz.PALETTE['blue']),\n",
    "    constants.V2_CWT1D: dict(marker='o', color=viz.PALETTE['red']),\n",
    "}\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT',\n",
    "}\n",
    "ref_order = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "pert_prefix_order = ['scale', 'invert', 'filter']\n",
    "pert_labels = {\n",
    "    'scale': 'Factor de escala',\n",
    "    'invert': 'Inversión',\n",
    "    'filter': 'Filtro rechaza-banda (Hz)',\n",
    "}\n",
    "pert_sorted_values = {\n",
    "    'scale': np.round(np.arange(0.5, 1.5 + 0.001, 0.1), 1),\n",
    "    'invert': ['none', 'value', 'time'],\n",
    "    'filter': [(0, 2), (2, 4), (4, 8), (8, 11), (10, 16), (16, 30)]\n",
    "}\n",
    "pert_names_table = {\n",
    "    'scale': {x: '%1.1f' % x for x in pert_sorted_values['scale']},\n",
    "    'invert': {x: x for x in pert_sorted_values['invert']},\n",
    "    'filter': {b: '%d-%d' % (b[0], b[1]) for b in pert_sorted_values['filter']}\n",
    "}\n",
    "pert_names_print = {\n",
    "    'scale': {x: '%1.1f' % x for x in pert_sorted_values['scale']},\n",
    "    'invert': {'none': 'Ninguna', 'time': 'Tiempo', 'value': 'Amplitud'},\n",
    "    'filter': {b: '%d-%d' % (b[0], b[1]) for b in pert_sorted_values['filter']}\n",
    "}\n",
    "metrics_sorted = ['F1-score', 'Recall', 'Precision', 'mIoU']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 9), dpi=200)\n",
    "axes = np.concatenate([axes[:1, :], axes[1:, :]], axis=1) \n",
    "axes = axes.flatten()\n",
    "\n",
    "ax_loc = -1  # global loc\n",
    "for i_config, config in enumerate(eval_configs):\n",
    "    table = metrics_list[i_config]\n",
    "    model_names = [m for m in ref_order if m in np.unique(table['Detector'])]\n",
    "    for model_name in model_names:\n",
    "        original_perf_row = table[(table['Detector'] == model_name) & (table['Perturbación'] == 'scale-1.0')].to_dict(orient='records')[0]\n",
    "        original_perf_row['Perturbación'] = 'invert-none'\n",
    "        table = table.append(original_perf_row, ignore_index=True)\n",
    "    n_models = len(model_names)\n",
    "    for pert_prefix in pert_prefix_order:\n",
    "        ax_loc += 1\n",
    "        ax = axes[ax_loc]\n",
    "        pert_values = pert_sorted_values[pert_prefix]\n",
    "        pert_table = ['%s-%s' % (pert_prefix, pert_names_table[pert_prefix][pert_value]) for pert_value in pert_values]\n",
    "    \n",
    "        n_groups = len(pert_values)\n",
    "        positions = np.arange(n_groups)\n",
    "        groups_width = groups_total_width / n_models\n",
    "        initial_group_center_offset = groups_width * (1 - n_models) / 2\n",
    "        offsets = initial_group_center_offset + np.arange(n_models) * groups_width\n",
    "        distance_from_edge = groups_total_width / 2 + 0.05\n",
    "\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            \n",
    "            model_table = table[(table['Detector'] == model_name) & (np.isin(table['Perturbación'], pert_table))].drop(columns=['Detector'])\n",
    "            model_table = model_table.set_index('Perturbación')\n",
    "            metrics_dict = model_table.to_dict('index')\n",
    "            \n",
    "            metric_lims = []\n",
    "            for i, metric_name in enumerate(metrics_sorted):\n",
    "                offset = - 1.2 * i\n",
    "                metric_lims.append([offset, offset + 1])\n",
    "                # Margins\n",
    "                ax.axhline(offset, linewidth=0.8, color=\"k\")\n",
    "                ax.axhline(offset + 1, linewidth=0.8, color=\"k\")\n",
    "                ax.plot(\n",
    "                    [positions[0]-distance_from_edge, positions[0]-distance_from_edge], \n",
    "                    [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "                ax.plot(\n",
    "                    [positions[-1]+distance_from_edge, positions[-1]+distance_from_edge], \n",
    "                    [offset + 0.01, offset + 1 - 0.01], linewidth=1.6, color=\"k\")\n",
    "                if pert_prefix == 'scale':\n",
    "                    center_pos = n_groups // 2\n",
    "                    ax.plot(\n",
    "                        [positions[center_pos], positions[center_pos]], \n",
    "                        [offset + 0.01, offset + 1 - 0.01], linewidth=1.1, color=\"k\", zorder=10, linestyle=\"--\")\n",
    "                # Data\n",
    "                mean_data = offset + np.array([metrics_dict[pert_name_table]['%s_mean' % metric_name] for pert_name_table in pert_table])\n",
    "                std_data = np.array([metrics_dict[pert_name_table]['%s_std' % metric_name] for pert_name_table in pert_table])\n",
    "                this_positions = positions + offsets[j]\n",
    "                ax.plot(\n",
    "                    this_positions, mean_data, label=print_model_names[model_name], linestyle=\"None\",\n",
    "                    marker=model_specs[model_name][\"marker\"], markersize=markersize, markeredgewidth=0.0,\n",
    "                    color=model_specs[model_name][\"color\"], zorder=30)\n",
    "                for i_sg in range(n_groups):\n",
    "                    ax.plot(\n",
    "                        [this_positions[i_sg], this_positions[i_sg]],\n",
    "                        [\n",
    "                            mean_data[i_sg] - number_of_std*std_data[i_sg], \n",
    "                            mean_data[i_sg] + number_of_std*std_data[i_sg]\n",
    "                        ],\n",
    "                        linewidth=1, color=model_specs[model_name][\"color\"], zorder=20\n",
    "                    )\n",
    "        \n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlim([positions[0] - distance_from_edge, positions[-1] + distance_from_edge])\n",
    "        ax.set_ylim([offset, 1])\n",
    "        yticks = np.concatenate([[m[0], (m[0]+m[1])/2, m[1]] for m in metric_lims])\n",
    "        yticklabels = np.concatenate([[0, n, 1] for n in metrics_sorted])\n",
    "        yticks_minor = np.concatenate([np.arange(m[0], m[1]+0.001, 0.1) for m in metric_lims])\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "        ax.set_yticks(yticks_minor, minor=True)\n",
    "        for i_tick, t in enumerate(ax.get_yticklabels()):\n",
    "            if i_tick % 3 == 1:\n",
    "                t.set_rotation('vertical')\n",
    "                t.set_verticalalignment('center')\n",
    "        ax.grid(axis=\"y\", which=\"minor\")\n",
    "        ax.yaxis.labelpad = -8\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xticks([i for i in range(n_groups)])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xticklabels([pert_names_print[pert_prefix][pert_value] for pert_value in pert_values])\n",
    "        ax.set_xlabel(pert_labels[pert_prefix], fontsize=8)\n",
    "        dataset_str = print_dataset_names[(config['dataset_name'], config['expert'])]\n",
    "        ax.set_title(\"Perturbación en %s\" % dataset_str, loc=\"center\", fontsize=8)   \n",
    "        ax.text(\n",
    "            x=-0.01, y=1.02, fontsize=16, s=r\"$\\bf{%s}$\" % letters[ax_loc],\n",
    "            ha=\"left\", transform=ax.transAxes)\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "# Get legend methods\n",
    "labels_to_lines_dict = {}\n",
    "for ax in axes:\n",
    "    t_lines, t_labels = ax.get_legend_handles_labels()\n",
    "    for lbl, lin in zip(t_labels, t_lines):\n",
    "        labels_to_lines_dict[lbl] = lin\n",
    "labels = [\"REDv2-Time\", \"REDv2-CWT\"]\n",
    "lines = [labels_to_lines_dict[lbl] for lbl in labels]\n",
    "lg1 = fig.legend(\n",
    "    lines, labels, fontsize=8, loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.01), ncol=len(labels), frameon=False, handletextpad=0.5)\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_indata_perturbations\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_extra_artists=(lg1,), bbox_inches=\"tight\", pad_inches=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79872dd7",
   "metadata": {},
   "source": [
    "# PINK\n",
    "\n",
    "Respuesta del modelo ya entrenado ante ruido rosado , incluyendo qué pasa si el ruido rosado cambia su desviacion estandar (escalamiento de 1 a 2 con paso 0.5). Medir tasa promedio de falsos por horas de la señal, y visualizar detecciones en caso de haber, y ver qué tan estable es dicha deteccion (si la predicen varios checkpoints o no). \n",
    "\n",
    "Todos los checkpoints predicen todo pink.\n",
    "\n",
    "Ideas:\n",
    "- Numero de detecciones por minuto u hora vs escala.\n",
    "- Probabilidad asignada a las detecciones vs escala.\n",
    "- Para la escala 1.0: Distribucion de amplitud, duracion, y frecuencia si es SS.\n",
    "- Visualización de algunas detecciones con amplitud creciente, mostrando los vectores de probabilidad ajustada de todos los folds.\n",
    "- Grand-average de \"complejo K\" alineado con el peak negativo (minimo valor dentro de la marca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [constants.V2_TIME, constants.V2_CWT1D]\n",
    "print_model_names = {\n",
    "    constants.V2_TIME: 'REDv2-Time',\n",
    "    constants.V2_CWT1D: 'REDv2-CWT'\n",
    "}\n",
    "print_dataset_names = {\n",
    "    (constants.MASS_SS_NAME, 1): \"MASS-SS2-E1SS\",\n",
    "    (constants.MASS_SS_NAME, 2): \"MASS-SS2-E2SS\",\n",
    "    (constants.MASS_KC_NAME, 1): \"MASS-SS2-KC\",\n",
    "    (constants.MODA_SS_NAME, 1): \"MASS-MODA\",\n",
    "    (constants.INTA_SS_NAME, 1): \"INTA-UCH\",\n",
    "}\n",
    "\n",
    "eval_configs = [\n",
    "    dict(dataset_name=constants.MODA_SS_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "    dict(dataset_name=constants.MASS_KC_NAME, expert=1, strategy='5cv', seeds=3),\n",
    "]\n",
    "for config in eval_configs:\n",
    "    print(\"\\nLoading\", config)\n",
    "    source_dataset = reader.load_dataset(config[\"dataset_name\"], verbose=False)\n",
    "    _, _, test_ids_list = get_partitions(source_dataset, config[\"strategy\"], config[\"seeds\"])\n",
    "    n_folds = len(test_ids_list)\n",
    "    # Collect predictions\n",
    "    pred_dict = {}\n",
    "    for model_version in models:\n",
    "        pred_dict[model_version] = {}\n",
    "        tmp_pert_dict = fig_utils.get_red_predictions_for_pink(model_version, config[\"strategy\"], source_dataset, config[\"expert\"], verbose=False)\n",
    "        perturbations = list(tmp_pert_dict.keys())\n",
    "        for perturbation_name in perturbations:\n",
    "            pred_dict[model_version][perturbation_name] = {}\n",
    "            tmp_dict = tmp_pert_dict[perturbation_name]\n",
    "            for k in tmp_dict.keys():\n",
    "                fold_subjects = tmp_dict[k][constants.TEST_SUBSET].all_ids\n",
    "                fold_predictions = tmp_dict[k][constants.TEST_SUBSET].get_stamps()\n",
    "                pred_dict[model_version][perturbation_name][k] = {s: pred for s, pred in zip(fold_subjects, fold_predictions)}\n",
    "    # Medir numero de detecciones por hora\n",
    "    time_unit_in_seconds = 3600\n",
    "    average_mode = constants.MACRO_AVERAGE\n",
    "    \n",
    "        \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734aa80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict['v2_time']['scale-1.0'][14][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81736621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pink = reader.load_dataset(constants.PINK_NAME, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_signal = pink.get_subject_signal(subject_id=20, normalize_clip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = np.array(\n",
    "    [449240, 449343]\n",
    ")\n",
    "\n",
    "fs = pink.fs\n",
    "center_sample = int(mark.mean())\n",
    "window_size = int(10 * fs)\n",
    "start_sample = int(center_sample - window_size // 2)\n",
    "end_sample = start_sample + window_size\n",
    "segment_signal = tmp_signal[start_sample:end_sample]\n",
    "time_axis = np.arange(start_sample, end_sample) / fs\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=100)\n",
    "ax.plot(time_axis, segment_signal, linewidth=0.8)\n",
    "ax.fill_between(mark / fs, -100, 100, facecolor='k', alpha=0.2)\n",
    "ax.set_ylim([-150, 150])\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel(\"Tiempo (s)\", fontsize=8)\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
