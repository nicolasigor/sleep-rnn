{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntapia/miniconda3/envs/tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants, viz, pkeys\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import det_utils\n",
    "from figs_thesis import fig_utils\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event\n",
    "\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')\n",
    "LETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(parts_to_load, dataset, thr=0.5, verbose=True):\n",
    "    if thr == 0.5:\n",
    "        extra_str = ''\n",
    "    else:\n",
    "        extra_str = '_%1.2f' % thr\n",
    "    pred_objects = []\n",
    "    for part in parts_to_load:\n",
    "        filepath = os.path.join(\n",
    "            RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "            'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "            'v2_time',\n",
    "            'prediction%s_part%d.pkl' % (extra_str, part)\n",
    "        )\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            pred_object = pickle.load(handle)\n",
    "        pred_object.set_parent_dataset(dataset)\n",
    "        pred_objects.append(pred_object)\n",
    "    return pred_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NSRR dataset and pre-computed predicted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsrr_ss with 3 patients.\n",
      "Loading from checkpoint... Loaded\n",
      "Global STD: None\n",
      "Dataset nsrr_ss with 11593 patients.\n"
     ]
    }
   ],
   "source": [
    "parts_to_load = [0]  # 0 to 11\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True)\n",
    "pred_objects_1 = load_predictions(parts_to_load, nsrr)\n",
    "pred_objects_0 = load_predictions(parts_to_load, nsrr, thr=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames of dataset checkpoints\n",
    "byevent_proba_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_byevent_proba.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating table of parameters\n",
      "Processing Part 1 / 1\n",
      "145 1778\n",
      "(326,)\n",
      "(1778, 2)\n",
      "(1778, 2)\n",
      "145 1778\n",
      "207 1642\n",
      "(435,)\n",
      "(1642, 2)\n",
      "(1642, 2)\n",
      "207 1642\n",
      "212 1316\n",
      "(445,)\n",
      "(1316, 2)\n",
      "(1316, 2)\n",
      "212 1316\n",
      "206 1113\n",
      "(428,)\n",
      "(1113, 2)\n",
      "(1113, 2)\n",
      "206 1113\n",
      "266 1072\n",
      "(476,)\n",
      "(1072, 2)\n",
      "(1072, 2)\n",
      "266 1072\n",
      "326 2114\n",
      "(490,)\n",
      "(2114, 2)\n",
      "(2114, 2)\n",
      "326 2114\n",
      "188 1601\n",
      "(487,)\n",
      "(1601, 2)\n",
      "(1601, 2)\n",
      "188 1601\n",
      "190 1159\n",
      "(393,)\n",
      "(1159, 2)\n",
      "(1159, 2)\n",
      "190 1159\n",
      "104 384\n",
      "(366,)\n",
      "(384, 2)\n",
      "(384, 2)\n",
      "104 384\n",
      "213 1154\n",
      "(486,)\n",
      "(1154, 2)\n",
      "(1154, 2)\n",
      "213 1154\n",
      "Done.\n",
      "15390\n",
      "(15390, 6)\n"
     ]
    }
   ],
   "source": [
    "params_load_checkpoint = False\n",
    "\n",
    "# ############################\n",
    "\n",
    "if params_load_checkpoint:\n",
    "    print(\"Loading from checkpoint\")\n",
    "    table_byevent_proba = pd.read_csv(byevent_proba_ckpt_path)\n",
    "\n",
    "else:\n",
    "    # Perform computation and save checkpoint\n",
    "    table_byevent_proba = {\n",
    "        'subject_id': [],\n",
    "        'center_sample': [],\n",
    "        'prediction_part': [],\n",
    "        'category': [],\n",
    "        'probability': [],\n",
    "        'duration': [], \n",
    "        #'frequency': [],\n",
    "        #'amplitude_pp': [],\n",
    "        #'amplitude_rms': [],\n",
    "        #'c10_density': [],\n",
    "        #'c20_density': [],\n",
    "    }\n",
    "\n",
    "    min_n2_minutes = 60\n",
    "    verbose_min_minutes = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Generating table of parameters\")\n",
    "    n_parts = len(pred_objects_1)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for part_id in range(n_parts):\n",
    "        predictions_1 = pred_objects_1[part_id]\n",
    "        predictions_0 = pred_objects_0[part_id]\n",
    "        print(\"Processing Part %d / %d\" % (part_id + 1, n_parts))\n",
    "        for i_subject in range(10):\n",
    "        # for i_subject in tqdm(range(100)):\n",
    "            subject_id = predictions_1.all_ids[i_subject]\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "            if n2_minutes < min_n2_minutes:\n",
    "                if verbose_min_minutes:\n",
    "                    print(\"Skipped by N2 minutes: Subject %s with %d N2 minutes\" % (subject_id, n2_minutes))\n",
    "                continue\n",
    "\n",
    "            # Class 1 spindles (real):\n",
    "            marks_1 = predictions_1.get_subject_stamps(subject_id, pages_subset='wn')\n",
    "            # Class 0 \"spindles\" (false):\n",
    "            marks_0 = predictions_0.get_subject_stamps(subject_id, pages_subset='wn')\n",
    "            # Let only those class 0 without intersecting class 1\n",
    "            # If marks_1.size = 0 then marks_0 is by definition not intersecting\n",
    "            if marks_1.size > 0:\n",
    "                ov_mat = utils.get_overlap_matrix(marks_0, marks_1)\n",
    "                is_intersecting = ov_mat.sum(axis=1)\n",
    "                marks_0 = marks_0[is_intersecting == 0]\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "            \n",
    "            #print(marks_0.shape[0], marks_1.shape[0])\n",
    "            \n",
    "            # Now only keep N2 stage marks\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            print(n2_pages.shape)\n",
    "            page_size = int(nsrr.fs * nsrr.original_page_duration)\n",
    "            if marks_1.size > 0:\n",
    "                #print(marks_1.shape)\n",
    "                marks_1 = utils.extract_pages_for_stamps(marks_1, n2_pages, page_size)\n",
    "                #print(marks_1.shape)\n",
    "            #if marks_0.size > 0:\n",
    "            #    marks_0 = utils.extract_pages_for_stamps(marks_0, n2_pages, page_size)\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "                \n",
    "            print(marks_0.shape[0], marks_1.shape[0])\n",
    "            \n",
    "            marks = []\n",
    "            marks_class = []\n",
    "            if marks_1.size > 0:\n",
    "                marks.append(marks_1)\n",
    "                marks_class.append([1] * marks_1.shape[0])\n",
    "            if marks_0.size > 0:\n",
    "                marks.append(marks_0)\n",
    "                marks_class.append([0] * marks_0.shape[0])\n",
    "            marks = np.concatenate(marks, axis=0).astype(np.int32)\n",
    "            marks_class = np.concatenate(marks_class).astype(np.int32)\n",
    "            n_marks = marks.shape[0]\n",
    "            counter += n_marks\n",
    "            \n",
    "            # Extract proba\n",
    "            subject_proba = predictions_1.get_subject_probabilities(subject_id, return_adjusted=False)\n",
    "            marks_proba = det_utils.get_event_probabilities(marks, subject_proba, downsampling_factor=8, proba_prc=75)\n",
    "            marks_proba = marks_proba.astype(np.float32)\n",
    "            \n",
    "            # Parameters\n",
    "            duration = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "            \n",
    "            table_byevent_proba['subject_id'].append([subject_id] * n_marks)\n",
    "            table_byevent_proba['center_sample'].append(marks.mean(axis=1).astype(np.int32))\n",
    "            table_byevent_proba['prediction_part'].append(np.array([part_id] * n_marks, dtype=np.int32))\n",
    "            table_byevent_proba['category'].append(marks_class)\n",
    "            table_byevent_proba['probability'].append(marks_proba)\n",
    "            table_byevent_proba['duration'].append(duration)\n",
    "            \n",
    "    for key in table_byevent_proba:\n",
    "        table_byevent_proba[key] = np.concatenate(table_byevent_proba[key])\n",
    "    table_byevent_proba = pd.DataFrame.from_dict(table_byevent_proba)\n",
    "    print(\"Done.\") \n",
    "    \n",
    "print(counter)\n",
    "print(table_byevent_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = utils.extract_pages_for_stamps(marks_1, [10], page_size)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61544, 61695],\n",
       "       [62560, 62639],\n",
       "       [64144, 64319],\n",
       "       [65272, 65423]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(table_byevent_proba.probability, table_byevent_proba.category)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = table_byevent_proba.category == 0\n",
    "plt.scatter(table_byevent_proba.probability[locs], table_byevent_proba.duration[locs], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba[\n",
    "    (table_byevent_proba.category == 0) & (table_byevent_proba.probability > 0.6)\n",
    "].sort_values(by=\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba[\n",
    "    (table_byevent_proba.subject_id == \"ccshs-trec-1800065\") \n",
    "    & (table_byevent_proba.center_sample > 1692000)\n",
    "    & (table_byevent_proba.center_sample < 1692400)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "loc_to_viz = 5197\n",
    "window_duration = 20\n",
    "\n",
    "#\n",
    "subject_info = table_byevent_proba.loc[loc_to_viz]\n",
    "print(subject_info)\n",
    "subject_data = nsrr.read_subject_data(subject_info.subject_id, exclusion_of_pages=False)\n",
    "signal = subject_data['signal']\n",
    "predictions = pred_objects_1[subject_info.prediction_part]\n",
    "center_sample = subject_info.center_sample\n",
    "start_sample = int(center_sample - window_duration * nsrr.fs // 2)\n",
    "end_sample = int(start_sample + window_duration * nsrr.fs)\n",
    "proba = predictions.get_subject_probabilities(\n",
    "    subject_info.subject_id, )\n",
    "proba_up = np.repeat(proba, 8)\n",
    "time_axis = np.arange(start_sample, end_sample) / nsrr.fs\n",
    "n2_pages = predictions.data[subject_info.subject_id]['n2_pages']\n",
    "n2_pages_vector = np.zeros(signal.shape, dtype=np.int32)\n",
    "page_size = int(nsrr.original_page_duration * nsrr.fs)\n",
    "for p in n2_pages:\n",
    "    start_page = p * page_size\n",
    "    end_page = start_page + page_size\n",
    "    n2_pages_vector[start_page:end_page] = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2.5), dpi=140)\n",
    "ax.plot(time_axis, signal[start_sample:end_sample], linewidth=.6)\n",
    "ax.fill_between(\n",
    "    time_axis,\n",
    "    200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    -200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    facecolor=\"k\", alpha=0.1\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    time_axis, \n",
    "    -300 - 50 * proba_up[start_sample:end_sample], \n",
    "    -300 + 50 * proba_up[start_sample:end_sample],\n",
    "    color=viz.PALETTE['red'], alpha=1.0\n",
    ")\n",
    "ax.axhline(-300 - 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 + 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 - 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 0, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.set_ylim([-400, 200])\n",
    "ax.set_xlim([start_sample/nsrr.fs, end_sample/nsrr.fs])\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "ax.tick_params(labelsize=8)\n",
    "title_str = 'Subject %s. Loc %d. Center category %d' % (subject_info.subject_id, loc_to_viz, subject_info.category)\n",
    "ax.set_title(title_str)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
