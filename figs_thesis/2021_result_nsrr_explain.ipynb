{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants, viz, pkeys\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import det_utils\n",
    "from figs_thesis import fig_utils\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event\n",
    "\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')\n",
    "LETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(parts_to_load, dataset, thr=0.5, verbose=True):\n",
    "    if thr == 0.5:\n",
    "        extra_str = ''\n",
    "    else:\n",
    "        extra_str = '_%1.2f' % thr\n",
    "    pred_objects = []\n",
    "    for part in parts_to_load:\n",
    "        filepath = os.path.join(\n",
    "            RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "            'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "            'v2_time',\n",
    "            'prediction%s_part%d.pkl' % (extra_str, part)\n",
    "        )\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            pred_object = pickle.load(handle)\n",
    "        pred_object.set_parent_dataset(dataset)\n",
    "        pred_objects.append(pred_object)\n",
    "    return pred_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NSRR dataset and pre-computed predicted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_to_load = [0]  # 0 to 11\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True, params={pkeys.PAGE_DURATION: 30})\n",
    "pred_objects_1 = load_predictions(parts_to_load, nsrr)\n",
    "pred_objects_0 = load_predictions(parts_to_load, nsrr, thr=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames of dataset checkpoints\n",
    "byevent_proba_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_byevent_proba.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages_for_stamps_strict(stamps, pages_indices, page_size):\n",
    "    \"\"\"Returns stamps that are at completely contained on pages.\"\"\"\n",
    "    stamps_start_page = np.floor(stamps[:, 0] / page_size)\n",
    "    stamps_end_page = np.floor(stamps[:, 1] / page_size)\n",
    "    useful_idx = np.where(\n",
    "        np.isin(stamps_start_page, pages_indices) & np.isin(stamps_end_page, pages_indices)\n",
    "    )[0]\n",
    "    pages_data = stamps[useful_idx, :]\n",
    "    return pages_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_load_checkpoint = True\n",
    "\n",
    "# ############################\n",
    "\n",
    "if params_load_checkpoint:\n",
    "    print(\"Loading from checkpoint\")\n",
    "    table_byevent_proba = pd.read_csv(byevent_proba_ckpt_path)\n",
    "\n",
    "else:\n",
    "    # Perform computation and save checkpoint\n",
    "    \n",
    "    bands_for_mean_power = [\n",
    "        (0, 2),\n",
    "        (2, 4),\n",
    "        (4, 8),\n",
    "        (8, 10),\n",
    "        (11, 16),\n",
    "        (16, 30),\n",
    "        (4.5, 30),\n",
    "    ]\n",
    "\n",
    "    table_byevent_proba = {\n",
    "        'subject_id': [],\n",
    "        'age': [],\n",
    "        'female': [],\n",
    "        'center_sample': [],\n",
    "        'prediction_part': [],\n",
    "        'category': [],\n",
    "        'probability': [],\n",
    "        'duration': [], \n",
    "        'frequency': [],\n",
    "        'amplitude_pp': [],\n",
    "        'amplitude_rms': [],\n",
    "        'correlation': [],\n",
    "        'covariance': [],\n",
    "        'c10_density_real': [],\n",
    "        'c10_density_all': [],\n",
    "        'c10_abs_sigma_power': [],\n",
    "        'c10_rel_sigma_power': [],\n",
    "        'c10_abs_sigma_power_masked': [],\n",
    "        'c10_rel_sigma_power_masked': [],\n",
    "        'c20_density_real': [],\n",
    "        'c20_density_all': [],\n",
    "        'c20_abs_sigma_power': [],\n",
    "        'c20_rel_sigma_power': [],\n",
    "        'c20_abs_sigma_power_masked': [],\n",
    "        'c20_rel_sigma_power_masked': [],\n",
    "    }\n",
    "    table_byevent_proba.update(\n",
    "        {'mean_power_%s_%s' % band: [] for band in bands_for_mean_power}\n",
    "    )\n",
    "\n",
    "    min_n2_minutes = 60\n",
    "    verbose_min_minutes = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Generating table of parameters\")\n",
    "    n_parts = len(pred_objects_1)\n",
    "    for part_id in range(n_parts):\n",
    "        predictions_1 = pred_objects_1[part_id]\n",
    "        predictions_0 = pred_objects_0[part_id]\n",
    "        print(\"Processing Part %d / %d\" % (part_id + 1, n_parts))\n",
    "        \n",
    "        n_subjects = len(predictions_1.all_ids)\n",
    "        \n",
    "        for i_subject in tqdm(range(n_subjects)):\n",
    "            subject_id = predictions_1.all_ids[i_subject]\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "            if n2_minutes < min_n2_minutes:\n",
    "                if verbose_min_minutes:\n",
    "                    print(\"Skipped by N2 minutes: Subject %s with %d N2 minutes\" % (subject_id, n2_minutes))\n",
    "                continue\n",
    "\n",
    "            marks_1 = predictions_1.get_subject_stamps(subject_id)  # Class 1 spindles (real)\n",
    "            marks_0 = predictions_0.get_subject_stamps(subject_id)  # Class 0 \"spindles\" (false)\n",
    "            # Let only those class 0 without intersecting class 1\n",
    "            # If marks_1.size = 0 then marks_0 is by definition not intersecting\n",
    "            if marks_1.size > 0:\n",
    "                ov_mat = utils.get_overlap_matrix(marks_0, marks_1)\n",
    "                is_intersecting = ov_mat.sum(axis=1)\n",
    "                marks_0 = marks_0[is_intersecting == 0]\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "            \n",
    "            # Now only keep N2 stage marks\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            page_size = int(nsrr.fs * nsrr.original_page_duration)\n",
    "            if marks_1.size > 0:\n",
    "                marks_1 = extract_pages_for_stamps_strict(marks_1, n2_pages, page_size)\n",
    "            if marks_0.size > 0:\n",
    "                marks_0 = extract_pages_for_stamps_strict(marks_0, n2_pages, page_size)\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "                \n",
    "            marks = []\n",
    "            marks_class = []\n",
    "            if marks_1.size > 0:\n",
    "                marks.append(marks_1)\n",
    "                marks_class.append([1] * marks_1.shape[0])\n",
    "            if marks_0.size > 0:\n",
    "                marks.append(marks_0)\n",
    "                marks_class.append([0] * marks_0.shape[0])\n",
    "            marks = np.concatenate(marks, axis=0).astype(np.int32)\n",
    "            marks_class = np.concatenate(marks_class).astype(np.int32)\n",
    "            n_marks = marks.shape[0]\n",
    "            \n",
    "            # Extract proba\n",
    "            subject_proba = predictions_1.get_subject_probabilities(subject_id, return_adjusted=False)\n",
    "            marks_proba = det_utils.get_event_probabilities(marks, subject_proba, downsampling_factor=8, proba_prc=75)\n",
    "            marks_proba = marks_proba.astype(np.float32)\n",
    "            # Extract signal\n",
    "            subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "            signal = subject_data['signal'].astype(np.float64)\n",
    "            age = float(subject_data['age'].item())\n",
    "            female = int(subject_data['sex'].item() == 'f')\n",
    "            \n",
    "            # Parameters\n",
    "            be_duration = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "\n",
    "            filt_signal = param_filtering_fn(signal, nsrr.fs, constants.SPINDLE).astype(np.float64)\n",
    "            signal_events = [filt_signal[e[0]:(e[1]+1)] for e in marks]\n",
    "\n",
    "            be_amplitude_pp = np.array([\n",
    "                param_amplitude_fn(s, nsrr.fs, constants.SPINDLE) for s in signal_events\n",
    "            ])\n",
    "            \n",
    "            be_amplitude_rms = np.array([\n",
    "                np.sqrt(np.mean(s ** 2)) for s in signal_events\n",
    "            ])\n",
    "\n",
    "            be_frequency = np.array([\n",
    "                param_frequency_fn(s, nsrr.fs) for s in signal_events\n",
    "            ])\n",
    "            \n",
    "            # New parameters\n",
    "            signal_raw_events = [signal[e[0]:(e[1]+1)] for e in marks]\n",
    "            \n",
    "            # Measure mean power\n",
    "            for band in bands_for_mean_power:\n",
    "                table_byevent_proba['mean_power_%s_%s' % band].append([])\n",
    "            for s in signal_raw_events:\n",
    "                freq, power = fig_utils.get_fft_spectrum(s, nsrr.fs, pad_to_duration=10, f_min=0, f_max=30, apply_hann_window=False)\n",
    "                for band in bands_for_mean_power:\n",
    "                    power_in_band = power[(freq >= band[0]) & (freq <= band[1])].mean()\n",
    "                    table_byevent_proba['mean_power_%s_%s' % band][-1].append(power_in_band)\n",
    "            for band in bands_for_mean_power:\n",
    "                table_byevent_proba['mean_power_%s_%s' % band][-1] = np.array(table_byevent_proba['mean_power_%s_%s' % band][-1], dtype=np.float32)\n",
    "            \n",
    "            # Covariance and correlation between sigma band and broad band\n",
    "            cov_l = []\n",
    "            corr_l = []\n",
    "            for s, filt_s in zip(signal_raw_events, signal_events):\n",
    "                s = s - s.mean()\n",
    "                filt_s = filt_s - filt_s.mean()\n",
    "                # covariance\n",
    "                cov = np.mean(s * filt_s)\n",
    "                cov_l.append(cov)\n",
    "                # correlation\n",
    "                corr = np.corrcoef(s, filt_s)[0, 1]\n",
    "                corr_l.append(corr)\n",
    "                \n",
    "            cov_l = np.array(cov_l, dtype=np.float32)\n",
    "            corr_l = np.array(corr_l, dtype=np.float32)\n",
    "            \n",
    "            # Local stuff\n",
    "            context_params = {\n",
    "                'c10_density_real': [],\n",
    "                'c10_density_all': [],\n",
    "                'c10_abs_sigma_power': [],\n",
    "                'c10_rel_sigma_power': [],\n",
    "                'c10_abs_sigma_power_masked': [],\n",
    "                'c10_rel_sigma_power_masked': [],\n",
    "                'c20_density_real': [],\n",
    "                'c20_density_all': [],\n",
    "                'c20_abs_sigma_power': [],\n",
    "                'c20_rel_sigma_power': [],\n",
    "                'c20_abs_sigma_power_masked': [],\n",
    "                'c20_rel_sigma_power_masked': [],\n",
    "            }\n",
    "            window_durations = [10, 20]\n",
    "            for i_mark, mark in enumerate(marks):\n",
    "                central_sample = mark.mean()\n",
    "                for window_duration in window_durations:\n",
    "                    window_size = int(window_duration * nsrr.fs)\n",
    "                    start_sample = int(central_sample - window_size // 2)\n",
    "                    end_sample = start_sample + window_size\n",
    "                    # Local number of marks, by category\n",
    "                    local_nmarks_real = utils.filter_stamps(marks_1, start_sample, end_sample).shape[0]\n",
    "                    local_nmarks_both = utils.filter_stamps(marks, start_sample, end_sample).shape[0]\n",
    "                    # Local sigma activity\n",
    "                    segment_signal = signal[start_sample:end_sample]\n",
    "                    \n",
    "                    # including event\n",
    "                    freq, power = utils.power_spectrum_by_sliding_window(segment_signal, nsrr.fs, window_duration=5)\n",
    "                    # a) Absolute sigma power \n",
    "                    local_abs_sigma_power = power[(freq >= 11) & (freq <= 16)].mean()\n",
    "                    # b) Relative sigma power (as in Lacourse)\n",
    "                    local_broad_power = power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "                    local_rel_sigma_power = local_abs_sigma_power / local_broad_power\n",
    "                    \n",
    "                    # masking event\n",
    "                    local_start = mark[0] - start_sample\n",
    "                    local_end = mark[1] - start_sample\n",
    "                    segment_signal_masked = segment_signal.copy()\n",
    "                    segment_signal_masked[local_start:local_end] = 0\n",
    "                    freq, power = utils.power_spectrum_by_sliding_window(segment_signal_masked, nsrr.fs, window_duration=5)\n",
    "                    # a) Absolute sigma power \n",
    "                    local_mask_abs_sigma_power = power[(freq >= 11) & (freq <= 16)].mean()\n",
    "                    # b) Relative sigma power (as in Lacourse)\n",
    "                    local_mask_broad_power = power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "                    local_mask_rel_sigma_power = local_mask_abs_sigma_power / local_mask_broad_power\n",
    "                    \n",
    "                    # Save\n",
    "                    context_params['c%d_density_real' % window_duration].append(local_nmarks_real)\n",
    "                    context_params['c%d_density_all' % window_duration].append(local_nmarks_both)\n",
    "                    context_params['c%d_abs_sigma_power' % window_duration].append(local_abs_sigma_power)\n",
    "                    context_params['c%d_rel_sigma_power' % window_duration].append(local_rel_sigma_power)\n",
    "                    context_params['c%d_abs_sigma_power_masked' % window_duration].append(local_mask_abs_sigma_power)\n",
    "                    context_params['c%d_rel_sigma_power_masked' % window_duration].append(local_mask_rel_sigma_power)\n",
    "            \n",
    "            for key in context_params.keys():\n",
    "                context_params[key] = np.array(context_params[key], dtype=np.float32)\n",
    "    \n",
    "            # New parameters\n",
    "            table_byevent_proba['subject_id'].append([subject_id] * n_marks)\n",
    "            table_byevent_proba['age'].append(np.array([age] * n_marks, dtype=np.float32))\n",
    "            table_byevent_proba['female'].append(np.array([female] * n_marks, dtype=np.int32))\n",
    "            table_byevent_proba['center_sample'].append(marks.mean(axis=1).astype(np.int32))\n",
    "            table_byevent_proba['prediction_part'].append(np.array([part_id] * n_marks, dtype=np.int32))\n",
    "            table_byevent_proba['category'].append(marks_class)\n",
    "            table_byevent_proba['probability'].append(marks_proba.astype(np.float32))\n",
    "            table_byevent_proba['duration'].append(be_duration.astype(np.float32))\n",
    "            table_byevent_proba['frequency'].append(be_frequency.astype(np.float32))\n",
    "            table_byevent_proba['amplitude_pp'].append(be_amplitude_pp.astype(np.float32))\n",
    "            table_byevent_proba['amplitude_rms'].append(be_amplitude_rms.astype(np.float32))\n",
    "            table_byevent_proba['covariance'].append(cov_l)\n",
    "            table_byevent_proba['correlation'].append(corr_l)\n",
    "            for key in context_params.keys():\n",
    "                table_byevent_proba[key].append(context_params[key])\n",
    "            \n",
    "    for key in table_byevent_proba:\n",
    "        table_byevent_proba[key] = np.concatenate(table_byevent_proba[key])\n",
    "    table_byevent_proba = pd.DataFrame.from_dict(table_byevent_proba)\n",
    "    \n",
    "    # compute relative powers\n",
    "    powers = table_byevent_proba[[col for col in table_byevent_proba.columns if 'mean_power' in col and \"11_16\" not in col]]\n",
    "    powers = powers.div(table_byevent_proba[\"mean_power_11_16\"], axis=0)\n",
    "    powers = 1.0 / powers\n",
    "    powers = powers.add_prefix(\"mean_power_11_16_relto_\")\n",
    "    table_byevent_proba = table_byevent_proba.merge(powers, left_index=True, right_index=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    print(\"Saving checkpoint\")\n",
    "    table_byevent_proba.to_csv(byevent_proba_ckpt_path, index=False)\n",
    "    print(\"Done.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba.age.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when plotting I could take the logarithm of either the abs power o the relative power\n",
    "# specially the relative power to get rid of the decision of showing a/b or b/a, since in the logarithm is just a change in sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    table_byevent_proba.probability, \n",
    "    table_byevent_proba.category - 0.25 + 0.5 * np.random.RandomState(seed=0).uniform(size=len(table_byevent_proba)),\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.xlabel(\"Probabilidad\")\n",
    "plt.ylabel(\"Categoría\")\n",
    "plt.yticks([0, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table_byevent_proba.probability, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_logit(x):\n",
    "    return np.log(x / (1-x))\n",
    "def get_proba(l):\n",
    "    return 1 / (1 + np.exp(-l))\n",
    "\n",
    "\n",
    "param_names = [n for n in table_byevent_proba.columns if n not in ['subject_id', 'center_sample', 'prediction_part', 'category', 'probability']]\n",
    "n_params = len(param_names)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_params / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 3 * n_rows), dpi=120)\n",
    "axes = axes.flatten()\n",
    "\n",
    "use_logits = True\n",
    "\n",
    "for ax, param_name in zip(axes, param_names):\n",
    "    p = table_byevent_proba.probability\n",
    "    if use_logits:\n",
    "        p = np.clip(p, a_min=1e-3, a_max=1 - 1e-3)\n",
    "        p = get_logit(p)\n",
    "        ax.axvline(get_logit(0.25), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.50), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.75), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.90), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.95), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.99), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "     \n",
    "    ax.hist2d(p, table_byevent_proba[param_name], bins=50, cmap=\"binary\")\n",
    "    #ax.plot(\n",
    "    #    p,\n",
    "    #    table_byevent_proba[param_name],\n",
    "     #   marker='o', markeredgewidth=0, markersize=4, alpha=0.1, linestyle=\"none\",\n",
    "     #   color=viz.PALETTE['blue'],\n",
    "    #)\n",
    "    ax.set_title(param_name, fontsize=8)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba[\n",
    "    (table_byevent_proba.category == 1) & (table_byevent_proba.probability < 0.5)\n",
    "].sort_values(by=\"probability\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_to_viz = 2\n",
    "window_duration = 20\n",
    "\n",
    "# -------------------\n",
    "subject_info = table_byevent_proba.loc[loc_to_viz]\n",
    "print(subject_info)\n",
    "subject_data = nsrr.read_subject_data(subject_info.subject_id, exclusion_of_pages=False)\n",
    "signal = subject_data['signal']\n",
    "predictions = pred_objects_1[subject_info.prediction_part]\n",
    "m_reals = predictions.get_subject_stamps(subject_info.subject_id)\n",
    "center_sample = subject_info.center_sample\n",
    "start_sample = int(center_sample - window_duration * nsrr.fs // 2)\n",
    "end_sample = int(start_sample + window_duration * nsrr.fs)\n",
    "proba = predictions.get_subject_probabilities(\n",
    "    subject_info.subject_id, )\n",
    "proba_up = np.repeat(proba, 8)\n",
    "time_axis = np.arange(start_sample, end_sample) / nsrr.fs\n",
    "n2_pages = predictions.data[subject_info.subject_id]['n2_pages']\n",
    "n2_pages_vector = np.zeros(signal.shape, dtype=np.int32)\n",
    "page_size = int(nsrr.original_page_duration * nsrr.fs)\n",
    "for p in n2_pages:\n",
    "    start_page = p * page_size\n",
    "    end_page = start_page + page_size\n",
    "    n2_pages_vector[start_page:end_page] = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2.5), dpi=140)\n",
    "ax.plot(time_axis, signal[start_sample:end_sample], linewidth=.6)\n",
    "ax.fill_between(\n",
    "    time_axis,\n",
    "    200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    -200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    facecolor=\"k\", alpha=0.1\n",
    ")\n",
    "ax.fill_between(\n",
    "    time_axis, \n",
    "    -300 - 50 * proba_up[start_sample:end_sample], \n",
    "    -300 + 50 * proba_up[start_sample:end_sample],\n",
    "    color=viz.PALETTE['red'], alpha=1.0\n",
    ")\n",
    "ax.axhline(-300 - 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 + 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 - 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 0, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.set_ylim([-400, 200])\n",
    "ax.set_xlim([start_sample/nsrr.fs, end_sample/nsrr.fs])\n",
    "\n",
    "this_reals = utils.filter_stamps(m_reals, start_sample, end_sample)\n",
    "for m in this_reals:\n",
    "    ax.plot(m/nsrr.fs, [-150]*2, linewidth=2, color=viz.PALETTE['red'])\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "ax.tick_params(labelsize=8)\n",
    "title_str = 'Subject %s. Loc %d. Center category %d' % (subject_info.subject_id, loc_to_viz, subject_info.category)\n",
    "ax.set_title(title_str)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
