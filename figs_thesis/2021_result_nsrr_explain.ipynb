{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants, viz, pkeys\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import det_utils\n",
    "from figs_thesis import fig_utils\n",
    "\n",
    "viz.notebook_full_width()\n",
    "\n",
    "param_filtering_fn = fig_utils.get_filtered_signal_for_event\n",
    "param_frequency_fn = fig_utils.get_frequency_by_fft\n",
    "param_amplitude_fn = fig_utils.get_amplitude_event\n",
    "\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')\n",
    "LETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(parts_to_load, dataset, thr=0.5, verbose=True):\n",
    "    if thr == 0.5:\n",
    "        extra_str = ''\n",
    "    else:\n",
    "        extra_str = '_%1.2f' % thr\n",
    "    pred_objects = []\n",
    "    for part in parts_to_load:\n",
    "        filepath = os.path.join(\n",
    "            RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "            'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "            'v2_time',\n",
    "            'prediction%s_part%d.pkl' % (extra_str, part)\n",
    "        )\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            pred_object = pickle.load(handle)\n",
    "        pred_object.set_parent_dataset(dataset)\n",
    "        pred_objects.append(pred_object)\n",
    "    return pred_objects\n",
    "\n",
    "\n",
    "def extract_pages_for_stamps_strict(stamps, pages_indices, page_size):\n",
    "    \"\"\"Returns stamps that are at completely contained on pages.\"\"\"\n",
    "    stamps_start_page = np.floor(stamps[:, 0] / page_size)\n",
    "    stamps_end_page = np.floor(stamps[:, 1] / page_size)\n",
    "    useful_idx = np.where(\n",
    "        np.isin(stamps_start_page, pages_indices) & np.isin(stamps_end_page, pages_indices)\n",
    "    )[0]\n",
    "    pages_data = stamps[useful_idx, :]\n",
    "    return pages_data\n",
    "\n",
    "\n",
    "def get_logit(x):\n",
    "    return np.log(x / (1-x))\n",
    "\n",
    "\n",
    "def get_proba(l):\n",
    "    return 1 / (1 + np.exp(-l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NSRR dataset and pre-computed predicted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_to_load = [0] # 0 to 11\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True, params={pkeys.PAGE_DURATION: 30})\n",
    "pred_objects_1 = load_predictions(parts_to_load, nsrr)\n",
    "pred_objects_0 = load_predictions(parts_to_load, nsrr, thr=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames of dataset checkpoints\n",
    "byevent_proba_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_byevent_proba.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_load_checkpoint = True\n",
    "\n",
    "# ############################\n",
    "\n",
    "if params_load_checkpoint:\n",
    "    print(\"Loading from checkpoint\")\n",
    "    table_byevent_proba = pd.read_csv(byevent_proba_ckpt_path)\n",
    "    # Change 64 bits dtypes to 32 bits to save memory\n",
    "    int_cols = [\"female\", \"center_sample\", \"prediction_part\", \"category\"]\n",
    "    float_cols = [col for col in table_byevent_proba.columns if col not in int_cols+[\"subject_id\"]]\n",
    "    table_byevent_proba[int_cols] = table_byevent_proba[int_cols].astype(np.int32)\n",
    "    table_byevent_proba[float_cols] = table_byevent_proba[float_cols].astype(np.float32)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    # Perform computation and save checkpoint\n",
    "    \n",
    "    bands_for_mean_power = [\n",
    "        (0, 2),\n",
    "        (2, 4),\n",
    "        (4, 8),\n",
    "        (8, 10),\n",
    "        (11, 16),\n",
    "        (16, 30),\n",
    "        (4.5, 30),\n",
    "    ]\n",
    "\n",
    "    table_byevent_proba = {\n",
    "        'subject_id': [],\n",
    "        'age': [],\n",
    "        'female': [],\n",
    "        'center_sample': [],\n",
    "        'prediction_part': [],\n",
    "        'category': [],\n",
    "        'probability': [],\n",
    "        'duration': [], \n",
    "        'frequency': [],\n",
    "        'amplitude_pp': [],\n",
    "        'amplitude_rms': [],\n",
    "        'correlation': [],\n",
    "        'covariance': [],\n",
    "        'c10_density_real': [],\n",
    "        'c10_density_all': [],\n",
    "        'c10_abs_sigma_power': [],\n",
    "        'c10_rel_sigma_power': [],\n",
    "        'c10_abs_sigma_power_masked': [],\n",
    "        'c10_rel_sigma_power_masked': [],\n",
    "        'c20_density_real': [],\n",
    "        'c20_density_all': [],\n",
    "        'c20_abs_sigma_power': [],\n",
    "        'c20_rel_sigma_power': [],\n",
    "        'c20_abs_sigma_power_masked': [],\n",
    "        'c20_rel_sigma_power_masked': [],\n",
    "    }\n",
    "    table_byevent_proba.update(\n",
    "        {'mean_power_%s_%s' % band: [] for band in bands_for_mean_power}\n",
    "    )\n",
    "\n",
    "    min_n2_minutes = 60\n",
    "    verbose_min_minutes = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Generating table of parameters\")\n",
    "    n_parts = len(pred_objects_1)\n",
    "    for part_id in range(n_parts):\n",
    "        predictions_1 = pred_objects_1[part_id]\n",
    "        predictions_0 = pred_objects_0[part_id]\n",
    "        print(\"Processing Part %d / %d\" % (part_id + 1, n_parts))\n",
    "        \n",
    "        n_subjects = len(predictions_1.all_ids)\n",
    "        \n",
    "        for i_subject in tqdm(range(n_subjects)):\n",
    "            subject_id = predictions_1.all_ids[i_subject]\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            n2_minutes = n2_pages.size * nsrr.original_page_duration / 60\n",
    "            if n2_minutes < min_n2_minutes:\n",
    "                if verbose_min_minutes:\n",
    "                    print(\"Skipped by N2 minutes: Subject %s with %d N2 minutes\" % (subject_id, n2_minutes))\n",
    "                continue\n",
    "\n",
    "            marks_1 = predictions_1.get_subject_stamps(subject_id)  # Class 1 spindles (real)\n",
    "            marks_0 = predictions_0.get_subject_stamps(subject_id)  # Class 0 \"spindles\" (false)\n",
    "            # Let only those class 0 without intersecting class 1\n",
    "            # If marks_1.size = 0 then marks_0 is by definition not intersecting\n",
    "            if marks_1.size > 0:\n",
    "                ov_mat = utils.get_overlap_matrix(marks_0, marks_1)\n",
    "                is_intersecting = ov_mat.sum(axis=1)\n",
    "                marks_0 = marks_0[is_intersecting == 0]\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "            \n",
    "            # Now only keep N2 stage marks\n",
    "            n2_pages = predictions_1.data[subject_id]['n2_pages']\n",
    "            page_size = int(nsrr.fs * nsrr.original_page_duration)\n",
    "            if marks_1.size > 0:\n",
    "                marks_1 = extract_pages_for_stamps_strict(marks_1, n2_pages, page_size)\n",
    "            if marks_0.size > 0:\n",
    "                marks_0 = extract_pages_for_stamps_strict(marks_0, n2_pages, page_size)\n",
    "            if (marks_1.size + marks_0.size) == 0:\n",
    "                continue  # There are no marks to work with\n",
    "                \n",
    "            marks = []\n",
    "            marks_class = []\n",
    "            if marks_1.size > 0:\n",
    "                marks.append(marks_1)\n",
    "                marks_class.append([1] * marks_1.shape[0])\n",
    "            if marks_0.size > 0:\n",
    "                marks.append(marks_0)\n",
    "                marks_class.append([0] * marks_0.shape[0])\n",
    "            marks = np.concatenate(marks, axis=0).astype(np.int32)\n",
    "            marks_class = np.concatenate(marks_class).astype(np.int32)\n",
    "            n_marks = marks.shape[0]\n",
    "            \n",
    "            # Extract proba\n",
    "            subject_proba = predictions_1.get_subject_probabilities(subject_id, return_adjusted=False)\n",
    "            marks_proba = det_utils.get_event_probabilities(marks, subject_proba, downsampling_factor=8, proba_prc=75)\n",
    "            marks_proba = marks_proba.astype(np.float32)\n",
    "            # Extract signal\n",
    "            subject_data = nsrr.read_subject_data(subject_id, exclusion_of_pages=False)\n",
    "            signal = subject_data['signal'].astype(np.float64)\n",
    "            age = float(subject_data['age'].item())\n",
    "            female = int(subject_data['sex'].item() == 'f')\n",
    "            \n",
    "            # Parameters\n",
    "            be_duration = (marks[:, 1] - marks[:, 0] + 1) / nsrr.fs\n",
    "\n",
    "            filt_signal = param_filtering_fn(signal, nsrr.fs, constants.SPINDLE).astype(np.float64)\n",
    "            signal_events = [filt_signal[e[0]:(e[1]+1)] for e in marks]\n",
    "\n",
    "            be_amplitude_pp = np.array([\n",
    "                param_amplitude_fn(s, nsrr.fs, constants.SPINDLE) for s in signal_events\n",
    "            ])\n",
    "            \n",
    "            be_amplitude_rms = np.array([\n",
    "                np.sqrt(np.mean(s ** 2)) for s in signal_events\n",
    "            ])\n",
    "\n",
    "            be_frequency = np.array([\n",
    "                param_frequency_fn(s, nsrr.fs) for s in signal_events\n",
    "            ])\n",
    "            \n",
    "            # New parameters\n",
    "            signal_raw_events = [signal[e[0]:(e[1]+1)] for e in marks]\n",
    "            \n",
    "            # Measure mean power\n",
    "            for band in bands_for_mean_power:\n",
    "                table_byevent_proba['mean_power_%s_%s' % band].append([])\n",
    "            for s in signal_raw_events:\n",
    "                freq, power = fig_utils.get_fft_spectrum(s, nsrr.fs, pad_to_duration=10, f_min=0, f_max=30, apply_hann_window=False)\n",
    "                for band in bands_for_mean_power:\n",
    "                    power_in_band = power[(freq >= band[0]) & (freq <= band[1])].mean()\n",
    "                    table_byevent_proba['mean_power_%s_%s' % band][-1].append(power_in_band)\n",
    "            for band in bands_for_mean_power:\n",
    "                table_byevent_proba['mean_power_%s_%s' % band][-1] = np.array(table_byevent_proba['mean_power_%s_%s' % band][-1], dtype=np.float32)\n",
    "            \n",
    "            # Covariance and correlation between sigma band and broad band\n",
    "            cov_l = []\n",
    "            corr_l = []\n",
    "            for s, filt_s in zip(signal_raw_events, signal_events):\n",
    "                s = s - s.mean()\n",
    "                filt_s = filt_s - filt_s.mean()\n",
    "                # covariance\n",
    "                cov = np.mean(s * filt_s)\n",
    "                cov_l.append(cov)\n",
    "                # correlation\n",
    "                corr = np.corrcoef(s, filt_s)[0, 1]\n",
    "                corr_l.append(corr)\n",
    "                \n",
    "            cov_l = np.array(cov_l, dtype=np.float32)\n",
    "            corr_l = np.array(corr_l, dtype=np.float32)\n",
    "            \n",
    "            # Local stuff\n",
    "            context_params = {\n",
    "                'c10_density_real': [],\n",
    "                'c10_density_all': [],\n",
    "                'c10_abs_sigma_power': [],\n",
    "                'c10_rel_sigma_power': [],\n",
    "                'c10_abs_sigma_power_masked': [],\n",
    "                'c10_rel_sigma_power_masked': [],\n",
    "                'c20_density_real': [],\n",
    "                'c20_density_all': [],\n",
    "                'c20_abs_sigma_power': [],\n",
    "                'c20_rel_sigma_power': [],\n",
    "                'c20_abs_sigma_power_masked': [],\n",
    "                'c20_rel_sigma_power_masked': [],\n",
    "            }\n",
    "            window_durations = [10, 20]\n",
    "            for i_mark, mark in enumerate(marks):\n",
    "                central_sample = mark.mean()\n",
    "                for window_duration in window_durations:\n",
    "                    window_size = int(window_duration * nsrr.fs)\n",
    "                    start_sample = int(central_sample - window_size // 2)\n",
    "                    end_sample = start_sample + window_size\n",
    "                    # Local number of marks, by category\n",
    "                    local_nmarks_real = utils.filter_stamps(marks_1, start_sample, end_sample).shape[0]\n",
    "                    local_nmarks_both = utils.filter_stamps(marks, start_sample, end_sample).shape[0]\n",
    "                    # Local sigma activity\n",
    "                    segment_signal = signal[start_sample:end_sample]\n",
    "                    \n",
    "                    # including event\n",
    "                    freq, power = utils.power_spectrum_by_sliding_window(segment_signal, nsrr.fs, window_duration=5)\n",
    "                    # a) Absolute sigma power \n",
    "                    local_abs_sigma_power = power[(freq >= 11) & (freq <= 16)].mean()\n",
    "                    # b) Relative sigma power (as in Lacourse)\n",
    "                    local_broad_power = power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "                    local_rel_sigma_power = local_abs_sigma_power / local_broad_power\n",
    "                    \n",
    "                    # masking event\n",
    "                    local_start = mark[0] - start_sample\n",
    "                    local_end = mark[1] - start_sample\n",
    "                    segment_signal_masked = segment_signal.copy()\n",
    "                    segment_signal_masked[local_start:local_end] = 0\n",
    "                    freq, power = utils.power_spectrum_by_sliding_window(segment_signal_masked, nsrr.fs, window_duration=5)\n",
    "                    # a) Absolute sigma power \n",
    "                    local_mask_abs_sigma_power = power[(freq >= 11) & (freq <= 16)].mean()\n",
    "                    # b) Relative sigma power (as in Lacourse)\n",
    "                    local_mask_broad_power = power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "                    local_mask_rel_sigma_power = local_mask_abs_sigma_power / local_mask_broad_power\n",
    "                    \n",
    "                    # Save\n",
    "                    context_params['c%d_density_real' % window_duration].append(local_nmarks_real)\n",
    "                    context_params['c%d_density_all' % window_duration].append(local_nmarks_both)\n",
    "                    context_params['c%d_abs_sigma_power' % window_duration].append(local_abs_sigma_power)\n",
    "                    context_params['c%d_rel_sigma_power' % window_duration].append(local_rel_sigma_power)\n",
    "                    context_params['c%d_abs_sigma_power_masked' % window_duration].append(local_mask_abs_sigma_power)\n",
    "                    context_params['c%d_rel_sigma_power_masked' % window_duration].append(local_mask_rel_sigma_power)\n",
    "            \n",
    "            for key in context_params.keys():\n",
    "                context_params[key] = np.array(context_params[key], dtype=np.float32)\n",
    "    \n",
    "            # New parameters\n",
    "            table_byevent_proba['subject_id'].append([subject_id] * n_marks)\n",
    "            table_byevent_proba['age'].append(np.array([age] * n_marks, dtype=np.float32))\n",
    "            table_byevent_proba['female'].append(np.array([female] * n_marks, dtype=np.int32))\n",
    "            table_byevent_proba['center_sample'].append(marks.mean(axis=1).astype(np.int32))\n",
    "            table_byevent_proba['prediction_part'].append(np.array([part_id] * n_marks, dtype=np.int32))\n",
    "            table_byevent_proba['category'].append(marks_class)\n",
    "            table_byevent_proba['probability'].append(marks_proba.astype(np.float32))\n",
    "            table_byevent_proba['duration'].append(be_duration.astype(np.float32))\n",
    "            table_byevent_proba['frequency'].append(be_frequency.astype(np.float32))\n",
    "            table_byevent_proba['amplitude_pp'].append(be_amplitude_pp.astype(np.float32))\n",
    "            table_byevent_proba['amplitude_rms'].append(be_amplitude_rms.astype(np.float32))\n",
    "            table_byevent_proba['covariance'].append(cov_l)\n",
    "            table_byevent_proba['correlation'].append(corr_l)\n",
    "            for key in context_params.keys():\n",
    "                table_byevent_proba[key].append(context_params[key])\n",
    "            \n",
    "    for key in table_byevent_proba:\n",
    "        table_byevent_proba[key] = np.concatenate(table_byevent_proba[key])\n",
    "    table_byevent_proba = pd.DataFrame.from_dict(table_byevent_proba)\n",
    "    \n",
    "    # compute relative powers\n",
    "    powers = table_byevent_proba[[col for col in table_byevent_proba.columns if 'mean_power' in col and \"11_16\" not in col]]\n",
    "    powers = powers.div(table_byevent_proba[\"mean_power_11_16\"], axis=0)\n",
    "    powers = 1.0 / powers\n",
    "    powers = powers.add_prefix(\"mean_power_11_16_relto_\")\n",
    "    table_byevent_proba = table_byevent_proba.merge(powers, left_index=True, right_index=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    print(\"Saving checkpoint\")\n",
    "    table_byevent_proba.to_csv(byevent_proba_ckpt_path, index=False)\n",
    "    print(\"Done.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba.age.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when plotting I could take the logarithm of either the abs power o the relative power\n",
    "# specially the relative power to get rid of the decision of showing a/b or b/a, since in the logarithm is just a change in sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table_byevent_proba.probability, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_names = [n for n in table_byevent_proba.columns if n not in ['subject_id', 'center_sample', 'prediction_part', 'category', 'probability']]\n",
    "n_params = len(param_names)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_params / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 3 * n_rows), dpi=120)\n",
    "axes = axes.flatten()\n",
    "\n",
    "use_logits = True\n",
    "\n",
    "for ax, param_name in zip(axes, param_names):\n",
    "    p = table_byevent_proba.probability\n",
    "    if use_logits:\n",
    "        p = np.clip(p, a_min=1e-3, a_max=1 - 1e-3)\n",
    "        p = get_logit(p)\n",
    "        ax.axvline(get_logit(0.25), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.50), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.75), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.90), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.95), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "        ax.axvline(get_logit(0.99), linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "     \n",
    "    ax.hist2d(p, table_byevent_proba[param_name], bins=50, cmap=\"binary\")\n",
    "    #ax.plot(\n",
    "    #    p,\n",
    "    #    table_byevent_proba[param_name],\n",
    "     #   marker='o', markeredgewidth=0, markersize=4, alpha=0.1, linestyle=\"none\",\n",
    "     #   color=viz.PALETTE['blue'],\n",
    "    #)\n",
    "    ax.set_title(param_name, fontsize=8)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba[\n",
    "    (table_byevent_proba.category == 1) & (table_byevent_proba.probability < 0.5)\n",
    "].sort_values(by=\"probability\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización genérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_to_viz = 2\n",
    "window_duration = 20\n",
    "\n",
    "# -------------------\n",
    "subject_info = table_byevent_proba.loc[loc_to_viz]\n",
    "print(subject_info)\n",
    "subject_data = nsrr.read_subject_data(subject_info.subject_id, exclusion_of_pages=False)\n",
    "signal = subject_data['signal']\n",
    "predictions = pred_objects_1[subject_info.prediction_part]\n",
    "m_reals = predictions.get_subject_stamps(subject_info.subject_id)\n",
    "center_sample = subject_info.center_sample\n",
    "start_sample = int(center_sample - window_duration * nsrr.fs // 2)\n",
    "end_sample = int(start_sample + window_duration * nsrr.fs)\n",
    "proba = predictions.get_subject_probabilities(\n",
    "    subject_info.subject_id, )\n",
    "proba_up = np.repeat(proba, 8)\n",
    "time_axis = np.arange(start_sample, end_sample) / nsrr.fs\n",
    "n2_pages = predictions.data[subject_info.subject_id]['n2_pages']\n",
    "n2_pages_vector = np.zeros(signal.shape, dtype=np.int32)\n",
    "page_size = int(nsrr.original_page_duration * nsrr.fs)\n",
    "for p in n2_pages:\n",
    "    start_page = p * page_size\n",
    "    end_page = start_page + page_size\n",
    "    n2_pages_vector[start_page:end_page] = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2.5), dpi=140)\n",
    "ax.plot(time_axis, signal[start_sample:end_sample], linewidth=.6)\n",
    "ax.fill_between(\n",
    "    time_axis,\n",
    "    200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    -200 * (1 - n2_pages_vector[start_sample:end_sample]),\n",
    "    facecolor=\"k\", alpha=0.1\n",
    ")\n",
    "ax.fill_between(\n",
    "    time_axis, \n",
    "    -300 - 50 * proba_up[start_sample:end_sample], \n",
    "    -300 + 50 * proba_up[start_sample:end_sample],\n",
    "    color=viz.PALETTE['red'], alpha=1.0\n",
    ")\n",
    "ax.axhline(-300 - 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 + 50, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.axhline(-300 - 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 25, linewidth=0.7, linestyle=\"--\", color=\"k\")\n",
    "ax.axhline(-300 + 0, linewidth=0.7, linestyle=\"-\", color=\"k\")\n",
    "ax.set_ylim([-400, 200])\n",
    "ax.set_xlim([start_sample/nsrr.fs, end_sample/nsrr.fs])\n",
    "\n",
    "this_reals = utils.filter_stamps(m_reals, start_sample, end_sample)\n",
    "for m in this_reals:\n",
    "    ax.plot(m/nsrr.fs, [-150]*2, linewidth=2, color=viz.PALETTE['red'])\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "ax.tick_params(labelsize=8)\n",
    "title_str = 'Subject %s. Loc %d. Center category %d' % (subject_info.subject_id, loc_to_viz, subject_info.category)\n",
    "ax.set_title(title_str)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conteos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full table:\", table_byevent_proba.shape)\n",
    "print(\"Category 1:\", np.sum(table_byevent_proba.category == 1))\n",
    "print(\"Category 0:\", np.sum(table_byevent_proba.category == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.1\n",
    "\n",
    "class0_data = table_byevent_proba[\n",
    "    (table_byevent_proba.category == 0) & (table_byevent_proba.probability <= (0.5 - margin))\n",
    "]\n",
    "print(\"Class 0 data\", class0_data.shape, \"Probability range\", class0_data.probability.min(), class0_data.probability.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_proba_class0 = class0_data.probability.min()\n",
    "class1_data = table_byevent_proba[\n",
    "    (table_byevent_proba.category == 1) & (table_byevent_proba.probability >= (0.5 + margin)) & (table_byevent_proba.probability <= (1.0 - min_proba_class0))\n",
    "]\n",
    "print(\"Class 1 data\", class1_data.shape, \"Probability range\", class1_data.probability.min(), class1_data.probability.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadísticas de la probabilidad del evento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = table_byevent_proba[table_byevent_proba.category == 1][[\"probability\", \"age\", \"female\"]]\n",
    "real_data[\"age\"] = np.clip(real_data[\"age\"], a_min=5, a_max=None)\n",
    "real_data['age_bin'] = pd.cut(real_data.age, bins=np.arange(5, 90 + 0.001, 5).astype(np.int32))\n",
    "real_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.probability.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 2.5), dpi=200)\n",
    "ax = axes[0]\n",
    "ax.hist(\n",
    "    real_data.probability, \n",
    "    bins=np.arange(0.4, 1.0 + 0.001, 0.01),\n",
    "    color=viz.PALETTE['blue'],\n",
    "    label=None\n",
    ")\n",
    "\n",
    "ax = axes[1]\n",
    "real_data.boxplot(\n",
    "    column=[\"probability\"],\n",
    "    by=\"age_bin\", \n",
    "    ax=ax, fontsize=8, rot=90,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    boxprops=dict(color=viz.PALETTE['blue']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['blue']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    ")\n",
    "\n",
    "fig.suptitle('')\n",
    "for i_ax, ax in enumerate(axes.flatten()):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.text(\n",
    "        x=-0.06, y=1.05, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlabel(\"Probabilidad del evento\", fontsize=8)\n",
    "ax.set_yticks([])\n",
    "ax.set_ylabel(\"Número de husos\", fontsize=8)\n",
    "ax.set_xlim([0.4, 1.0])\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_xticks(np.arange(0.4, 1.0 + 0.001, 0.1))\n",
    "mean_proba = np.mean(real_data.probability)\n",
    "median_proba = np.median(real_data.probability)\n",
    "ax.axvline(mean_proba, linestyle=\":\", color=\"k\", linewidth=1.2, label=\"Promedio %1.2f\" % mean_proba)\n",
    "ax.axvline(median_proba, linestyle=\"--\", color=\"k\", linewidth=1.2, label=\"Mediana %1.2f\" % median_proba)\n",
    "ax.legend(loc=\"upper left\", fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(\"Probabilidad del evento\", fontsize=8, loc=\"left\")\n",
    "ax.set_xlabel(\"Edad (años)\", fontsize=8)\n",
    "ax.set_ylim([0.4, 1.0])\n",
    "ax.set_yticks(np.arange(0.4, 1.0 + 0.001, 0.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_explain_proba_stat\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de prototipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = table_byevent_proba[table_byevent_proba.category == 1].sort_values(by=\"probability\")\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = 5\n",
    "\n",
    "best_data = real_data[-n_cases:]\n",
    "worst_data = real_data[:n_cases]\n",
    "\n",
    "parts_to_load = np.concatenate([best_data.prediction_part, worst_data.prediction_part])\n",
    "parts_to_load = np.unique(parts_to_load)\n",
    "\n",
    "nsrr = load_dataset(constants.NSRR_SS_NAME, load_checkpoint=True, params={pkeys.PAGE_DURATION: 30})\n",
    "pred_objects = load_predictions(parts_to_load, nsrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "window_duration = 20  # s\n",
    "ylim = 75  # uV\n",
    "# tratar que 1s aprox 50 uV\n",
    "\n",
    "# ----\n",
    "window_size = int(window_duration * nsrr.fs)\n",
    "time_axis = np.arange(0, window_size) / nsrr.fs\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 9), dpi=200, sharex=True)\n",
    "\n",
    "for i_ax, df in enumerate([best_data, worst_data]):\n",
    "    ax = axes[i_ax]\n",
    "    offset = 0\n",
    "    for index, row in df.iterrows():\n",
    "        part_loc = np.where(parts_to_load == row.prediction_part)[0][0]\n",
    "        predictions = pred_objects[part_loc]\n",
    "        subject_data = nsrr.read_subject_data(row.subject_id, exclusion_of_pages=False)\n",
    "        signal = subject_data['signal']\n",
    "        m_reals = predictions.get_subject_stamps(row.subject_id)\n",
    "        proba = predictions.get_subject_probabilities(row.subject_id)\n",
    "        proba_up = np.repeat(proba, 8)\n",
    "\n",
    "        start_sample = int(row.center_sample - window_size // 2)\n",
    "        end_sample = int(start_sample + window_size)\n",
    "\n",
    "        ax.plot(\n",
    "            time_axis,\n",
    "            offset + signal[start_sample:end_sample],\n",
    "            linewidth=0.7,\n",
    "            color=viz.GREY_COLORS[6]\n",
    "        )\n",
    "\n",
    "        this_marks = utils.filter_stamps(m_reals, start_sample, end_sample)\n",
    "        for e in this_marks:\n",
    "            e_proba = np.percentile(proba_up[e[0]:e[1]+1], 75)\n",
    "            e = np.clip(e, a_min=start_sample, a_max=end_sample)\n",
    "            e = e - start_sample  # relative\n",
    "            ax.fill_between(e / nsrr.fs, offset - ylim, offset + ylim, facecolor=viz.PALETTE['red'], alpha=0.25)\n",
    "            p_x_origin = e.mean() / nsrr.fs\n",
    "            p_y_origin = offset + 0.95 * ylim\n",
    "            ax.annotate(\"%02d\" % np.round(100 * e_proba), (p_x_origin, p_y_origin), ha=\"center\", va=\"top\", fontsize=7)\n",
    "\n",
    "        offset -= 2 * ylim\n",
    "\n",
    "for i_ax, ax in enumerate(axes):\n",
    "    ymax = ylim\n",
    "    ymin = - (n_cases - 0.5) * 2 * ylim\n",
    "    \n",
    "    # Add scale\n",
    "    x_length = 1\n",
    "    y_length = 50\n",
    "    y_origin = ymin + ylim - y_length\n",
    "    x_origin = 0.5\n",
    "    ax.fill_between(\n",
    "        [x_origin - 0.1 * x_length, x_origin + 1.1 * x_length],\n",
    "        y_origin - 0.1 * y_length, \n",
    "        y_origin + 1.1 * y_length,\n",
    "        facecolor=\"w\",\n",
    "        zorder=30,\n",
    "        alpha=0.75\n",
    "    )\n",
    "    ax.plot([x_origin, x_origin + x_length], [y_origin] * 2, linewidth=2, color=\"k\", zorder=31)\n",
    "    ax.plot([x_origin] * 2, [y_origin, y_origin + y_length], linewidth=2, color=\"k\", zorder=31)\n",
    "    ax.annotate(\"%d s\" % x_length, (x_origin + x_length, y_origin + 0.1 * y_length), ha=\"right\", va=\"bottom\", fontsize=8, zorder=31)\n",
    "    ax.annotate(\"%d $\\mu$V\" % y_length, (x_origin + 0.1 * x_length, y_origin + 1.1 * y_length), ha=\"left\", va=\"top\", fontsize=8, zorder=31)\n",
    "    \n",
    "    # Other stuff\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticks(np.arange(0, window_duration + 0.001, 0.5), minor=True)\n",
    "    ax.set_xlim([0, window_duration])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(axis=\"x\", which=\"minor\")\n",
    "    \n",
    "    ax.text(\n",
    "        x=0, y=1.01, fontsize=14, s=r\"$\\bf{%s}$\" % LETTERS[i_ax],\n",
    "        ha=\"left\", transform=ax.transAxes)\n",
    "\n",
    "axes[0].set_title(\"Mejores detecciones\", fontsize=8, loc=\"center\")\n",
    "axes[1].set_title(\"Peores detecciones\", fontsize=8, loc=\"center\")\n",
    "# axes[1].set_xlabel(\"Intervalos de 0.5 s\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figure:\n",
    "    # Save figure\n",
    "    fname_prefix = \"result_nsrr_explain_best_worst\"\n",
    "    plt.savefig(\"%s.pdf\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.png\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "    plt.savefig(\"%s.svg\" % fname_prefix, bbox_inches=\"tight\", pad_inches=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlaciones con la probabilidad (o logit / puntaje no normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_events = np.clip(table_byevent_proba.probability.values.astype(np.float64), a_min=1e-6, a_max=1 - 1e-6)\n",
    "logit_events = get_logit(proba_events)\n",
    "table_byevent_proba[\"logit\"] = logit_events.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min Max\", table_byevent_proba.logit.min(), table_byevent_proba.logit.max())\n",
    "logit_bins = np.arange(-1.5, 4.5 + 0.001, 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2.5), dpi=100)\n",
    "ax.hist(\n",
    "    table_byevent_proba.logit, \n",
    "    bins=logit_bins,\n",
    "    color=viz.PALETTE['blue'],\n",
    "    label=None\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_byevent_proba['logit_bin'] = pd.cut(table_byevent_proba.logit, bins=logit_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms densities so that it does not count the central event, only context (only neighbours)\n",
    "table_byevent_proba['c10_neighbours'] = table_byevent_proba['c10_density_real']\n",
    "table_byevent_proba.loc[table_byevent_proba.category == 1, 'c10_neighbours'] -= 1\n",
    "\n",
    "table_byevent_proba['c20_neighbours'] = table_byevent_proba['c20_density_real']\n",
    "table_byevent_proba.loc[table_byevent_proba.category == 1, 'c20_neighbours'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfeat_columns = ['subject_id', 'center_sample', 'prediction_part', 'category', 'probability', 'logit', 'logit_bin', 'age', 'female']\n",
    "exclude_feat_columns = [\n",
    "    'amplitude_rms',\n",
    "    'c10_abs_sigma_power', 'c10_abs_sigma_power_masked',\n",
    "    'c20_abs_sigma_power', 'c20_abs_sigma_power_masked',\n",
    "    'c10_rel_sigma_power', 'c20_rel_sigma_power',\n",
    "    'c10_density_all', 'c20_density_all',\n",
    "    'c10_density_real', 'c20_density_real',\n",
    "    'mean_power_0_2',\n",
    "    'mean_power_2_4',\n",
    "    'mean_power_4_8',\n",
    "    'mean_power_8_10',\n",
    "    'mean_power_16_30',\n",
    "    'mean_power_4.5_30',\n",
    "    'c10_rel_sigma_power_masked', 'c20_rel_sigma_power_masked',\n",
    "]\n",
    "feat_columns = [col for col in table_byevent_proba.columns if col not in nonfeat_columns+exclude_feat_columns]\n",
    "feat_columns.append('c10_rel_sigma_power_masked')\n",
    "feat_columns.append('c20_rel_sigma_power_masked')\n",
    "feat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_params = len(feat_columns)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_params / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 2 * n_rows), dpi=140, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "table_byevent_proba.boxplot(\n",
    "    column=feat_columns,\n",
    "    by=\"logit_bin\", \n",
    "    ax=axes[:n_params], fontsize=8, rot=90,\n",
    "    flierprops=dict(markersize=3, marker='o', markerfacecolor=\"k\", alpha=0.4, markeredgewidth=0),\n",
    "    boxprops=dict(color=viz.PALETTE['blue']),\n",
    "    whiskerprops=dict(color=viz.PALETTE['blue']),\n",
    "    medianprops=dict(color=\"k\"),\n",
    "    showfliers=False,\n",
    "    showmeans=True,\n",
    "    meanprops=dict(color=\"k\", marker=\"o\", markersize=3),\n",
    "    widths=0.75,\n",
    ")\n",
    "fig.suptitle('')\n",
    "for i_ax, ax in enumerate(axes[:n_params]):\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(feat_columns[i_ax][:25], fontsize=6)\n",
    "    ax.axvline(3.5, linewidth=0.8, color=\"k\", linestyle=\"--\")\n",
    "    ax.set_xlim(0, len(logit_bins))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efecto del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noncontext_feats = [col for col in feat_columns if 'c10_' not in col and 'c20_' not in col]\n",
    "window_duration = 20\n",
    "fs = 200\n",
    "window_size = int(window_duration * fs)\n",
    "\n",
    "load_context_from_checkpoint = True\n",
    "\n",
    "byevent_context_ckpt_path = os.path.join(\n",
    "    RESULTS_PATH, 'predictions_nsrr_ss',\n",
    "    'ckpt_20210716_from_20210529_thesis_indata_5cv_e1_n2_train_moda_ss_ensemble_to_e1_n2_train_nsrr_ss',\n",
    "    'v2_time',\n",
    "    'table_byevent_context.csv'\n",
    ")\n",
    "if load_context_from_checkpoint:\n",
    "    print(\"Loading from checkpoint\")\n",
    "    table_context = pd.read_csv(byevent_context_ckpt_path)\n",
    "    # Change 64 bits dtypes to 32 bits to save memory\n",
    "    int_cols = [\"center_sample\"]\n",
    "    float_cols = [col for col in table_context.columns if col not in int_cols+[\"subject_id\"]]\n",
    "    table_context[int_cols] = table_context[int_cols].astype(np.int32)\n",
    "    table_context[float_cols] = table_context[float_cols].astype(np.float32)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    # solo contar como vecino si el centro esta dentro del contexto\n",
    "    # solo considerar vecinos categoria 1\n",
    "    subject_ids = np.unique(table_byevent_proba.subject_id)\n",
    "    table_context = {'subject_id': [], 'center_sample': []}\n",
    "    table_context.update({\n",
    "        'c%davg_%s' % (window_duration, key): [] for key in noncontext_feats\n",
    "    })\n",
    "    for subject_id in subject_ids:\n",
    "        subset = table_byevent_proba.loc[ \n",
    "            table_byevent_proba.subject_id == subject_id,  \n",
    "            ['center_sample', 'category'] + noncontext_feats\n",
    "        ]\n",
    "        event_centers = subset.center_sample.values\n",
    "        subset_real = subset[subset.category==1].drop(columns=\"category\").set_index(\"center_sample\")\n",
    "\n",
    "        for event_center in event_centers:\n",
    "            start_window = int(event_center - window_size // 2)\n",
    "            end_window = int(start_window + window_size)\n",
    "            neighbours = subset_real.loc[start_window:end_window]\n",
    "            if event_center in neighbours.index:\n",
    "                neighbours = neighbours.drop(event_center)\n",
    "            # now we only have real neighbours, without considering event of interest\n",
    "            if len(neighbours) > 0:\n",
    "                mean_context = neighbours.mean()\n",
    "            else:\n",
    "                mean_context = {key: np.nan for key in noncontext_feats}\n",
    "            # now save\n",
    "            table_context['subject_id'].append(subject_id)\n",
    "            table_context['center_sample'].append(event_center)\n",
    "            for key in noncontext_feats:\n",
    "                table_context['c%davg_%s' % (window_duration, key)].append(mean_context[key])\n",
    "    table_context = pd.DataFrame.from_dict(table_context)\n",
    "    \n",
    "    print(\"Saving checkpoint\")\n",
    "    table_context.to_csv(byevent_context_ckpt_path, index=False)\n",
    "    print(\"Done.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join tables\n",
    "table_context = table_byevent_proba.merge(table_context, on=[\"subject_id\", \"center_sample\"])\n",
    "table_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_context.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
