{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import gridspec\n",
    "import pprint\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "detector_path = '../..'\n",
    "results_path = os.path.join(detector_path, 'results')\n",
    "sys.path.append(detector_path)\n",
    "\n",
    "from sleep.utils import constants\n",
    "from sleep.utils import checks\n",
    "from sleep.data.mass_ss import MassSS\n",
    "# from sleep.inta import INTA\n",
    "from sleep.data import postprocessing\n",
    "from sleep.data import data_ops\n",
    "from sleep.data import metrics\n",
    "\n",
    "DPI = 150\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mass_ss with 15 patients.\n",
      "Train size: 11. Test size: 4\n",
      "Train subjects: \n",
      " [1, 3, 5, 7, 9, 10, 11, 14, 17, 18, 19]\n",
      "Test subjects: \n",
      " [2, 6, 12, 13]\n",
      "Loading from checkpoint... Loaded\n",
      "Loading train set and splitting train/val\n",
      "All Training set IDs: [1, 3, 5, 7, 9, 10, 11, 14, 17, 18, 19]\n",
      "Loading test set\n",
      "Testing set IDs: [2, 6, 12, 13]\n",
      "Loading signals and marks\n",
      "Loading pages\n",
      "Loading predictions from /home/ntapia/Projects/GitNico/sleep-rnn/results/predictions_mass_ss/20190408_bsf_train_mass/bsf/avg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ntapia/Projects/GitNico/sleep-rnn/results/predictions_mass_ss/20190408_bsf_train_mass/bsf/avg/y_pred_alltrain.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c7e8c9088788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_pred_%s.npy'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Keep only class 1 probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthis_y_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis_y_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ntapia/Projects/GitNico/sleep-rnn/results/predictions_mass_ss/20190408_bsf_train_mass/bsf/avg/y_pred_alltrain.npy'"
     ]
    }
   ],
   "source": [
    "# Best ones:\n",
    "# MASS: ckpt_folder = os.path.join('20190408_bsf_train_mass', 'bsf', 'avg'), thr=0.45\n",
    "# INTA: ckpt_folder = os.path.join('20190407_bsf_train_inta', 'bsf', 'avg'), thr=0.475\n",
    "\n",
    "# Select database\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "# Select predictions ckpt folder\n",
    "ckpt_folder = os.path.join('20190408_bsf_train_mass', 'bsf', 'avg')\n",
    "thr = 0.45\n",
    "verbose = False\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Marks are binary sequences for each page, 200 fs resolution\n",
    "# Load data\n",
    "checks.check_valid_value(\n",
    "    dataset_name, 'dataset_name',\n",
    "    [constants.MASS_SS_NAME, constants.INTA_SS_NAME, constants.MASS_KC_NAME])\n",
    "if dataset_name == constants.MASS_SS_NAME:\n",
    "    dataset = MassSS(load_checkpoint=True)\n",
    "elif dataset_name == constants.INTA_SS_NAME:\n",
    "    dataset = IntaSS(load_checkpoint=True)\n",
    "else:\n",
    "    dataset = MassKC(load_checkpoint=True)\n",
    "\n",
    "# Get training set ids\n",
    "print('Loading train set and splitting train/val')\n",
    "all_train_ids = dataset.train_ids\n",
    "print('All Training set IDs:', all_train_ids)\n",
    "# Get test data\n",
    "print('Loading test set')\n",
    "test_ids = dataset.test_ids\n",
    "print('Testing set IDs:', test_ids)\n",
    "idx_dict = {'alltrain': all_train_ids, 'test': test_ids}\n",
    "\n",
    "# Get subjects data, with the expert used for training\n",
    "print('Loading signals and marks')\n",
    "# set_list = ['train', 'val', 'test']\n",
    "set_list = ['alltrain', 'test']\n",
    "x = {}\n",
    "y = {}\n",
    "pages = {}\n",
    "x['alltrain'], y['alltrain'] = dataset.get_subset_data(all_train_ids, which_expert=1, verbose=verbose)\n",
    "x['test'], y['test'] = dataset.get_subset_data(test_ids, which_expert=1, verbose=verbose)\n",
    "print('Loading pages')\n",
    "pages['alltrain'] = dataset.get_subset_pages(all_train_ids, verbose=verbose)\n",
    "pages['test'] = dataset.get_subset_pages(test_ids, verbose=verbose)\n",
    "\n",
    "# Load predictions (probability vectors for each page), 200/factor resolution (default factor 8)\n",
    "ckpt_path = os.path.abspath(os.path.join(results_path, 'predictions_%s' % dataset_name, ckpt_folder))\n",
    "print('Loading predictions from %s' % ckpt_path)\n",
    "y_pred = {}\n",
    "for set_name in set_list:\n",
    "    y_pred[set_name] = np.load(os.path.join(ckpt_path, 'y_pred_%s.npy' % set_name), allow_pickle=True)\n",
    "    # Keep only class 1 probability\n",
    "    y_pred[set_name] = [this_y_pred[..., 1] for this_y_pred in y_pred[set_name]]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ID 13, 818 N2 pages, Expert 2\n"
     ]
    }
   ],
   "source": [
    "subset_name = 'test'\n",
    "subject_id = 13\n",
    "\n",
    "# -----\n",
    "idx_subject = idx_dict[subset_name].index(subject_id)\n",
    "this_signal = x[subset_name][idx_subject]\n",
    "this_marks_1 = y[subset_name][idx_subject]\n",
    "this_n2_pages = pages[subset_name][idx_subject]\n",
    "this_prob = y_pred[subset_name][idx_subject]\n",
    "\n",
    "y_pred_thr = postprocessing.generate_mark_intervals(\n",
    "    y_pred[subset_name][idx_subject], pages[subset_name][idx_subject], 200//8, 200, thr=thr)\n",
    "y_pred_thr_seq = data_ops.inter2seq(y_pred_thr, 0, (pages[subset_name][idx_subject].max()+1)*200*20-1)\n",
    "# Now reshape into pages\n",
    "this_det = data_ops.extract_pages(y_pred_thr_seq, pages[subset_name][idx_subject], 200*20, border_size=0)\n",
    "\n",
    "if dataset_name == constants.MASS_NAME:\n",
    "    _, this_marks_2 = dataset.get_subject_data(subject_id, which_expert=2, verbose=True)\n",
    "    channel_name = 'C3-CLE'\n",
    "else:\n",
    "    channel_name = 'F4-C4'\n",
    "    \n",
    "# make a color map of fixed colors\n",
    "cmap = colors.ListedColormap(['white', '#c62828'])\n",
    "\n",
    "def plot_page(page_idx):\n",
    "    if dataset_name == constants.MASS_NAME:\n",
    "        fig = plt.figure(figsize=(15, 4), dpi=DPI)\n",
    "        gs = gridspec.GridSpec(3, 1, height_ratios=[4, 1, 1])\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(15, 3), dpi=DPI)\n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1])\n",
    "    \n",
    "    page_idx = page_idx - 1\n",
    "    \n",
    "    segment_signal = this_signal[page_idx, :]\n",
    "    segment_marks_1 = this_marks_1[page_idx, :]\n",
    "    segment_prob = this_prob[page_idx, :]\n",
    "    segment_det = this_det[page_idx, :]\n",
    "    time_axis = np.arange(this_signal.shape[1]) / dataset.fs\n",
    "    \n",
    "    if dataset_name == constants.MASS_NAME:\n",
    "        segment_marks_2 = this_marks_2[page_idx, :]\n",
    "    \n",
    "    gs_idx = 0\n",
    "    \n",
    "    # Signal\n",
    "    y_max = 10\n",
    "    ax = fig.add_subplot(gs[gs_idx])\n",
    "    gs_idx = gs_idx + 1\n",
    "    ax.plot(time_axis, segment_signal, linewidth=1, color='#455a64', label='EEG %s' % channel_name)\n",
    "    ax.fill_between(time_axis, y_max * segment_marks_1, -y_max * segment_marks_1, facecolor='#c62828', alpha=0.3, label='Sleep Spindle')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([0, 20])\n",
    "    ax.set_ylim([-y_max, y_max])\n",
    "    ax.set_title('Subject %d (%s-%s). Page in record: %d (intervals of 0.5s are shown).' \n",
    "                 % (subject_id, dataset_name.upper(), subset_name.capitalize(), this_n2_pages[page_idx]), fontsize=10)\n",
    "    ax.set_xticks([0, 5, 10, 15, 20])\n",
    "    ax.set_xticks(np.arange(0, 20, 0.5), minor=True)\n",
    "    ax.grid(b=True, axis='x', which='minor')\n",
    "    lg = ax.legend(loc='upper right', fontsize=8.5)\n",
    "    # lh = lg.legendHandles[1]\n",
    "    # lh.set_facecolor(lh.get_facecolor())\n",
    "    # lh.set_alpha(1.0)\n",
    "    ax.tick_params(labelsize=8.5)\n",
    "    \n",
    "    # Neural net\n",
    "    delta_y = 0.1\n",
    "    time_axis_short = time_axis[::8]\n",
    "    ax = fig.add_subplot(gs[gs_idx])\n",
    "    gs_idx = gs_idx + 1\n",
    "    ax.plot(time_axis_short, segment_prob, color='#c62828', linewidth=1.5, zorder=7)\n",
    "    ax.fill_between(time_axis, (1+delta_y)*segment_det, (-delta_y)*segment_det, facecolor='grey', zorder=6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylim([-delta_y, 1+delta_y])\n",
    "    ax.set_xlim([0, 20])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticks([thr], minor=True)\n",
    "    ax.grid(b=True, axis='y', linewidth=1.5, which='minor', zorder=5)\n",
    "    ax.tick_params(labelsize=8.5)\n",
    "    ax.set_title('Model probability prediction (%1.2f threshold and postprocessed detections are shown)' % thr, fontsize=10)\n",
    "    \n",
    "    if dataset_name == constants.MASS_NAME:\n",
    "        ax = fig.add_subplot(gs[gs_idx])\n",
    "        ax.imshow(segment_marks_2[np.newaxis, :], interpolation=None, aspect='auto', cmap=cmap)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Second expert, not used for training (red indicates event)', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c94eee010584be4be7434ec82fc54eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='page_idx', max=818, min=1), Outâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interact(\n",
    "    lambda page_idx: plot_page(page_idx),\n",
    "    page_idx=widgets.IntSlider(min=1,max=this_n2_pages.shape[0],step=1,value=1, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
