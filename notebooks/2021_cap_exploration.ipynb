{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import pyedflib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from sleeprnn.helpers import reader\n",
    "from sleeprnn.data import utils\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "\n",
    "viz.notebook_full_width()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 25\n",
      "['2-002',\n",
      " '4-001',\n",
      " '4-003',\n",
      " '4-005',\n",
      " '5-003',\n",
      " '5-004',\n",
      " '5-005',\n",
      " '5-007',\n",
      " '5-009',\n",
      " '5-010',\n",
      " '5-011',\n",
      " '5-015',\n",
      " '5-016',\n",
      " '5-017',\n",
      " '5-018',\n",
      " '5-019',\n",
      " '5-020',\n",
      " '5-021',\n",
      " '5-023',\n",
      " '5-028',\n",
      " '5-030',\n",
      " '5-034',\n",
      " '5-036',\n",
      " '5-038',\n",
      " '5-039']\n"
     ]
    }
   ],
   "source": [
    "CAP_DATA_PATH = \"/home/ntapia/projects/sleep-rnn/resources/datasets/unlabeled_cap\"\n",
    "REC_PATH = os.path.join(CAP_DATA_PATH, \"register\")\n",
    "STATE_PATH = os.path.join(CAP_DATA_PATH, \"label/state\")\n",
    "invalid_subjects = ['5-006', '5-027', '5-031', '5-033']\n",
    "\n",
    "subject_ids = [s[:5] for s in os.listdir(REC_PATH) if \".edf\" in s]\n",
    "subject_ids = [s for s in subject_ids if s not in invalid_subjects]\n",
    "\n",
    "subject_ids = [s for s in subject_ids if '1-' not in s]\n",
    "\n",
    "subject_ids.sort()\n",
    "print(\"Number of subjects:\", len(subject_ids))\n",
    "pprint(subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNE - PyEDFlib check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_file_check = os.path.join(REC_PATH, \"1-003 PSG.edf\")\n",
    "channel_check = 'C4-A1'\n",
    "\n",
    "# Read with pyedflib\n",
    "with pyedflib.EdfReader(eeg_file_check) as file:\n",
    "    channel_names = file.getSignalLabels()\n",
    "    chn = channel_names.index(channel_check)\n",
    "    signal_pyedflib = file.readSignal(chn)\n",
    "    fs_pyedflib = file.samplefrequency(chn)\n",
    "print(\"\\npyedflib\")\n",
    "print(\"fs\", fs_pyedflib)\n",
    "print(\"signal\", signal_pyedflib.size, signal_pyedflib.shape, signal_pyedflib.dtype, signal_pyedflib.min(), signal_pyedflib.max(), signal_pyedflib.mean(), signal_pyedflib.std())\n",
    "\n",
    "# Read with mne\n",
    "raw_data = mne.io.read_raw_edf(eeg_file_check, verbose=False)\n",
    "chn = raw_data.ch_names.index(channel_check)\n",
    "signal_mne, _ = raw_data[chn, :]\n",
    "signal_mne = np.squeeze(signal_mne)\n",
    "fs_mne = raw_data.info['sfreq']\n",
    "if signal_mne.std() < 0.001:\n",
    "    signal_mne = 1e6 * signal_mne\n",
    "print(\"\\nmne\")\n",
    "print(\"fs\", fs_mne)\n",
    "print(\"signal\", signal_mne.size, signal_mne.shape, signal_mne.dtype, signal_mne.min(), signal_mne.max(), signal_mne.mean(), signal_mne.std())\n",
    "\n",
    "mse_signals = np.mean((signal_mne - signal_pyedflib) ** 2)\n",
    "print(\"\\nMSE between signals:\", mse_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fs = 200\n",
    "signal_dict = {}\n",
    "starting_times_dict = {}\n",
    "for subject_id in subject_ids:\n",
    "    path_eeg_file = os.path.join(REC_PATH, \"%s PSG.edf\" % subject_id)\n",
    "    channels_to_try = [\n",
    "        (\"C4-A1\",),\n",
    "        (\"C4A1\",),\n",
    "        (\"C4\", \"A1\"),  # For 5-033\n",
    "        (\"C3-A2\",),\n",
    "        (\"C3A2\",),\n",
    "        (\"C3\", \"A2\"),\n",
    "    ]\n",
    "    raw_data = mne.io.read_raw_edf(path_eeg_file, verbose=False)\n",
    "    \n",
    "    recording_start_time_hh_mm_ss = raw_data.info['meas_date'].strftime(\"%H:%M:%S\")\n",
    "    starting_times_dict[subject_id] = recording_start_time_hh_mm_ss\n",
    "    fs_old = raw_data.info['sfreq']\n",
    "    channel_names = raw_data.ch_names\n",
    "    while len(channels_to_try) > 0:\n",
    "        channel = channels_to_try.pop(0)\n",
    "        if np.all([chn in channel_names for chn in channel]):\n",
    "            break\n",
    "    channel_index = channel_names.index(channel[0])\n",
    "    signal, _ = raw_data[channel_index, :]\n",
    "    if len(channel) == 2:\n",
    "        channel_index_2 = channel_names.index(channel[1])\n",
    "        signal2, _ = raw_data[channel_index_2, :]\n",
    "        signal = signal - signal2\n",
    "        # Check\n",
    "        print('Subject %s | Channel extracted: %s minus %s at %s Hz' % (subject_id, channel_names[channel_index], channel_names[channel_index_2], fs_old))\n",
    "    else:\n",
    "        # Check\n",
    "        print('Subject %s | Channel extracted: %s at %s Hz' % (subject_id, channel_names[channel_index], fs_old))  \n",
    "    signal = signal[0, :]\n",
    "    if signal.std() < 0.001:\n",
    "        # Signal is in volts, transform to microvolts for simplicity\n",
    "        signal = 1e6 * signal\n",
    "    # The sampling frequency is already an integer in CAP\n",
    "    fs_old_round = int(np.round(fs_old))\n",
    "    # Broand bandpass filter to signal\n",
    "    signal = utils.broad_filter(signal, fs_old_round)\n",
    "    # Now resample to the required frequency\n",
    "    if fs != fs_old_round:\n",
    "        print('Resampling from %d Hz to required %d Hz' % (fs_old_round, fs))\n",
    "        signal = utils.resample_signal(signal, fs_old=fs_old_round, fs_new=fs)\n",
    "    else:\n",
    "        print('Signal already at required %d Hz' % fs)\n",
    "    signal = signal.astype(np.float32)\n",
    "    # No robust normalization nor clipping for now\n",
    "    signal_dict[subject_id] = signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypnogram reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_to_relative_time(abs_time_str, start_time_str):\n",
    "    start_t = datetime.strptime(start_time_str, '%H:%M:%S')\n",
    "    end_t = datetime.strptime(abs_time_str, '%H:%M:%S')\n",
    "    delta = end_t - start_t\n",
    "    return delta.seconds\n",
    "\n",
    "\n",
    "def get_skiprows_cap_states(states_file_path):\n",
    "    with open(states_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    skiprows = 0\n",
    "    for line in lines:\n",
    "        if 'Sleep Stage' in line:\n",
    "            break\n",
    "        skiprows += 1\n",
    "    return skiprows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n2_id = 'SLEEP-S2'\n",
    "original_page_duration = 30\n",
    "page_duration = 20\n",
    "page_size = int(fs * page_duration)\n",
    "\n",
    "total_30s_pages = 0\n",
    "total_20s_pages = 0\n",
    "\n",
    "n2_pages_dict = {}\n",
    "for subject_id in subject_ids:\n",
    "    print(\"\")\n",
    "    signal_length = signal_dict[subject_id].size\n",
    "    print(\"Signal length\", signal_length, \"Total 30s pages\", signal_length / (fs * original_page_duration))\n",
    "    path_states_file = os.path.join(STATE_PATH, \"%s Base.txt\" % subject_id)\n",
    "    starting_time = starting_times_dict[subject_id]\n",
    "    skiprows = get_skiprows_cap_states(path_states_file)\n",
    "    states_df = pd.read_csv(path_states_file, skiprows=skiprows, sep='\\t')\n",
    "    states_df = states_df.dropna()\n",
    "    column_names = states_df.columns.values\n",
    "    duration_col_name = column_names[[('Duration' in s) for s in column_names]][0]\n",
    "    time_hhmmss_col_name = column_names[[('hh:mm:ss' in s) for s in column_names]][0]\n",
    "    states_df['Time [s]'] = states_df[time_hhmmss_col_name].apply(lambda x: absolute_to_relative_time(x, starting_time))\n",
    "    n2_stages = states_df.loc[states_df['Event'] == n2_id]\n",
    "    n2_stages = n2_stages.loc[n2_stages[duration_col_name] == original_page_duration]\n",
    "    # These are pages with 30s durations. To work with 20s pages\n",
    "    # We consider the intersections with 20s divisions\n",
    "    n2_pages_original = n2_stages[\"Time [s]\"].values / original_page_duration\n",
    "    print(\"First page before rounding\", n2_pages_original[0])\n",
    "    n2_pages_original = n2_pages_original.astype(np.int32)\n",
    "    print(\"Max N2 page\", n2_pages_original.max())\n",
    "    print('Original N2 pages: %d' % n2_pages_original.size)\n",
    "    onsets_original = n2_pages_original * original_page_duration\n",
    "    offsets_original = (n2_pages_original + 1) * original_page_duration\n",
    "    total_pages = int(np.ceil(signal_length / page_size))\n",
    "    n2_pages_onehot = np.zeros(total_pages, dtype=np.int16)\n",
    "    for i in range(total_pages):\n",
    "        onset_new_page = i * page_duration\n",
    "        offset_new_page = (i + 1) * page_duration\n",
    "        for j in range(n2_pages_original.size):\n",
    "            intersection = (onset_new_page < offsets_original[j]) and (onsets_original[j] < offset_new_page)\n",
    "            if intersection:\n",
    "                n2_pages_onehot[i] = 1\n",
    "                break\n",
    "    n2_pages = np.where(n2_pages_onehot == 1)[0]\n",
    "    # Drop first, last and second to last page of the whole registers\n",
    "    # if they where selected.\n",
    "    last_page = total_pages - 1\n",
    "    n2_pages = n2_pages[\n",
    "        (n2_pages != 0)\n",
    "        & (n2_pages != last_page)\n",
    "        & (n2_pages != last_page - 1)]\n",
    "    n2_pages = n2_pages.astype(np.int16)\n",
    "    print(\"Subject %s - Total N2 pages %d\" % (subject_id, n2_pages.size))\n",
    "    n2_pages_dict[subject_id] = n2_pages\n",
    "    total_30s_pages += n2_pages_original.size\n",
    "    total_20s_pages += n2_pages.size\n",
    "print(\"Total 30s pages:\", total_30s_pages)\n",
    "print(\"Total 20s pages:\", total_20s_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = subject_ids[28] #'5-027' # \"1-001\"\n",
    "\n",
    "\n",
    "def draw_single_page(page_index_to_show, dpi, ylim=150):\n",
    "    page = n2_pages_dict[subject_id][page_index_to_show]\n",
    "    start_sample = int(page * page_size)\n",
    "    end_sample = start_sample + page_size\n",
    "    segment_signal = signal_dict[subject_id][start_sample:end_sample]\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 4), dpi=dpi)\n",
    "    axes[0].plot(segment_signal, linewidth=0.6)\n",
    "    axes[0].set_ylim([-ylim, ylim])\n",
    "    sigma_signal = utils.broad_filter(segment_signal, fs, lowcut=11, highcut=16)\n",
    "    axes[1].plot(sigma_signal, linewidth=0.6)\n",
    "    axes[1].set_ylim([-ylim, ylim])\n",
    "    axes[0].set_title(\"Subject %s, page in record %d\" % (subject_id, page))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    power, freq = utils.power_spectrum(segment_signal, fs)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=dpi)\n",
    "    ax.plot(freq, power, linewidth=0.7)\n",
    "    ax.set_xlim([0, 30])\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_index_to_show = 90 # 115\n",
    "\n",
    "\n",
    "dpi = 120\n",
    "print('Total N2 pages: %d' % n2_pages_dict[subject_id].size)\n",
    "style = {'description_width': 'initial'}\n",
    "layout= widgets.Layout(width='1000px')\n",
    "widgets.interact(\n",
    "    lambda page_id: draw_single_page(page_id, dpi=dpi),\n",
    "    page_id=widgets.IntSlider(\n",
    "        min=0, max=n2_pages_dict[subject_id].size-1, step=1, value=page_index_to_show, \n",
    "        continuous_update=False,\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    ));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_power_dict = {}\n",
    "for subject_id in signal_dict.keys():\n",
    "    all_pages_power = []\n",
    "    for page in n2_pages_dict[subject_id]:\n",
    "        start_sample = int(page * page_size)\n",
    "        end_sample = start_sample + page_size\n",
    "        segment_signal = signal_dict[subject_id][start_sample:end_sample]\n",
    "        power, freq = utils.power_spectrum(segment_signal, fs)\n",
    "        all_pages_power.append(power)\n",
    "    mean_power_dict[subject_id] = np.stack(all_pages_power, axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4), dpi=120)\n",
    "min_freq = 0.5\n",
    "max_freq = 30\n",
    "plot_indices = np.where((freq >= min_freq) & (freq <= max_freq))[0]\n",
    "for subject_id in subject_ids:\n",
    "    if subject_id == subject_ids[24]:\n",
    "        ax.plot(freq[plot_indices], mean_power_dict[subject_id][plot_indices], linewidth=0.7, color=\"r\", alpha=1, zorder=20, label=subject_id)\n",
    "    else:\n",
    "        ax.plot(freq[plot_indices], mean_power_dict[subject_id][plot_indices], linewidth=0.7, color=\"k\", alpha=0.2)\n",
    "ax.set_xlim([0, 30])\n",
    "ax.set_yticks([])\n",
    "ax.legend()\n",
    "ax.set_title(\"Mean N2 spectrum of CAP subjects (n=%d)\" % len(subject_ids))\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of spectrum with MASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = reader.load_dataset(constants.MASS_SS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_mean_power_dict = {}\n",
    "for subject_id in mass.train_ids:\n",
    "    all_pages_power = []\n",
    "    n2_pages_of_subject = mass.get_subject_pages(subject_id, pages_subset=constants.N2_RECORD)\n",
    "    print(subject_id, n2_pages_of_subject.size)\n",
    "    signal_of_subject = mass.get_subject_signal(subject_id, normalize_clip=True)\n",
    "    for page in n2_pages_of_subject:\n",
    "        start_sample = int(page * page_size)\n",
    "        end_sample = start_sample + page_size\n",
    "        segment_signal = signal_of_subject[start_sample:end_sample]\n",
    "        power, freq = utils.power_spectrum(segment_signal, fs)\n",
    "        all_pages_power.append(power)\n",
    "    mass_mean_power_dict[subject_id] = np.stack(all_pages_power, axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4), dpi=120)\n",
    "min_freq = 2\n",
    "max_freq = 30\n",
    "plot_indices = np.where((freq >= min_freq) & (freq <= max_freq))[0]\n",
    "for subject_id in mass_mean_power_dict.keys():\n",
    "    ax.plot(freq[plot_indices], mass_mean_power_dict[subject_id][plot_indices], linewidth=0.7, color=\"k\", alpha=0.2)\n",
    "    ax.set_xlim([0, 30])\n",
    "    ax.set_yticks([])\n",
    "ax.set_title(\"Mean N2 spectrum of MASS subjects (n=%d)\" % len(mass_mean_power_dict.keys()))\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# side by side\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), dpi=200, sharey=True, sharex=True)\n",
    "min_freq = 0.5\n",
    "max_freq = 30\n",
    "plot_indices = np.where((freq >= min_freq) & (freq <= max_freq))[0]\n",
    "\n",
    "# cap\n",
    "ax = axes[0]\n",
    "for subject_id in subject_ids:\n",
    "    ax.plot(freq[plot_indices], mean_power_dict[subject_id][plot_indices] / mass.global_std, linewidth=0.7, color=\"k\", alpha=0.2)\n",
    "    ax.set_xlim([0, 30])\n",
    "    # ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "ax.set_title(\"Mean N2 spectrum of CAP subjects (n=%d)\" % len(subject_ids))\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.grid()\n",
    "\n",
    "# mass\n",
    "ax = axes[1]\n",
    "for subject_id in mass_mean_power_dict.keys():\n",
    "    ax.plot(freq[plot_indices], mass_mean_power_dict[subject_id][plot_indices], linewidth=0.7, color=\"k\", alpha=0.2)\n",
    "    ax.set_xlim([0, 30])\n",
    "    # ax.set_yticks([])\n",
    "ax.set_title(\"Mean N2 spectrum of MASS subjects (n=%d)\" % len(mass_mean_power_dict.keys()))\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CAP *.mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"cap_subset_mat\", exist_ok=True)\n",
    "for subject_id in subject_ids:\n",
    "    this_signal = signal_dict[subject_id]\n",
    "    this_signal = np.clip(this_signal, a_min=-10*mass.global_std, a_max=10*mass.global_std) \n",
    "    this_n2_pages_from_zero = n2_pages_dict[subject_id]\n",
    "    fname = os.path.join(\"cap_subset_mat\", \"cap_s%s_fs_%s.mat\" % (subject_id, fs))\n",
    "    print(this_signal.min(), this_signal.max())\n",
    "    scipy.io.savemat(fname, {\"signal\": this_signal, \"n2_pages_from_zero\": this_n2_pages_from_zero})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
