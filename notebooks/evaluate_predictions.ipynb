{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.data.loader import load_dataset, RefactorUnpickler\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.common import constants, pkeys\n",
    "\n",
    "SEED_LIST = [123, 234, 345, 456]\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "COMPARISON_PATH = os.path.join(project_root, 'resources', 'comparison_data')\n",
    "DPI = 200\n",
    "CUSTOM_COLOR = {'red': '#c62828', 'grey': '#455a64', 'blue': '#0277bd', 'green': '#43a047'} \n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load database and predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mass_kc with 15 patients.\n",
      "Loading from checkpoint... Loaded\n",
      "Loaded seed 2/1 from /home/ntapia/Projects/GitNico/sleep-rnn/results/predictions_mass_kc/20190506_bsf_n2_train_mass_kc/bsf/seed1\n",
      "Optimal thr: [0.52, 0.52, 0.56, 0.46]\n"
     ]
    }
   ],
   "source": [
    "optimal_thr_for_ckpt_dict = {\n",
    "    os.path.join('20190504_bsf_wn_train_mass_ss', 'bsf'): [0.64, 0.52, 0.52, 0.48],\n",
    "    os.path.join('20190504_bsf_wn_train_mass_kc', 'bsf'): [0.52, 0.56, 0.54, 0.56],\n",
    "    os.path.join('20190506_bsf_n2_train_mass_ss', 'bsf'): [0.52, 0.46, 0.50, 0.50],\n",
    "    os.path.join('20190506_bsf_n2_train_mass_kc', 'bsf'): [0.52, 0.52, 0.56, 0.46],\n",
    "}\n",
    "\n",
    "\n",
    "ckpt_folder = os.path.join('20190506_bsf_n2_train_mass_kc', 'bsf')\n",
    "optimal_thr_list = optimal_thr_for_ckpt_dict[ckpt_folder]\n",
    "task_mode = constants.N2_RECORD\n",
    "dataset_name = constants.MASS_KC_NAME\n",
    "# seed_id_list = [0, 1, 2, 3]\n",
    "seed_id_list = [1]\n",
    "\n",
    "n_seeds = len(seed_id_list)\n",
    "# set_list = [constants.TRAIN_SUBSET, constants.VAL_SUBSET, constants.TEST_SUBSET]\n",
    "set_list = [constants.TEST_SUBSET]\n",
    "which_expert = 1\n",
    "verbose = False\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(dataset_name)\n",
    "all_train_ids = dataset.train_ids\n",
    "test_ids = dataset.test_ids\n",
    "predictions_dict = {}\n",
    "for k in seed_id_list:\n",
    "    # Restore predictions\n",
    "    ckpt_path = os.path.abspath(os.path.join(\n",
    "        RESULTS_PATH,\n",
    "        'predictions_%s' % dataset_name,\n",
    "        ckpt_folder,\n",
    "        'seed%d' % k\n",
    "    ))\n",
    "    this_dict = {}\n",
    "    for set_name in set_list:\n",
    "        filename = os.path.join(\n",
    "                ckpt_path,\n",
    "                'prediction_%s_%s.pkl' % (task_mode, set_name))\n",
    "        with open(filename, 'rb') as handle:\n",
    "            this_pred = RefactorUnpickler(handle).load()\n",
    "        this_dict[set_name] = this_pred\n",
    "    predictions_dict[k] = this_dict\n",
    "    print('Loaded seed %d/%d from %s' % (k + 1, n_seeds, ckpt_path))\n",
    "print('Optimal thr:', optimal_thr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output thr selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_id_for_selection = 1\n",
    "\n",
    "\n",
    "# Adjust thr\n",
    "res_thr = 0.02\n",
    "start_thr = 0.3\n",
    "end_thr = 0.7\n",
    "n_thr = int(np.round((end_thr - start_thr) / res_thr + 1))\n",
    "thr_list = np.array([start_thr + res_thr * i for i in range(n_thr)])\n",
    "thr_list = np.round(thr_list, 2)\n",
    "print('%d thresholds to be evaluated between %1.4f and %1.4f'\n",
    "      % (n_thr, thr_list[0], thr_list[-1]))\n",
    "\n",
    "per_seed_af1 = []\n",
    "# Validation split\n",
    "train_ids, val_ids = utils.split_ids_list(\n",
    "    all_train_ids, seed=SEED_LIST[seed_id_for_selection], verbose=False)\n",
    "ids_dict = {\n",
    "    constants.TRAIN_SUBSET: train_ids,\n",
    "    constants.VAL_SUBSET: val_ids}\n",
    "for thr in thr_list:\n",
    "    events_list = []\n",
    "    detections_list = []\n",
    "    for set_name in [constants.TRAIN_SUBSET, constants.VAL_SUBSET]:\n",
    "        # Prepare expert labels\n",
    "        data_inference = FeederDataset(\n",
    "            dataset, ids_dict[set_name], task_mode,\n",
    "            which_expert=which_expert)\n",
    "        this_events = data_inference.get_stamps()\n",
    "        # Prepare model predictions\n",
    "        prediction_obj = predictions_dict[seed_id_for_selection][set_name]\n",
    "        prediction_obj.set_probability_threshold(thr)\n",
    "        this_detections = prediction_obj.get_stamps()\n",
    "        events_list = events_list + this_events\n",
    "        detections_list = detections_list + this_detections\n",
    "    # Compute AF1\n",
    "    af1_at_thr = metrics.average_metric_with_list(\n",
    "        events_list, detections_list, verbose=False)\n",
    "    per_seed_af1.append(af1_at_thr)\n",
    "print('Done')\n",
    "max_idx = np.argmax(per_seed_af1).item()\n",
    "this_best_thr = thr_list[max_idx]\n",
    "print('Best thr: %1.2f' % this_best_thr)\n",
    "\n",
    "# For best thr, compute F1 vs IoU curve\n",
    "events_list = []\n",
    "detections_list = []\n",
    "for set_name in [constants.TRAIN_SUBSET, constants.VAL_SUBSET]:\n",
    "    # Prepare expert labels\n",
    "    data_inference = FeederDataset(\n",
    "        dataset, ids_dict[set_name], task_mode,\n",
    "        which_expert=which_expert)\n",
    "    this_events = data_inference.get_stamps()\n",
    "    # Prepare model predictions\n",
    "    prediction_obj = predictions_dict[seed_id_for_selection][set_name]\n",
    "    prediction_obj.set_probability_threshold(this_best_thr)\n",
    "    this_detections = prediction_obj.get_stamps()\n",
    "    events_list = events_list + this_events\n",
    "    detections_list = detections_list + this_detections\n",
    "# Compute AF1\n",
    "first_iou = 0\n",
    "last_iou = 1\n",
    "res_iou = 0.01\n",
    "n_points = int(np.round((last_iou - first_iou) / res_iou))\n",
    "full_iou_list = np.arange(n_points + 1) * res_iou + first_iou\n",
    "f1_alltrain = metrics.metric_vs_iou_with_list(\n",
    "    events_list, detections_list, full_iou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), dpi=200)\n",
    "\n",
    "ax[0].plot(full_iou_list, f1_alltrain, color=CUSTOM_COLOR['red'])\n",
    "ax[0].fill_between(full_iou_list, f1_alltrain, 0, color=CUSTOM_COLOR['red'], alpha=0.5)\n",
    "ax[0].set_title('F1 vs IoU curve at $\\mu$=%1.2f' % this_best_thr, fontsize=10)\n",
    "ax[0].set_xlim([0, 1])\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[0].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[0].tick_params(labelsize=7)\n",
    "ax[0].set_xlabel('IoU Threshold', fontsize=7)\n",
    "ax[0].set_ylabel('F1-score', fontsize=7)\n",
    "\n",
    "min_y = np.min(per_seed_af1) - 0.05\n",
    "max_y = np.max(per_seed_af1) + 0.05\n",
    "ax[1].plot(\n",
    "    thr_list, per_seed_af1, \n",
    "    color=CUSTOM_COLOR['red'], marker='o', markersize=4, label='Train+val set', zorder=10)\n",
    "ax[1].plot(\n",
    "    [this_best_thr, this_best_thr], [min_y, max_y], \n",
    "    color=CUSTOM_COLOR['grey'], linestyle='--', linewidth=1.5, label='Optimal $\\mu$=%1.2f' % this_best_thr, zorder=5)\n",
    "ax[1].set_title('Adjustment of $\\mu$', fontsize=10)\n",
    "ax[1].set_xticks([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "ax[1].set_ylim([min_y, max_y])\n",
    "ax[1].tick_params(labelsize=7)\n",
    "ax[1].set_xlabel('Model Output Threshold $\\mu$', fontsize=7)\n",
    "ax[1].set_ylabel('AF1', fontsize=7)\n",
    "ax[1].legend(loc='upper right', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance: F1 vs IoU curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------- Compute performance\n",
    "f1_vs_iou_dict = {}\n",
    "iou_list = np.arange(21) * 0.05\n",
    "\n",
    "# Prepare expert labels\n",
    "data_test = FeederDataset(\n",
    "    dataset, test_ids, task_mode, which_expert=which_expert)\n",
    "this_events = data_test.get_stamps()\n",
    "\n",
    "seed_stamps = []\n",
    "for k in seed_id_list:\n",
    "    # Prepare model predictions\n",
    "    prediction_test = predictions_dict[k][constants.TEST_SUBSET]\n",
    "    prediction_test.set_probability_threshold(optimal_thr_list[k])\n",
    "    this_detections = prediction_test.get_stamps()\n",
    "    seed_stamps.append(this_detections)\n",
    "    this_f1_vs_iou = metrics.metric_vs_iou_with_list(\n",
    "        this_events, this_detections, iou_list)\n",
    "    f1_vs_iou_dict[k] = this_f1_vs_iou\n",
    "    \n",
    "# Mean performance\n",
    "mean_f1_vs_iou = np.stack([f1_vs_iou_dict[k] for k in seed_id_list], axis=1).mean(axis=1)\n",
    "std_f1_vs_iou = np.stack([f1_vs_iou_dict[k] for k in seed_id_list], axis=1).std(axis=1)\n",
    "\n",
    "print('IoU   Mean    Std')\n",
    "for k in range(0, iou_list.size, 2):\n",
    "    print('%1.2f  %1.4f  %1.4f' % (iou_list[k], mean_f1_vs_iou[k], std_f1_vs_iou[k]))\n",
    "\n",
    "test_af1_list = []\n",
    "for k in seed_id_list:\n",
    "    this_curve = f1_vs_iou_dict[k]\n",
    "    test_af1_list.append((this_curve[0]/2 + this_curve[1:-1].sum() + this_curve[-1]/2) / (this_curve.shape[0] - 1))\n",
    "print('Test AF1: %1.4f +- %1.4f' % (np.mean(test_af1_list), np.std(test_af1_list)))\n",
    "print('Test AF1 per seed:', test_af1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "filename = '%s_f1_vs_iou.npy' % (ckpt_folder.split('/')[0])\n",
    "data = np.stack([iou_list, mean_f1_vs_iou, std_f1_vs_iou], axis=1).astype(np.float32)\n",
    "np.save(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_thr = 0.5  # Majority vote is 0.5\n",
    "binarized_first = True\n",
    "\n",
    "detections_half_rule_list = []\n",
    "detections_greater_half_rule_list = []\n",
    "\n",
    "postprocessor = PostProcessor(\n",
    "    predictions_dict[0][constants.TEST_SUBSET].event_name, \n",
    "    predictions_dict[0][constants.TEST_SUBSET].params)\n",
    "\n",
    "for j in range(len(test_ids)):\n",
    "    if task_mode == constants.N2_RECORD:\n",
    "        pages_indices_subset = predictions_dict[0][constants.TEST_SUBSET].get_subject_pages(\n",
    "            test_ids[j], pages_subset=constants.N2_RECORD, verbose=False)\n",
    "    else:\n",
    "        pages_indices_subset = None\n",
    "    \n",
    "    this_subject_detections_list_seq = [predictions_dict[k][constants.TEST_SUBSET].get_probabilities()[j].astype(np.float32) for k in seed_id_list]\n",
    "    if binarized_first:\n",
    "        this_subject_detections_list_seq = [(this_det >= this_thr).astype(np.int32) for (this_det, this_thr) in zip(this_subject_detections_list_seq, optimal_thr_list)]\n",
    "    this_subject_detections_mean = np.stack(this_subject_detections_list_seq, axis=0).mean(axis=0)\n",
    "    \n",
    "    detections_half_rule = postprocessor.proba2stamps(\n",
    "        this_subject_detections_mean, None, pages_indices_subset, thr=ensamble_thr)\n",
    "    detections_half_rule_list.append(detections_half_rule)\n",
    "    \n",
    "half_rule_f1_vs_iou = metrics.metric_vs_iou_with_list(\n",
    "        this_events, detections_half_rule_list, iou_list)\n",
    "half_rule_af1 = (half_rule_f1_vs_iou[0]/2 + half_rule_f1_vs_iou[1:-1].sum() + half_rule_f1_vs_iou[-1]/2) / (half_rule_f1_vs_iou.shape[0] - 1)\n",
    "\n",
    "print('Test AF1 Ensamble:')\n",
    "print('Half Rule: %1.4f' % half_rule_af1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsf_name = 'Proposed (%s)' % task_mode.upper()\n",
    "\n",
    "compare_expert = False\n",
    "compare_chambon = True\n",
    "show_seed_std = True\n",
    "show_ensamble = False\n",
    "alpha = 0.3\n",
    "color_list = {'model_mean': CUSTOM_COLOR['red'] , 'expert': CUSTOM_COLOR['grey'], 'dosed': CUSTOM_COLOR['blue']}\n",
    "linewidth_model = 1.5\n",
    "markersize_model = 7\n",
    "linewidth_others = 1.5\n",
    "markersize_others = 7\n",
    "\n",
    "# -------------------- P L O T ----------------------    \n",
    "\n",
    "# Comparison data\n",
    "compare_expert = (compare_expert and (dataset.event_name == constants.SPINDLE))\n",
    "if compare_expert:\n",
    "    expert_f1_curve_mean = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_mean.csv'), delimiter=',')\n",
    "    expert_f1_curve_std = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_std.csv'), delimiter=',')\n",
    "if compare_chambon and (dataset_name == constants.MASS_SS_NAME):\n",
    "    dosed_f1_curve_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_ss_wn_f1_vs_iou_dosed_separately.csv'), delimiter=',')\n",
    "if compare_chambon and (dataset_name == constants.MASS_KC_NAME):\n",
    "    dosed_f1_curve_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_kc_wn_f1_vs_iou_dosed_separately.csv'), delimiter=',')\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "\n",
    "\n",
    "# Complete plot\n",
    "if compare_expert:\n",
    "    ax.plot(expert_f1_curve_mean[:, 0], expert_f1_curve_mean[:, 1], linewidth=linewidth_others, \n",
    "               markersize=markersize_others, marker='.', \n",
    "               label='Expert Performance\\nPrivate Dataset\\nWarby et al. 2014', color=color_list['expert'])\n",
    "    ax.fill_between(\n",
    "        expert_f1_curve_mean[:, 0], \n",
    "        expert_f1_curve_mean[:, 1] - expert_f1_curve_std[:, 1], \n",
    "        expert_f1_curve_mean[:, 1] + expert_f1_curve_std[:, 1], \n",
    "        alpha=alpha, facecolor=color_list['expert'])\n",
    "\n",
    "if compare_chambon and (dataset_name in [constants.MASS_SS_NAME, constants.MASS_KC_NAME]):\n",
    "    ax.plot(dosed_f1_curve_wn[:, 0], dosed_f1_curve_wn[:, 1], linewidth=linewidth_others, \n",
    "               markersize=markersize_others, marker='.', \n",
    "               label='DOSED (WN) (Paper)\\nChambon et al. 2019', color=color_list['dosed'])\n",
    "    idx_to_show = np.where(np.isclose(dosed_f1_curve_wn[:, 0], 0.3))[0][0]\n",
    "    print('%1.4f Dosed F1 at 0.3' % dosed_f1_curve_wn[idx_to_show, 1])\n",
    "\n",
    "ax.plot(iou_list, mean_f1_vs_iou, \n",
    "           linewidth=linewidth_model, markersize=markersize_model, marker='.', \n",
    "           label='%s (Mean)' % bsf_name, color=color_list['model_mean'])\n",
    "idx_to_show = np.where(np.isclose(iou_list, 0.3))[0][0]\n",
    "print('%1.4f Proposed mean F1 at 0.3' % mean_f1_vs_iou[idx_to_show])\n",
    "if show_seed_std:\n",
    "    ax.fill_between(\n",
    "        iou_list, \n",
    "        mean_f1_vs_iou - std_f1_vs_iou, \n",
    "        mean_f1_vs_iou + std_f1_vs_iou, \n",
    "        alpha=alpha, facecolor=color_list['model_mean'])\n",
    "\n",
    "if show_ensamble:\n",
    "    ax.plot(iou_list, half_rule_f1_vs_iou, \n",
    "           linewidth=linewidth_model, markersize=markersize_model, marker='.', \n",
    "           label='%s (Ensemble)' % bsf_name, color=CUSTOM_COLOR['green'])\n",
    "    \n",
    "ax.set_title('Test Performance (%s Dataset)' % dataset_name.upper(), fontsize=10)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax.set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax.set_ylabel('F1-score', fontsize=8.5)\n",
    "ax.yaxis.grid()\n",
    "ax.legend(loc='lower left', labelspacing=1.5, fontsize=6.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance: Precision-Recall curve, average per set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_thr = 0.3\n",
    "res_thr = 0.02\n",
    "start_thr = 0.1\n",
    "end_thr = 0.9\n",
    "n_thr = int(np.round((end_thr - start_thr) / res_thr + 1))\n",
    "thr_list = np.array([start_thr + res_thr * i for i in range(n_thr)])\n",
    "\n",
    "pr_curve = {}\n",
    "# Prepare expert labels\n",
    "data_test = FeederDataset(\n",
    "    dataset, test_ids, task_mode, which_expert=which_expert)\n",
    "this_events = data_test.get_stamps()\n",
    "for k in seed_id_list:\n",
    "    print('Processing seed %d' % k, flush=True)\n",
    "    # Columns are [x: recall, y: precision]\n",
    "    pr_curve[k] = np.zeros((n_thr, 2))\n",
    "    for i, thr in enumerate(thr_list):\n",
    "        # Prepare model predictions\n",
    "        prediction_test = predictions_dict[k][constants.TEST_SUBSET]\n",
    "        prediction_test.set_probability_threshold(thr)\n",
    "        this_detections = prediction_test.get_stamps()\n",
    "        \n",
    "        this_stats = [metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "                    for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "        this_recall = np.mean([m[constants.RECALL] for m in this_stats])\n",
    "        this_precision = np.mean([m[constants.PRECISION] for m in this_stats])\n",
    "        pr_curve[k][i, 0] = this_recall\n",
    "        pr_curve[k][i, 1] = this_precision\n",
    "\n",
    "# Mean of runs\n",
    "# pr_curve['mean_runs'] = np.stack([pr_curve[k] for k in seed_id_list], axis=2).mean(axis=2)\n",
    "print('Done', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "filename = '%s_pr_curve.npy' % (ckpt_folder.split('/')[0])\n",
    "data = []\n",
    "this_thr = np.asarray(thr_list)[:, np.newaxis]\n",
    "for k in seed_id_list:\n",
    "    this_pr = pr_curve[k]\n",
    "    this_pr = np.concatenate([this_thr, this_pr], axis=1)\n",
    "    data.append(this_pr)\n",
    "data = np.stack(data, axis=0).astype(np.float32)\n",
    "np.save(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "bsf_name = 'Proposed (%s)' % task_mode.upper()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "markersize = 6\n",
    "alpha = 0.4\n",
    "text_space = 0.01\n",
    "compare_chambon = True\n",
    "show_seeds = True\n",
    "axis_lims = [0, 1.0]\n",
    "color_list = {'model_mean': CUSTOM_COLOR['red'] , 'dosed': CUSTOM_COLOR['blue']}\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "\n",
    "CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax.clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "\n",
    "# Chambon\n",
    "if compare_chambon and (dataset_name == constants.MASS_SS_NAME):\n",
    "    dosed_rec_prec_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_ss_wn_pr_dosed_separately.csv'), delimiter=',')\n",
    "    ax.plot(dosed_rec_prec_wn[0], dosed_rec_prec_wn[1], \n",
    "           markersize=8, c=color_list['dosed'], zorder=10, \n",
    "            label='DOSED (WN) (Paper)', marker='o', linestyle=\"None\")\n",
    "if compare_chambon and (dataset_name == constants.MASS_KC_NAME):\n",
    "    dosed_rec_prec_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_kc_wn_pr_dosed_separately.csv'), delimiter=',')\n",
    "    ax.plot(dosed_rec_prec_wn[0], dosed_rec_prec_wn[1], \n",
    "           markersize=8, c=color_list['dosed'], zorder=10, \n",
    "            label='DOSED (WN) (Paper)', marker='o', linestyle=\"None\")\n",
    "    \n",
    "if show_seeds:\n",
    "    # Show single seeds\n",
    "    seed_shown = False\n",
    "    for k in seed_id_list:\n",
    "        if not seed_shown:\n",
    "            seed_shown=True\n",
    "            label = '%s (Seed curve)' % bsf_name\n",
    "            label_2 = '%s (Seed O.P.)' % bsf_name\n",
    "        else:\n",
    "            label = None\n",
    "            label_2 = None\n",
    "        ax.plot(pr_curve[k][:, 0], pr_curve[k][:, 1], \n",
    "            label=label,\n",
    "            linewidth=1, color=color_list['model_mean'], zorder=7, alpha=alpha)\n",
    "        \n",
    "        chosen_thr_idx = np.where(np.isclose(thr_list, optimal_thr_list[k]))[0].item()\n",
    "        ax.scatter(pr_curve[k][chosen_thr_idx, 0], pr_curve[k][chosen_thr_idx, 1], \n",
    "                   s=50, c=color_list['model_mean'], zorder=7, alpha=alpha, label=label_2)\n",
    "    \n",
    "# Mean of runs\n",
    "# ax.plot(pr_curve['mean_runs'][:, 0], pr_curve['mean_runs'][:, 1], \n",
    "#         label='%s (Mean)' % bsf_name,\n",
    "#         linewidth=1.5, color=color_list['model_mean'], zorder=10)\n",
    "\n",
    "# Highlight chosen operating point\n",
    "# chosen_thr_idx = np.where(np.isclose(thr_list, thr_run))[0].item()\n",
    "# ax.scatter(pr_curve['mean_runs'][chosen_thr_idx, 0], pr_curve['mean_runs'][chosen_thr_idx, 1], \n",
    "#            s=50, c=color_list['model_mean'], zorder=10, label='Operating point $\\mu$=%1.2f' % thr_run)\n",
    "# ax.annotate('$\\mu$=%1.3f' % thr_run, \n",
    "#             (pr_curve['mean_runs'][chosen_thr_idx, 0], \n",
    "#              pr_curve['mean_runs'][chosen_thr_idx, 1] + text_space*2), \n",
    "#             fontsize=7, color='#1b2631', zorder=30)  \n",
    "\n",
    "ax.set_title('Test PR Curve with IoU$>$%1.1f (%s)' % (iou_thr, dataset_name.upper()), fontsize=10)\n",
    "ax.set_xlabel('Recall', fontsize=8.5)\n",
    "ax.set_ylabel('Precision', fontsize=8.5)\n",
    "ax.set_xlim(axis_lims)\n",
    "ax.set_ylim(axis_lims)\n",
    "\n",
    "lg = ax.legend(loc='lower left', labelspacing=1, fontsize=6.5)\n",
    "for lh in lg.legendHandles:\n",
    "    lh.set_alpha(1.0)\n",
    "\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance: Precision-Recall plot, separated subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_thr = 0.3\n",
    "\n",
    "# Prepare model predictions\n",
    "be_stats = {}\n",
    "for k in seed_id_list:\n",
    "    print('Processing seed %d' % k, flush=True)\n",
    "    be_stats[k] = {}\n",
    "    train_ids, val_ids = utils.split_ids_list(all_train_ids, seed=SEED_LIST[k], verbose=False)\n",
    "    train_ids.sort()\n",
    "    val_ids.sort()\n",
    "    ids_per_set_dict = {\n",
    "        constants.TRAIN_SUBSET: train_ids,\n",
    "        constants.VAL_SUBSET: val_ids,\n",
    "        constants.TEST_SUBSET: test_ids\n",
    "    }\n",
    "    for set_name in set_list:\n",
    "        data_feeder = FeederDataset(\n",
    "            dataset, ids_per_set_dict[set_name], task_mode, which_expert=which_expert)\n",
    "        this_events = data_feeder.get_stamps()\n",
    "        prediction_set = predictions_dict[k][set_name]\n",
    "        prediction_set.set_probability_threshold(optimal_thr_list[k])\n",
    "        this_detections = prediction_set.get_stamps()\n",
    "        be_stats[k][set_name] = [\n",
    "            metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "            for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot \n",
    "bsf_name = 'Proposed (%s)' % task_mode.upper()\n",
    "fig, ax = plt.subplots(1, n_seeds, figsize=(2.3*n_seeds, 2.3), dpi=DPI)\n",
    "markersize = 10\n",
    "alpha = 0.3\n",
    "text_space = 0.01\n",
    "compare_chambon = False\n",
    "show_ids = False\n",
    "axis_lims = [0.5, 1.0]\n",
    "show_set_list = ['train', 'val', 'test']\n",
    "color_list = {'train': CUSTOM_COLOR['green'] , 'val': CUSTOM_COLOR['grey'], 'test': CUSTOM_COLOR['red'], 'dosed': CUSTOM_COLOR['blue']}\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "\n",
    "# Chambon\n",
    "if compare_chambon and (dataset_name == constants.MASS_SS_NAME):\n",
    "    dosed_rec_prec_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_ss_wn_pr_dosed_separately.csv'), delimiter=',')\n",
    "if compare_chambon and (dataset_name == constants.MASS_KC_NAME):\n",
    "    dosed_rec_prec_wn = np.loadtxt(\n",
    "        os.path.join(COMPARISON_PATH, 'dosed_paper', 'mass_kc_wn_pr_dosed_separately.csv'), delimiter=',')\n",
    "    \n",
    "for j in range(n_seeds):\n",
    "    seed_id = seed_id_list[j]\n",
    "    train_ids, val_ids = utils.split_ids_list(all_train_ids, seed=SEED_LIST[seed_id], verbose=False)\n",
    "    train_ids.sort()\n",
    "    val_ids.sort()\n",
    "    ids_per_set_dict = {\n",
    "        constants.TRAIN_SUBSET: train_ids,\n",
    "        constants.VAL_SUBSET: val_ids,\n",
    "        constants.TEST_SUBSET: test_ids\n",
    "    }\n",
    "    \n",
    "    \n",
    "    CS = ax[j].contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "    ax[j].clabel(CS, fontsize=6, fmt='%1.2f')\n",
    "    \n",
    "    if compare_chambon and (dataset_name in [constants.MASS_SS_NAME, constants.MASS_KC_NAME]):\n",
    "        ax[j].scatter(\n",
    "            dosed_rec_prec_wn[0], dosed_rec_prec_wn[1], \n",
    "            c=color_list['dosed'], \n",
    "            label='DOSED (WN)', marker='o',\n",
    "            s=markersize*3, zorder=10)\n",
    "    \n",
    "    for set_name in set_list:\n",
    "        if set_name in show_set_list:\n",
    "            for i, stats in enumerate(be_stats[seed_id][set_name]):\n",
    "                if i==0:\n",
    "                    label = 'Proposed (%s) (%s)' % (task_mode.upper(), set_name.capitalize())\n",
    "                else:\n",
    "                    label = None\n",
    "                ax[j].scatter(\n",
    "                    stats['recall'], stats['precision'], \n",
    "                    c=color_list[set_name], \n",
    "                    label=label, marker='s',\n",
    "                    s=markersize, zorder=10)\n",
    "                if show_ids:\n",
    "                    ax[j].annotate(\n",
    "                        ids_per_set_dict[set_name][i], \n",
    "                        (stats['recall']+text_space, stats['precision']+text_space), \n",
    "                        fontsize=7, color='#1b2631', zorder=20) \n",
    "    ax[j].set_title('Seed %d ($\\mu$=%1.2f and IoU>%1.1f)' % (seed_id, optimal_thr_list[j], iou_thr), fontsize=7)\n",
    "    ax[j].set_xlabel('Recall', fontsize=7)\n",
    "    ax[j].set_ylabel('Precision', fontsize=7)\n",
    "    ax[j].set_xlim(axis_lims)\n",
    "    ax[j].set_ylim(axis_lims)\n",
    "    ax[j].tick_params(labelsize=7)\n",
    "    ax[j].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "ax[0].legend(loc='lower left', bbox_to_anchor=(0.5, -0.4), labelspacing=1, fontsize=6, ncol=(1 + len(show_set_list)))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if constants.TEST_SUBSET in show_set_list:\n",
    "    # mean test\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=DPI)\n",
    "\n",
    "    CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "    ax.clabel(CS, fontsize=6, fmt='%1.2f')\n",
    "\n",
    "    if compare_chambon and (dataset_name in [constants.MASS_SS_NAME, constants.MASS_KC_NAME]):\n",
    "        ax.scatter(\n",
    "            dosed_rec_prec_wn[0], dosed_rec_prec_wn[1], \n",
    "            c=color_list['dosed'], \n",
    "            label='DOSED (WN) (Paper)', marker='o',\n",
    "            s=markersize*4, zorder=10)\n",
    "        dosed_f1 = 2 * dosed_rec_prec_wn[0] * dosed_rec_prec_wn[1] / (dosed_rec_prec_wn[0] + dosed_rec_prec_wn[1])\n",
    "        # print('%1.4f Dosed F1 at 0.3' % dosed_f1)\n",
    "\n",
    "    set_name = constants.TEST_SUBSET\n",
    "\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    for i in range(len(test_ids)):\n",
    "\n",
    "        recall_mean = np.stack([be_stats[k][set_name][i]['recall'] for k in seed_id_list]).mean()\n",
    "        precision_mean = np.stack([be_stats[k][set_name][i]['precision'] for k in seed_id_list]).mean()\n",
    "        recall_list.append(recall_mean)\n",
    "        precision_list.append(precision_mean)\n",
    "        if i==0:\n",
    "            label = '%s (Single subject)' % bsf_name\n",
    "        else:\n",
    "            label = None\n",
    "        ax.scatter(\n",
    "            recall_mean, precision_mean, \n",
    "            c=color_list[set_name], \n",
    "            label=label, marker='s',\n",
    "            s=markersize, zorder=10)\n",
    "        if show_ids:\n",
    "            ax.annotate(\n",
    "                ids_per_set_dict[set_name][i], \n",
    "                (recall_mean+text_space, precision_mean+text_space), \n",
    "                fontsize=7, color='#1b2631', zorder=20)\n",
    "\n",
    "    # mean of subjects\n",
    "\n",
    "    ax.scatter(\n",
    "        np.mean(recall_list), np.mean(precision_list), \n",
    "        c=color_list[set_name], \n",
    "        label='%s (Mean of subjects)' % bsf_name, marker='o',\n",
    "        s=markersize*4, zorder=10)\n",
    "\n",
    "    ax.set_title('Mean Test PR for IoU>%1.1f (%s Detection)' % (iou_thr, dataset_name[-2:].upper()), fontsize=8)\n",
    "    ax.set_xlabel('Recall', fontsize=7)\n",
    "    ax.set_ylabel('Precision', fontsize=7)\n",
    "    ax.set_xlim(axis_lims)\n",
    "    ax.set_ylim(axis_lims)\n",
    "    ax.tick_params(labelsize=7)\n",
    "    ax.grid()\n",
    "    ax.legend(loc='lower left', fontsize=7)\n",
    "    plt.show()\n",
    "    \n",
    "    each_recall = np.asarray(recall_list)\n",
    "    each_precision = np.asarray(precision_list)\n",
    "    model_f1 = np.mean(2 * each_recall * each_precision / (each_recall + each_precision))\n",
    "    # print('%1.4f Proposed mean F1 at 0.3' % model_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split IDs: Total 11 -- Training 8\n"
     ]
    }
   ],
   "source": [
    "seed_to_show = 1\n",
    "set_name = 'test'\n",
    "subject_id = 12\n",
    "show_only_n2 = True\n",
    "\n",
    "# -----\n",
    "if dataset_name in [constants.MASS_SS_NAME, constants.MASS_KC_NAME]:\n",
    "    channel_name = 'C3-CLE'\n",
    "else:\n",
    "    channel_name = 'F4-C4'\n",
    "\n",
    "if show_only_n2:\n",
    "    pages_subset = constants.N2_RECORD\n",
    "else:\n",
    "    pages_subset = constants.WN_RECORD\n",
    "    \n",
    "train_ids, val_ids = utils.split_ids_list(all_train_ids, seed=SEED_LIST[seed_to_show])\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids, 'alltrain': all_train_ids}\n",
    "idx_subject = idx_dict[set_name].index(subject_id)\n",
    "\n",
    "this_pages = dataset.get_subject_pages(subject_id=subject_id, pages_subset=pages_subset)\n",
    "this_hypnogram = dataset.get_subject_hypnogram(subject_id=subject_id)\n",
    "this_signal = dataset.get_subject_signal(subject_id=subject_id, normalization_mode=task_mode, which_expert=which_expert)\n",
    "this_stamps = dataset.get_subject_stamps(subject_id=subject_id, pages_subset=pages_subset, which_expert=which_expert)\n",
    "if (subject_id not in [4, 8, 15, 16]) and dataset_name == constants.MASS_SS_NAME:\n",
    "    this_stamps_2 = dataset.get_subject_stamps(subject_id=subject_id, pages_subset=pages_subset, which_expert=2)\n",
    "else:\n",
    "    this_stamps_2 = None\n",
    "\n",
    "event_name = dataset.event_name\n",
    "\n",
    "this_thr = optimal_thr_list[seed_to_show]\n",
    "prediction_set = predictions_dict[seed_to_show][set_name]\n",
    "prediction_set.set_probability_threshold(this_thr)\n",
    "this_predicted_stamps = prediction_set.get_subject_stamps(subject_id=subject_id, pages_subset=constants.WN_RECORD)\n",
    "this_proba = prediction_set.get_subject_probabilities(subject_id=subject_id)\n",
    "\n",
    "fs_real = dataset.fs\n",
    "down_factor = prediction_set.params[pkeys.TOTAL_DOWNSAMPLING_FACTOR]\n",
    "fs_proba = fs_real / down_factor\n",
    "\n",
    "# make a color map of fixed colors\n",
    "cmap = colors.ListedColormap(['white', '#c62828'])\n",
    "\n",
    "\n",
    "def filter_stamps(stamps, single_page, page_size):\n",
    "    pages_list = []\n",
    "    for i in range(stamps.shape[0]):\n",
    "        stamp_start_page = stamps[i, 0] // page_size\n",
    "        stamp_end_page = stamps[i, 1] // page_size\n",
    "\n",
    "        start_inside = (stamp_start_page == single_page)\n",
    "        end_inside = (stamp_end_page == single_page)\n",
    "\n",
    "        if start_inside or end_inside:\n",
    "            pages_list.append(stamps[i, :])\n",
    "    return pages_list\n",
    "\n",
    "\n",
    "def plot_page(page_idx):\n",
    "\n",
    "    if this_stamps_2 is None:\n",
    "        fig = plt.figure(figsize=(12, 3), dpi=DPI)\n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1])\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(12, 4), dpi=DPI)\n",
    "        gs = gridspec.GridSpec(3, 1, height_ratios=[4, 1, 1])\n",
    "    \n",
    "    page_idx = page_idx - 1\n",
    "    page_chosen = this_pages[page_idx]\n",
    "    page_state = this_hypnogram[page_chosen]\n",
    "    page_start = page_chosen * dataset.page_size\n",
    "    page_end = page_start + dataset.page_size\n",
    "    \n",
    "    segment_signal = this_signal[page_start:page_end]\n",
    "    \n",
    "    segment_stamps = filter_stamps(this_stamps, page_chosen, dataset.page_size)\n",
    "    if this_stamps_2 is not None:\n",
    "        segment_stamps_2 = filter_stamps(this_stamps_2, page_chosen, dataset.page_size)\n",
    "    segment_proba = this_proba[int(page_start/down_factor):int(page_end/down_factor)]\n",
    "    segment_predicted_stamps = filter_stamps(this_predicted_stamps, page_chosen, dataset.page_size)\n",
    "    \n",
    "    time_axis_real = np.arange(page_start, page_end) / fs_real\n",
    "    time_axis_proba = np.arange(int(page_start/down_factor), int(page_end/down_factor)) / fs_proba\n",
    "    \n",
    "    gs_idx = 0\n",
    "    \n",
    "    # Signal\n",
    "    y_max = 10\n",
    "    ax = fig.add_subplot(gs[gs_idx])\n",
    "    gs_idx = gs_idx + 1\n",
    "    ax.plot(\n",
    "        time_axis_real, segment_signal, \n",
    "        linewidth=1, color=CUSTOM_COLOR['grey'], label='EEG %s' % channel_name)\n",
    "    stamp_label_used = False\n",
    "    for expert_stamp in segment_stamps:\n",
    "        if stamp_label_used:\n",
    "            label = None\n",
    "        else:\n",
    "            label = event_name\n",
    "            stamp_label_used = True\n",
    "        ax.fill_between(\n",
    "            expert_stamp / fs_real, y_max, -y_max, \n",
    "            facecolor=CUSTOM_COLOR['blue'], alpha=0.3, label=label,\n",
    "            edgecolor='k', linewidth=1.5, \n",
    "        )\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([time_axis_real[0], time_axis_real[-1]])\n",
    "    ax.set_ylim([-y_max, y_max])\n",
    "    ax.set_title('Subject %d (%s-%s). Page in record: %d. State %s (intervals of 0.5s are shown).' \n",
    "                 % (subject_id, dataset_name.upper(), set_name.capitalize(), page_chosen, page_state), fontsize=10)\n",
    "    ax.set_xticks([\n",
    "        time_axis_real[0], \n",
    "        time_axis_real[0] + 5, \n",
    "        time_axis_real[0] + 10, \n",
    "        time_axis_real[0] + 15, \n",
    "        time_axis_real[0] + 20])\n",
    "    ax.set_xticks(np.arange(time_axis_real[0], time_axis_real[-1], 0.5), minor=True)\n",
    "    ax.grid(b=True, axis='x', which='minor')\n",
    "    lg = ax.legend(loc='upper right', fontsize=8.5)\n",
    "    # lh = lg.legendHandles[1]\n",
    "    # lh.set_facecolor(lh.get_facecolor())\n",
    "    # lh.set_alpha(1.0)\n",
    "    ax.tick_params(labelsize=8.5)\n",
    "    \n",
    "    # Neural net\n",
    "    delta_y = 0.1\n",
    "    ax = fig.add_subplot(gs[gs_idx])\n",
    "    gs_idx = gs_idx + 1\n",
    "    ax.plot(\n",
    "        time_axis_proba, segment_proba, \n",
    "        color=CUSTOM_COLOR['red'], linewidth=1.5, zorder=7)\n",
    "    for model_stamp in segment_predicted_stamps:\n",
    "        ax.fill_between(\n",
    "            model_stamp / fs_real, 1+delta_y, -delta_y, \n",
    "            facecolor=CUSTOM_COLOR['red'], zorder=6, alpha=0.3,\n",
    "            edgecolor='k', linewidth=1.5, \n",
    "        )\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylim([-delta_y, 1+delta_y])\n",
    "    ax.set_xlim([time_axis_real[0], time_axis_real[-1]])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticks([this_thr], minor=True)\n",
    "    ax.grid(b=True, axis='y', linewidth=1.5, which='minor', zorder=5)\n",
    "    ax.tick_params(labelsize=8.5)\n",
    "    ax.set_title('Model prediction: probability and postprocessed stamps (%1.2f threshold is shown)' % this_thr, fontsize=10)\n",
    "    \n",
    "    if this_stamps_2 is not None:\n",
    "        ax = fig.add_subplot(gs[gs_idx])\n",
    "        # ax.imshow(segment_marks_2[np.newaxis, :], interpolation=None, aspect='auto', cmap=cmap)\n",
    "        for expert_stamp in segment_stamps_2:\n",
    "            ax.fill_between(\n",
    "                expert_stamp / fs_real, y_max, -y_max, \n",
    "                facecolor=CUSTOM_COLOR['blue'], alpha=0.3,\n",
    "                edgecolor='k', linewidth=1.5, \n",
    "            )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylim([-delta_y, 1+delta_y])\n",
    "        ax.set_xlim([time_axis_real[0], time_axis_real[-1]])\n",
    "        ax.set_title('Second expert stamps (not used for training)', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11615e0dc01b410182de4387b03e30f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=22, continuous_update=False, description='page_idx', max=689, min=1), Ouâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interact(\n",
    "    lambda page_idx: plot_page(page_idx),\n",
    "    page_idx=widgets.IntSlider(min=1,max=this_pages.shape[0],step=1,value=22, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_set_error = 'test'\n",
    "thr = optimal_thr\n",
    "\n",
    "y_thr = y_stamps[chosen_set_error]\n",
    "\n",
    "# Prepare model predictions\n",
    "n_subjects = len(y_thr)\n",
    "pred_stamps = []\n",
    "for i in range(n_subjects):\n",
    "    print('Processing subject %d/%d' % (i+1, n_subjects), flush=True)\n",
    "    # Binarize\n",
    "    this_y_pred_thr = (y_pred[chosen_set_error][i] >= thr).astype(np.int32)\n",
    "    # Transform to intervals\n",
    "    this_y_pred_thr = data_ops.seq2inter_with_pages(\n",
    "        this_y_pred_thr, pages[chosen_set_error][i]\n",
    "    )\n",
    "    pred_stamps.append(this_y_pred_thr)\n",
    "fs_pred = 200 // 8 \n",
    "fs_real = 200\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Compute spacing ----------------------\n",
    "combine_thr = 0.3\n",
    "\n",
    "spacing = []\n",
    "spacing_expert = []\n",
    "for i in range(n_subjects):\n",
    "    print('Spacing for subject %d/%d' % (i+1, n_subjects), flush=True)\n",
    "    this_spacing = (pred_stamps[i][1:, 0] - pred_stamps[i][:-1, 1]) / fs_pred\n",
    "    this_spacing_expert = (y_stamps[chosen_set_error][i][1:, 0] - y_stamps[chosen_set_error][i][:-1, 1]) / fs_real\n",
    "    # Only consider for this analysis, spacing less than one second\n",
    "    this_spacing = this_spacing[this_spacing < 1]\n",
    "    this_spacing_expert = this_spacing_expert[this_spacing_expert < 1]\n",
    "    spacing.append(this_spacing)\n",
    "    spacing_expert.append(this_spacing_expert)\n",
    "print('Done')\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(8, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0, 0].set_title('Spacing between nearby expert marks', fontsize=10)\n",
    "ax[0, 1].set_title('Spacing between nearby detections', fontsize=10)\n",
    "y_max = 0\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # Real\n",
    "    n_real, _, _ = ax[i, 0].hist(spacing_expert[i], bins=[k*0.1 for k in range(11)], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 0].set_xticks([k*0.2 for k in range(6)])\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    ax[i, 0].legend(fontsize=8.5)\n",
    "    # Predicted\n",
    "    n_pred, _, _ = ax[i, 1].hist(spacing[i], bins=[k*0.1 for k in range(11)], color='#c62828', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 1].set_xticks([k*0.2 for k in range(6)])\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5)\n",
    "    max_y = max(np.max(n_real), np.max(n_pred))\n",
    "    ax[i, 0].plot([combine_thr, combine_thr], [0, max_y], '--', color='k', linewidth=2, alpha=0.6)\n",
    "    ax[i, 1].plot([combine_thr, combine_thr], [0, max_y], '--', color='k', linewidth=2, alpha=0.6)\n",
    "    # y_max = max(max_y, y_max)\n",
    "# for i in range(n_subjects):\n",
    "#     ax[i, 0].plot([combine_thr, combine_thr], [0, y_max], '--', color='k', linewidth=2, alpha=0.6)\n",
    "#     ax[i, 1].plot([combine_thr, combine_thr], [0, y_max], '--', color='k', linewidth=2, alpha=0.6)\n",
    "ax[-1, 0].set_xlabel('Spacing [s]', fontsize=8.5)\n",
    "ax[-1, 1].set_xlabel('Spacing [s]', fontsize=8.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Durations\n",
    "\n",
    "if dataset_name == constants.MASSK_NAME:\n",
    "    postprocess_predicted = False\n",
    "else:\n",
    "    postprocess_predicted = True\n",
    "\n",
    "durations = []\n",
    "durations_expert = []\n",
    "for i in range(n_subjects):\n",
    "    print('Durations for subject %d/%d' % (i+1, n_subjects), flush=True)\n",
    "    # First, combine close marks\n",
    "    if dataset_name == constants.MASSK_NAME:\n",
    "        this_pred_stamps = pred_stamps[i]\n",
    "    else:\n",
    "        this_pred_stamps = postprocessing.combine_close_marks(pred_stamps[i], fs_pred, combine_thr)\n",
    "    # Now compute durations\n",
    "    this_durations = (this_pred_stamps[:, 1] - this_pred_stamps[:, 0]) / fs_pred\n",
    "    this_durations_expert = (y_stamps[chosen_set_error][i][:, 1] - y_stamps[chosen_set_error][i][:, 0]) / fs_real\n",
    "    # Only consider for this analysis, spacing less than one second\n",
    "    durations.append(this_durations)\n",
    "    durations_expert.append(this_durations_expert)\n",
    "print('Done')\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(8, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0, 0].set_title('Duration of expert marks', fontsize=10)\n",
    "ax[0, 1].set_title('Duration of detections', fontsize=10)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "y_max = 0\n",
    "min_duration = 0.3  \n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # Real\n",
    "    n_real, _, _ = ax[i, 0].hist(durations_expert[i], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    ax[i, 0].legend(fontsize=8.5)\n",
    "    # Predicted\n",
    "    n_pred, _, _ = ax[i, 1].hist(durations[i], color='#c62828', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5)\n",
    "    max_y = max(np.max(n_real), np.max(n_pred))\n",
    "    ax[i, 0].plot([min_duration, min_duration], [0, max_y], '--', color='k', linewidth=2, alpha=0.6)\n",
    "    ax[i, 1].plot([min_duration, min_duration], [0, max_y], '--', color='k', linewidth=2, alpha=0.6)\n",
    "    y_max = max(max_y, y_max)\n",
    "    print(durations[i].max())\n",
    "    \n",
    "#for i in range(n_subjects):\n",
    "#    ax[i, 0].plot([min_duration, min_duration], [0, y_max], '--', color='k', linewidth=2, alpha=0.6)\n",
    "#    ax[i, 1].plot([min_duration, min_duration], [0, y_max], '--', color='k', linewidth=2, alpha=0.6)\n",
    "\n",
    "ax[-1, 0].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "ax[-1, 1].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Falses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == constants.MASSK_NAME:\n",
    "    min_separation = 0\n",
    "    min_duration = 0.3\n",
    "    max_duration = 4.0\n",
    "else:\n",
    "    min_separation = 0.3\n",
    "    min_duration = 0.2\n",
    "    max_duration = 4.0\n",
    "\n",
    "iou_array = []\n",
    "idx_array = []\n",
    "y_pred_thr = []\n",
    "for i in range(n_subjects):\n",
    "    print('Processing subject %d/%d' % (i+1, n_subjects), flush=True)\n",
    "    events = y_stamps[chosen_set_error][i]\n",
    "    detections = postprocessing.generate_mark_intervals(\n",
    "        y_pred[chosen_set_error][i], pages[chosen_set_error][i], 200//8, 200, thr=thr, \n",
    "        min_separation=min_separation, min_duration=min_duration, max_duration=max_duration)\n",
    "    print(events.shape, detections.shape)\n",
    "    this_iou_array, this_idx_array = metrics.matching(events, detections)\n",
    "    iou_array.append(this_iou_array)\n",
    "    idx_array.append(this_idx_array)\n",
    "    y_pred_thr.append(detections)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- False negatives\n",
    "iou_thr = 0.3\n",
    "\n",
    "fn_center = []\n",
    "for i in range(n_subjects):\n",
    "    idx_fn = iou_array[i] < iou_thr\n",
    "    fn_stamps = y_stamps[chosen_set_error][i][idx_fn]\n",
    "    this_fn_center = np.mean(fn_stamps, axis=1).astype(np.int32)\n",
    "    fn_center.append(this_fn_center)\n",
    "    \n",
    "# --- False positives\n",
    "fp_center = []\n",
    "for i in range(n_subjects):\n",
    "    # matched detections:\n",
    "    idx_fp_1 = iou_array[i] < iou_thr\n",
    "    idx_fp_1 = idx_array[i][idx_fp_1]\n",
    "    idx_fp_1 = [idx for idx in idx_fp_1 if idx != -1]\n",
    "    # Unmatched events\n",
    "    n_detections = y_pred_thr[i].shape[0]\n",
    "    idx_fp_2 = [idx for idx in range(n_detections) if (idx not in idx_array[i])]\n",
    "    idx_fp = idx_fp_1 + idx_fp_2\n",
    "    fp_stamps = y_pred_thr[i][idx_fp]\n",
    "    this_fp_center = np.mean(fp_stamps, axis=1).astype(np.int32)\n",
    "    fp_center.append(this_fp_center)\n",
    "\n",
    "    \n",
    "# --- Histogram of IoU values across real events\n",
    "fig, ax = plt.subplots(n_subjects, 1, figsize=(3.5, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0].set_title('IoU values on expert marks', fontsize=10)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i].hist(iou_array[i], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), bins=[0.05*i for i in range(21)])\n",
    "    ax[i].tick_params(labelsize=8.5)\n",
    "    ax[i].legend(fontsize=8.5, loc='upper center')\n",
    "    ax[i].set_xticks([0.2*i for i in range(6)])\n",
    "ax[-1].set_xlabel('Time within page [s]', fontsize=8.5)\n",
    "plt.show()\n",
    "    \n",
    "# --- Location in page\n",
    "fn_loc_page = [np.mod(this_fn_center, 20*fs_real) for this_fn_center in fn_center]\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(8, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0, 0].set_title('Location of expert marks with IoU < %1.2f' % (iou_thr), fontsize=11)\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i, 0].hist(fn_loc_page[i] / fs_real, bins=[i*4 for i in range(6)], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    ax[i, 0].legend(fontsize=8.5, loc='upper center')\n",
    "ax[-1, 0].set_xticks([i*4 for i in range(6)])\n",
    "ax[-1, 0].set_xlabel('Time within page [s]', fontsize=8.5)\n",
    "\n",
    "fp_loc_page = [np.mod(this_fp_center, 20*fs_real) for this_fp_center in fp_center]\n",
    "\n",
    "ax[0, 1].set_title('Location of detections with IoU < %1.2f' % (iou_thr), fontsize=10)\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i, 1].hist(fp_loc_page[i] / fs_real, bins=[i*4 for i in range(6)], color='#c62828', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5, loc='upper center')\n",
    "ax[-1, 1].set_xticks([i*4 for i in range(6)])\n",
    "ax[-1, 1].set_xlabel('Time within page [s]', fontsize=8.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Location in register\n",
    "\n",
    "fn_loc_register = [np.floor_divide(this_fn_center, 20*fs_real) for this_fn_center in fn_center]\n",
    "max_page_array = [np.max(pages[chosen_set_error][i]) for i in range(n_subjects)]\n",
    "\n",
    "max_of_all = np.max(max_page_array)\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(8, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0, 0].set_title('Location in register of expert marks with IoU < %1.1f' % (iou_thr), fontsize=10)\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i, 0].hist(fn_loc_register[i], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    ax[i, 0].legend(fontsize=8.5)\n",
    "ax[-1, 0].set_xlim([0, max_of_all+10])\n",
    "ax[-1, 0].set_xlabel('Page within register', fontsize=8.5)\n",
    "\n",
    "fp_loc_register = [np.floor_divide(this_fp_center, 20*fs_real) for this_fp_center in fp_center]\n",
    "max_page_array = [np.max(pages[chosen_set_error][i]) for i in range(n_subjects)]\n",
    "\n",
    "max_of_all = np.max(max_page_array)\n",
    "\n",
    "ax[0, 1].set_title('Location in register of detections with IoU < %1.1f' % (iou_thr), fontsize=11)\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i, 1].hist(fp_loc_register[i], color='#c62828', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5)\n",
    "ax[-1, 1].set_xlim([0, max_of_all+10])\n",
    "ax[-1, 1].set_xlabel('Page within register', fontsize=8.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ----- N2 pages\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 1, figsize=(3.5, 1*n_subjects), dpi=DPI, sharex=True, sharey=True)\n",
    "ax[0].set_title('Location in register of N2 pages (%s)' % (chosen_set_error.capitalize()), fontsize=10)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i].hist(pages[chosen_set_error][i], color='#455a64', label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()))\n",
    "    ax[i].tick_params(labelsize=8.5)\n",
    "    ax[i].legend(fontsize=8.5)\n",
    "ax[-1].set_xlim([0, max_of_all+10])\n",
    "ax[-1].set_xlabel('Page within register', fontsize=8.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU vs Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Scatter of IoU values and duration of real and detected events\n",
    "alpha = 0.2\n",
    "markersize=10\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(7, 1*n_subjects), dpi=DPI, sharex=True, sharey=True)\n",
    "ax[0, 0].set_title('IoU vs duration of expert marks', fontsize=10)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # Now compute durations\n",
    "    this_durations_expert = (y_stamps[chosen_set_error][i][:, 1] - y_stamps[chosen_set_error][i][:, 0]) / fs_real\n",
    "    ax[i, 0].scatter(this_durations_expert, iou_array[i], label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), color='#455a64', alpha=alpha, s=markersize)\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    lg = ax[i, 0].legend(fontsize=7, loc='upper right')\n",
    "    for lh in lg.legendHandles:\n",
    "        lh.set_facecolor(lh.get_facecolor())\n",
    "        lh.set_edgecolor(lh.get_edgecolor())\n",
    "        lh.set_alpha(1.0)\n",
    "    ax[i, 0].set_ylabel('IoU', fontsize=8.5)\n",
    "ax[-1, 0].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "\n",
    "ax[0, 1].set_title('IoU vs duration of detections', fontsize=10)\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # Now compute durations for IoU > 0 \n",
    "    idx_valid = (idx_array[i] > -1)\n",
    "    this_iou_1 = iou_array[i][idx_valid]\n",
    "    y_pred_stamps = y_pred_thr[i][idx_array[i]][idx_valid]\n",
    "    this_durations_1 = (y_pred_stamps[:, 1] - y_pred_stamps[:, 0]) / fs_real\n",
    "    # Now durations for IoU = 0\n",
    "    n_detections = y_pred_thr[i].shape[0]\n",
    "    idx_fp_2 = [idx for idx in range(n_detections) if (idx not in idx_array[i])]\n",
    "    y_pred_stamps = y_pred_thr[i][idx_fp_2]\n",
    "    this_durations_2 = (y_pred_stamps[:, 1] - y_pred_stamps[:, 0]) / fs_real\n",
    "    this_iou_2 = np.zeros(this_durations_2.shape[0])\n",
    "    # Concatenation\n",
    "    this_durations = np.concatenate([this_durations_1, this_durations_2])\n",
    "    this_iou = np.concatenate([this_iou_1, this_iou_2])\n",
    "    ax[i, 1].scatter(this_durations, this_iou, label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), color='#c62828', alpha=alpha, s=markersize)\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    lg = ax[i, 1].legend(fontsize=7, loc='upper right')\n",
    "    for lh in lg.legendHandles:\n",
    "        lh.set_facecolor(lh.get_facecolor())\n",
    "        lh.set_edgecolor(lh.get_edgecolor())\n",
    "        lh.set_alpha(1.0)\n",
    "    # ax[i, 1].set_ylabel('IoU', fontsize=8.5)\n",
    "ax[-1, 1].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Histogram of duration for IoU == 0\n",
    "fig, ax = plt.subplots(n_subjects, 2, figsize=(8, 1*n_subjects), dpi=DPI, sharex=True, sharey=False)\n",
    "ax[0, 0].set_title('Duration of unpaired expert marks', fontsize=10)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # Now compute durations\n",
    "    this_durations_expert = (y_stamps[chosen_set_error][i][:, 1] - y_stamps[chosen_set_error][i][:, 0]) / fs_real\n",
    "    idx_zero = (iou_array[i] == 0)\n",
    "    this_durations_expert_fn = this_durations_expert[idx_zero]\n",
    "    ax[i, 0].hist(this_durations_expert_fn, label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), color='#455a64')\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    lg = ax[i, 0].legend(fontsize=8.5, loc='upper right')\n",
    "    \n",
    "ax[-1, 0].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "\n",
    "ax[0, 1].set_title('Duration of unpaired detections', fontsize=10)\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n_detections = y_pred_thr[i].shape[0]\n",
    "    idx_fp_2 = [idx for idx in range(n_detections) if (idx not in idx_array[i])]\n",
    "    y_pred_stamps = y_pred_thr[i][idx_fp_2]\n",
    "    this_durations_fp = (y_pred_stamps[:, 1] - y_pred_stamps[:, 0]) / fs_real\n",
    "    ax[i, 1].hist(this_durations_fp, label='S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), color='#c62828')\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5, loc='upper right')\n",
    "ax[-1, 1].set_xlabel('Duration [s]', fontsize=8.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration of events and matched detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(5, 5), dpi=DPI, sharex=False, sharey=False)\n",
    "# plt.suptitle('Duration of matched events (%s)' % (chosen_set_error.capitalize()), fontsize=11)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    idx_valid = (idx_array[i] > -1)\n",
    "    # Now compute durations for real\n",
    "    this_durations_expert = (y_stamps[chosen_set_error][i][:, 1] - y_stamps[chosen_set_error][i][:, 0]) / fs_real\n",
    "    this_durations_expert = this_durations_expert[idx_valid]\n",
    "    # Now compute durations for matched detections\n",
    "    y_pred_stamps = y_pred_thr[i][idx_array[i]][idx_valid]\n",
    "    this_durations_det = (y_pred_stamps[:, 1] - y_pred_stamps[:, 0]) / fs_real\n",
    "    if i<2:\n",
    "        ax[0, i].scatter(this_durations_expert, this_durations_det, label='S%02d' % subject_idx, color='#455a64', alpha=alpha)\n",
    "        ax[0, i].tick_params(labelsize=8.5)\n",
    "        lg = ax[0, i].legend(fontsize=7)\n",
    "        for lh in lg.legendHandles:\n",
    "            lh.set_facecolor(lh.get_facecolor())\n",
    "            lh.set_edgecolor(lh.get_edgecolor())\n",
    "            lh.set_alpha(1.0)\n",
    "        max_dur = max(this_durations_expert.max(), this_durations_det.max())\n",
    "        ax[0, i].set_xlim([0, max_dur + 0.1])\n",
    "        ax[0, i].set_ylim([0, max_dur + 0.1])\n",
    "        if i == 0 :\n",
    "            ax[0, i].set_ylabel('Detected duration [s]', fontsize=8.5)\n",
    "        # ax[0, i].set_xlabel('Duration Real [s]', fontsize=8.5)\n",
    "        ax[0, i].plot([0.3, 2.0], [0.3, 2.0], '--k', alpha=0.6)\n",
    "    else:\n",
    "        ax[1, i-2].scatter(this_durations_expert, this_durations_det, label='S%02d' % subject_idx, color='#455a64', alpha=alpha)\n",
    "        ax[1, i-2].tick_params(labelsize=8.5)\n",
    "        lg = ax[1, i-2].legend(fontsize=7)\n",
    "        for lh in lg.legendHandles:\n",
    "            lh.set_facecolor(lh.get_facecolor())\n",
    "            lh.set_edgecolor(lh.get_edgecolor())\n",
    "            lh.set_alpha(1.0)\n",
    "        max_dur = max(this_durations_expert.max(), this_durations_det.max())\n",
    "        ax[1, i-2].set_xlim([0, max_dur + 0.1])\n",
    "        ax[1, i-2].set_ylim([0, max_dur + 0.1])\n",
    "        if i == 2:\n",
    "            ax[1, i-2].set_ylabel('Detected duration [s]', fontsize=8.5)\n",
    "        ax[1, i-2].set_xlabel('Real duration [s]', fontsize=8.5)\n",
    "        ax[1, i-2].plot([0.3, 2.0], [0.3, 2.0], '--k', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted probability to matched and unmatched events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare probabilities\n",
    "whole_y_proba = []\n",
    "for i in range(n_subjects):\n",
    "    this_proba = y_pred[chosen_set_error][i]\n",
    "    this_pages = pages[chosen_set_error][i]\n",
    "    page_size = this_proba.shape[1]\n",
    "    max_page = np.max(this_pages)\n",
    "    max_size = (max_page + 1) * page_size\n",
    "    whole_y_proba.append(np.zeros(max_size, dtype=np.float32))\n",
    "    for k, page in enumerate(this_pages):\n",
    "        sample_start = page * page_size\n",
    "        sample_end = (page + 1) * page_size\n",
    "        whole_y_proba[i][sample_start:sample_end] = this_proba[k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching probabilities (Candidates for TP according to IoU)\n",
    "# It is expected that they have more than 0.5 since that is the threshold used for detection\n",
    "print('Processing probability for matched events', flush=True)\n",
    "matching_proba = []\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # In this case, we look for the valid detections stamps \n",
    "    idx_valid = (idx_array[i] > -1)\n",
    "    y_pred_stamps = y_pred_thr[i][idx_array[i]][idx_valid]\n",
    "    # And now we need to get the probabilities inside those stamps\n",
    "    # First, we downsample the stamps\n",
    "    y_pred_stamps = y_pred_stamps // 8\n",
    "    # Now, we get the mean probability between stamps\n",
    "    mean_proba_list = []\n",
    "    for stamp in y_pred_stamps:\n",
    "        proba_inside = whole_y_proba[i][stamp[0]:(stamp[1]+1)]\n",
    "        mean_proba_inside = proba_inside.mean()\n",
    "        mean_proba_list.append(mean_proba_inside)\n",
    "    mean_proba_list = np.array(mean_proba_list)\n",
    "    print('Mean probability for S%02d: %1.4f' % (subject_idx, mean_proba_list.mean()))\n",
    "    matching_proba.append(mean_proba_list)\n",
    "    \n",
    "# Unmatched real events (FN)\n",
    "# It is expected that they have less than 0.5 since they were missed by the tresholding\n",
    "print('Processing probability for unmatched real events', flush=True)\n",
    "unmatching_fn_proba = []\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # In this case, we look for the missed real events \n",
    "    idx_valid = (idx_array[i] == -1)\n",
    "    fn_stamps = y_stamps[chosen_set_error][i][idx_valid]\n",
    "    # And now we need to get the probabilities inside those stamps\n",
    "    # First, we downsample the stamps\n",
    "    fn_stamps = fn_stamps // 8\n",
    "    # Now, we get the mean probability between stamps\n",
    "    mean_proba_list = []\n",
    "    for stamp in fn_stamps:\n",
    "        proba_inside = whole_y_proba[i][stamp[0]:(stamp[1]+1)]\n",
    "        mean_proba_inside = proba_inside.mean()\n",
    "        mean_proba_list.append(mean_proba_inside)\n",
    "    mean_proba_list = np.array(mean_proba_list)\n",
    "    print('Mean probability for S%02d: %1.4f' % (subject_idx, mean_proba_list.mean()))\n",
    "    unmatching_fn_proba.append(mean_proba_list)\n",
    "    \n",
    "# Unmatched detected events (FP)\n",
    "# It is expected that they have more than 0.5 since they were detected\n",
    "print('Processing probability for unmatched detected events', flush=True)\n",
    "unmatching_fp_proba = []\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # In this case, we look for detected events with no match \n",
    "    n_detections = y_pred_thr[i].shape[0]\n",
    "    idx_valid = [idx for idx in range(n_detections) if (idx not in idx_array[i])]\n",
    "    fp_stamps = y_pred_thr[i][idx_valid]\n",
    "    # And now we need to get the probabilities inside those stamps\n",
    "    # First, we downsample the stamps\n",
    "    fp_stamps = fp_stamps // 8\n",
    "    # Now, we get the mean probability between stamps\n",
    "    mean_proba_list = []\n",
    "    for stamp in fp_stamps:\n",
    "        proba_inside = whole_y_proba[i][stamp[0]:(stamp[1]+1)]\n",
    "        mean_proba_inside = proba_inside.mean()\n",
    "        mean_proba_list.append(mean_proba_inside)\n",
    "    mean_proba_list = np.array(mean_proba_list)\n",
    "    print('Mean probability for S%02d: %1.4f' % (subject_idx, mean_proba_list.mean()))\n",
    "    unmatching_fp_proba.append(mean_proba_list)\n",
    "    \n",
    "\n",
    "# Probability for real events\n",
    "print('Processing probability for real events', flush=True)\n",
    "real_proba = []\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    # And now we need to get the probabilities inside those stamps\n",
    "    # First, we downsample the stamps\n",
    "    y_pred_stamps = y_stamps[chosen_set_error][i] // 8\n",
    "    # Now, we get the mean probability between stamps\n",
    "    mean_proba_list = []\n",
    "    for stamp in y_pred_stamps:\n",
    "        proba_inside = whole_y_proba[i][stamp[0]:(stamp[1]+1)]\n",
    "        mean_proba_inside = proba_inside.mean()\n",
    "        mean_proba_list.append(mean_proba_inside)\n",
    "    mean_proba_list = np.array(mean_proba_list)\n",
    "    print('Mean probability for S%02d: %1.4f' % (subject_idx, mean_proba_list.mean()))\n",
    "    real_proba.append(mean_proba_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Histogram of probabilities\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 3, figsize=(14, 3*n_subjects), dpi=100, sharex=True)\n",
    "\n",
    "ax[0, 0].set_title('Model output on matched events (%s)' % (chosen_set_error.capitalize()), fontsize=11)\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n, _, _ = ax[i, 0].hist(matching_proba[i], label='S%02d' % subject_idx, color='#43a047')\n",
    "    max_n = np.max(n)\n",
    "    ax[i, 0].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "    ax[i, 0].tick_params(labelsize=8.5)\n",
    "    ax[i, 0].legend(fontsize=8.5)\n",
    "ax[-1, 0].set_xlim([0, 1])\n",
    "ax[-1, 0].set_xlabel('Probability', fontsize=8.5)\n",
    "\n",
    "ax[0, 1].set_title('Probability on unmatched real events (%s)' % (chosen_set_error.capitalize()), fontsize=11)\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n, _, _ = ax[i, 1].hist(unmatching_fn_proba[i], label='S%02d' % subject_idx, color='#455a64')\n",
    "    max_n = np.max(n)\n",
    "    ax[i, 1].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "    ax[i, 1].tick_params(labelsize=8.5)\n",
    "    ax[i, 1].legend(fontsize=8.5)\n",
    "ax[-1, 1].set_xlim([0, 1])\n",
    "ax[-1, 1].set_xlabel('Probability', fontsize=8.5)\n",
    "\n",
    "ax[0, 2].set_title('Probability on unmatched detections (%s)' % (chosen_set_error.capitalize()), fontsize=11)\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n, _, _ = ax[i, 2].hist(unmatching_fp_proba[i], label='S%02d' % subject_idx, color='#c62828')\n",
    "    max_n = np.max(n)\n",
    "    ax[i, 2].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "    ax[i, 2].tick_params(labelsize=8.5)\n",
    "    ax[i, 2].legend(fontsize=8.5)\n",
    "ax[-1, 2].set_xlim([0, 1])\n",
    "ax[-1, 2].set_xlabel('Probability', fontsize=8.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Histogram of probabilities\n",
    "\n",
    "n_points = 100\n",
    "x_points =np.arange(n_points + 1) / n_points\n",
    "\n",
    "fig, ax = plt.subplots(n_subjects, 1, figsize=(6, 5*n_subjects), dpi=100, sharex=True)\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    ax[i].set_title('Predicted probability for S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), fontsize=11)\n",
    "\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "max_n = 0\n",
    "for i in range(n_subjects):\n",
    "    \n",
    "    n, _, _ = ax[i].hist(\n",
    "        matching_proba[i], label='Matched real', color='#43a047', alpha=0.4, density=True,\n",
    "        bins = [i*0.05 for i in range(21)])\n",
    "    kernel = gaussian_kde(matching_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    ax[i].plot(x_points, y_kde, color='#43a047', linewidth=2)\n",
    "    max_n = max(np.max(n), max_n)\n",
    "    ax[i].tick_params(labelsize=8.5)\n",
    "ax[-1].set_xlim([0, 1])\n",
    "ax[-1].set_xlabel('Probability', fontsize=8.5)\n",
    "\n",
    "# for i in range(n_subjects):\n",
    "#     subject_idx = idx_dict[chosen_set_error][i]\n",
    "#     n, _, _ = ax[i].hist(\n",
    "#         unmatching_fn_proba[i], label='Unmatched real', color='#455a64', alpha=0.4, density=True,\n",
    "#         bins = [i*0.05 for i in range(21)])\n",
    "#     kernel = gaussian_kde(unmatching_fn_proba[i])\n",
    "#     y_kde = kernel(x_points)\n",
    "#     ax[i].plot(x_points, y_kde, color='#455a64', linewidth=2)\n",
    "#     max_n = max(np.max(n), max_n)\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n, _, _ = ax[i].hist(\n",
    "        unmatching_fp_proba[i], label='Unmatched detection', color='#c62828', alpha=0.4, density=True,\n",
    "        bins = [i*0.05 for i in range(21)])\n",
    "    kernel = gaussian_kde(unmatching_fp_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    ax[i].plot(x_points, y_kde, color='#c62828', linewidth=2)\n",
    "    max_n = max(np.max(n), max_n)\n",
    "    ax[i].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "    ax[i].legend(fontsize=8.5, loc='upper left')\n",
    "    ax[i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Histogram of probabilities\n",
    "\n",
    "n_points = 100\n",
    "x_points =np.arange(n_points + 1) / n_points\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 5), dpi=DPI, sharex=True)\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    if i < 2:\n",
    "        ax[0, i].set_title('Model Output for S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), fontsize=10)\n",
    "    else:\n",
    "        ax[1, i-2].set_title('Model Output for S%02d (%s)' % (subject_idx, chosen_set_error.capitalize()), fontsize=10)\n",
    "\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "max_n = 0\n",
    "for i in range(n_subjects):\n",
    "    if i < 2:\n",
    "        n, _, _ = ax[0, i].hist(\n",
    "            real_proba[i], label='Expert marks', color='#43a047', alpha=0.4, density=True,\n",
    "            bins = [k*0.05 for k in range(21)])\n",
    "    else:\n",
    "        n, _, _ = ax[1, i-2].hist(\n",
    "            real_proba[i], label='Real events\\n(Expert marks)', color='#43a047', alpha=0.4, density=True,\n",
    "            bins = [k*0.05 for k in range(21)])\n",
    "    kernel = gaussian_kde(real_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    if i<2:\n",
    "        ax[0, i].plot(x_points, y_kde, color='#43a047', linewidth=2)\n",
    "        ax[0, i].tick_params(labelsize=8.5)\n",
    "        ax[0, i].set_xlim([0, 1])\n",
    "    else:\n",
    "        ax[1, i-2].plot(x_points, y_kde, color='#43a047', linewidth=2)\n",
    "        ax[1, i-2].tick_params(labelsize=8.5)\n",
    "        ax[1, i-2].set_xlim([0, 1])\n",
    "    max_n = max(np.max(n), max_n)\n",
    "    \n",
    "\n",
    "# ax[-1].set_xlabel('Probability', fontsize=8.5)\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    kernel = gaussian_kde(unmatching_fp_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    max_n = max(np.max(n), max_n)\n",
    "    if i<2:\n",
    "        n, _, _ = ax[0, i].hist(\n",
    "            unmatching_fp_proba[i], label='Unpaired detections ($\\mu$=0.1)', color='#c62828', alpha=0.4, density=True,\n",
    "            bins = [k*0.05 for k in range(21)])\n",
    "        ax[0, i].plot(x_points, y_kde, color='#c62828', linewidth=2)\n",
    "        # ax[i].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "        # lg = ax[0, i].legend(fontsize=8.5, loc='upper center', bbox_to_anchor=(1.05, 0.15))\n",
    "        # for lh in lg.legendHandles:\n",
    "        #     lh.set_facecolor(lh.get_facecolor())\n",
    "        #     lh.set_alpha(1.0)\n",
    "        ax[0, i].set_yticks([])\n",
    "        ax[0, i].tick_params(labelsize=8.5)\n",
    "    else:\n",
    "        n, _, _ = ax[1, i-2].hist(\n",
    "            unmatching_fp_proba[i], label='False events\\n(Unpaired detections\\ngenerated with $\\mu$=0.1)', color='#c62828', alpha=0.4, density=True,\n",
    "            bins = [k*0.05 for k in range(21)])\n",
    "        ax[1, i-2].plot(x_points, y_kde, color='#c62828', linewidth=2)\n",
    "        # ax[i].plot([thr, thr], [0, max_n], '--', color='k', linewidth=1.5, alpha=0.6, label='thr')\n",
    "        ax[1, i-2].set_yticks([])\n",
    "        ax[1, i-2].set_xlabel('Class assignment', fontsize=8.5)\n",
    "        ax[1, i-2].tick_params(labelsize=8.5)\n",
    "lg = ax[1, 1].legend(fontsize=9, loc='upper left', bbox_to_anchor=(1.05, 1.35), labelspacing=3)\n",
    "for lh in lg.legendHandles:\n",
    "    lh.set_facecolor(lh.get_facecolor())\n",
    "    lh.set_alpha(1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Avg distributions over set\n",
    "\n",
    "\n",
    "# --- Histogram of probabilities\n",
    "\n",
    "n_points = 100\n",
    "x_points =np.arange(n_points + 1) / n_points\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5), dpi=100, sharex=True)\n",
    "\n",
    "ax.set_title('Predicted probability. Average for %s set' % (chosen_set_error.capitalize()), fontsize=11)\n",
    "\n",
    "y_kde_list = []\n",
    "for i in range(n_subjects):\n",
    "    kernel = gaussian_kde(matching_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    y_kde_list.append(y_kde)\n",
    "y_kde_avg_real = np.stack(y_kde_list, axis=1).mean(axis=1)\n",
    "ax.plot(x_points, y_kde_avg_real, color='#43a047', linewidth=2, label='Real event')\n",
    "ax.fill_between(x_points, y_kde_avg_real, 0*y_kde_avg_real, color='#43a047', alpha=0.4)\n",
    "\n",
    "y_kde_list = []\n",
    "for i in range(n_subjects):\n",
    "    kernel = gaussian_kde(unmatching_fp_proba[i])\n",
    "    y_kde = kernel(x_points)\n",
    "    y_kde_list.append(y_kde)\n",
    "y_kde_avg_false = np.stack(y_kde_list, axis=1).mean(axis=1)\n",
    "ax.plot(x_points, y_kde_avg_false, color='#c62828', linewidth=2, label='False event')\n",
    "ax.fill_between(x_points, y_kde_avg_false, 0*y_kde_avg_false, color='#c62828', alpha=0.4)\n",
    "\n",
    "max_y = max(np.max(y_kde_avg_real), np.max(y_kde_avg_false))\n",
    "\n",
    "# Find optimal threshold\n",
    "difference = y_kde_avg_false - y_kde_avg_real\n",
    "idx_thr = np.where(np.signbit(difference))[0][0]\n",
    "x_thr = x_points[idx_thr]\n",
    "print('Optimal Threshold: %1.4f' % x_thr)\n",
    "ax.plot([x_thr, x_thr], [0, max_y], '--', color='k', linewidth=1.5, alpha=0.6, label='Optimal Threshold')\n",
    "\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, max_y])\n",
    "ax.set_yticks([])\n",
    "ax.legend(fontsize=8.5, loc='upper left')\n",
    "ax.set_xlabel('Probability', fontsize=8.5)\n",
    "ax.set_xticks([i*0.1 for i in range(11)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of FP that are TP-E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unmatched events according to E1\n",
    "unmatched_stamps = []\n",
    "\n",
    "idx_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "for i in range(n_subjects):\n",
    "    # Unmatched detections\n",
    "    subject_idx = idx_dict[chosen_set_error][i]\n",
    "    n_detections = y_pred_thr[i].shape[0]\n",
    "    idx_fp = [idx for idx in range(n_detections) if (idx not in idx_array[i])]\n",
    "    fp_stamps = y_pred_thr[i][idx_fp]\n",
    "    print('\\n%d / %d (%1.2f %%) unmatched detections with E1 for S%02d' % (fp_stamps.shape[0], n_detections, 100*fp_stamps.shape[0]/n_detections, subject_idx))\n",
    "    unmatched_stamps.append(fp_stamps)\n",
    "    # Now match with E2\n",
    "    events = y2_stamps[chosen_set_error][i]\n",
    "    detections = fp_stamps\n",
    "    n_detections = fp_stamps.shape[0]\n",
    "    this_iou_array, this_idx_array = metrics.matching(events, detections)\n",
    "    matched_2 =  (this_idx_array > -1).sum()\n",
    "    print('%d were matched with E2 (%1.2f%% of previously unmatched detections)' % (matched_2, 100*matched_2 / fp_stamps.shape[0]))\n",
    "    # print(fp_stamps[this_idx_array[this_idx_array > -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral information of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_list = dataset.train_ids\n",
    "whole_night = True\n",
    "\n",
    "# Now plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=150)\n",
    "if whole_night:\n",
    "    title_str = 'Whole night average'\n",
    "else:\n",
    "    title_str = 'N2 only average'\n",
    "    \n",
    "for subject_id in subject_id_list:\n",
    "    x, _ = dataset.get_subject_data(subject_id, which_expert=1, verbose=False, whole_night=whole_night)\n",
    "    # Compute fft of each page \n",
    "    print('Computing FFT for S%02d... ' % subject_id, end='', flush=True)\n",
    "    fft_list = []\n",
    "    for x_page in x:\n",
    "        fft_page, freq_axis = data_ops.power_spectrum(x_page, 200)\n",
    "        fft_list.append(fft_page)\n",
    "    # Now average the fft:\n",
    "    fft_mean = np.stack(fft_list, axis=1).mean(axis=1)\n",
    "    print('Done')\n",
    "    # Now plot\n",
    "    ax.plot(freq_axis, fft_mean, label='S%02d' % subject_id, linewidth=1)\n",
    "\n",
    "ax.set_title(title_str, fontsize=10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Frequency [Hz]', fontsize=8.5)\n",
    "ax.set_ylabel('Power')\n",
    "ax.set_xlim([0, 25])\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.legend(loc='upper right', fontsize=8.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load invalid subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
