{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import freqz\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.data.cap_full_ss import ALL_IDS\n",
    "from sleeprnn.detection import metrics\n",
    "\n",
    "viz.notebook_full_width()\n",
    "%matplotlib inline\n",
    "\n",
    "DATASET_DIR = os.path.abspath(os.path.join(utils.PATH_DATA, 'unlabeled_cap_npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader to read on-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(i):\n",
    "    return ALL_IDS[i]\n",
    "\n",
    "\n",
    "class CapFullLoader(object):\n",
    "    def __init__(self, fs=200, min_ss_duration=0.3, max_ss_duration=3.0, page_duration=20, dataset_dir=DATASET_DIR):\n",
    "        self.fs = fs\n",
    "        self.min_ss_duration = min_ss_duration\n",
    "        self.max_ss_duration = max_ss_duration\n",
    "        self.page_duration = page_duration\n",
    "        # Parameters of dataset\n",
    "        self.original_page_duration = 30\n",
    "        self.n2_id = 'S2'\n",
    "        self.dataset_dir = dataset_dir\n",
    "        \n",
    "    def read_subject_data(self, subject_id):\n",
    "        file_npz, file_spindle_1, file_spindle_2 = self.get_file_paths(subject_id)\n",
    "        signal, hypnogram_original, meta = self.read_npz(file_npz)\n",
    "        n2_pages = self.get_n2_pages(hypnogram_original)\n",
    "        all_pages = self.get_all_pages(signal.size)\n",
    "        marks_1 = self.read_marks(file_spindle_1)\n",
    "        marks_2 = self.read_marks(file_spindle_2)\n",
    "        # n2 filtering in 20s pages\n",
    "        page_size = int(self.fs * self.page_duration)\n",
    "        marks_1_n2 = utils.extract_pages_for_stamps(marks_1, n2_pages, page_size)\n",
    "        marks_2_n2 = utils.extract_pages_for_stamps(marks_2, n2_pages, page_size)\n",
    "        \n",
    "        subject_data_dict = {\n",
    "            'signal': signal,\n",
    "            'hypnogram_original': hypnogram_original,\n",
    "            'n2_pages': n2_pages,\n",
    "            'wn_pages': all_pages,\n",
    "            'spindle1_all': marks_1,\n",
    "            'spindle2_all': marks_2,\n",
    "            'spindle1_n2': marks_1_n2,\n",
    "            'spindle2_n2': marks_2_n2,\n",
    "            'metadata': meta\n",
    "        }\n",
    "        return subject_data_dict\n",
    "    \n",
    "    def get_file_paths(self, subject_id):\n",
    "        file_npz = os.path.join(self.dataset_dir, 'register_and_state', \"cap_%s.npz\" % subject_id)\n",
    "        file_spindle_1 = os.path.join(\n",
    "            self.dataset_dir, 'spindle',\n",
    "            'spindle_1',\n",
    "            'EventDetection_s%s_absSigPow(1.75)_relSigPow(1.6)_sigCov(1.8)_sigCorr(0.75).txt' % subject_id)\n",
    "        file_spindle_2 = os.path.join(\n",
    "            self.dataset_dir, 'spindle',\n",
    "            'spindle_2',\n",
    "            'EventDetection_s%s_absSigPow(1.25)_relSigPow(1.6)_sigCov(1.3)_sigCorr(0.69).txt' % subject_id)\n",
    "        return file_npz, file_spindle_1, file_spindle_2\n",
    "    \n",
    "    def read_npz(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        hypnogram = data['hypnogram']\n",
    "        signal = data['signal']\n",
    "        # Signal is already filtered to 0.1-35 and sampled to 200Hz\n",
    "        original_fs = data['sampling_rate']\n",
    "        # Now resample to the required frequency\n",
    "        if self.fs != original_fs:\n",
    "            print('Resampling from %d Hz to required %d Hz' % (original_fs, self.fs))\n",
    "            signal = utils.resample_signal(signal, fs_old=original_fs, fs_new=self.fs)\n",
    "        signal = signal.astype(np.float32)\n",
    "        meta = {}\n",
    "        for key in data.files:\n",
    "            if key not in ['signal', 'hypnogram']:\n",
    "                meta[key] = data[key].item()\n",
    "        return signal, hypnogram, meta\n",
    "    \n",
    "    def get_n2_pages(self, hypnogram_original):\n",
    "        signal_total_duration = len(hypnogram_original) * self.original_page_duration\n",
    "        # Extract N2 pages\n",
    "        n2_pages_original = np.where(hypnogram_original == self.n2_id)[0]\n",
    "        onsets_original = n2_pages_original * self.original_page_duration\n",
    "        offsets_original = (n2_pages_original + 1) * self.original_page_duration\n",
    "        total_pages = int(np.ceil(signal_total_duration / self.page_duration))\n",
    "        n2_pages_onehot = np.zeros(total_pages, dtype=np.int16)\n",
    "        for i in range(total_pages):\n",
    "            onset_new_page = i * self.page_duration\n",
    "            offset_new_page = (i + 1) * self.page_duration\n",
    "            for j in range(n2_pages_original.size):\n",
    "                intersection = (onset_new_page < offsets_original[j]) and (onsets_original[j] < offset_new_page)\n",
    "                if intersection:\n",
    "                    n2_pages_onehot[i] = 1\n",
    "                    break\n",
    "        n2_pages = np.where(n2_pages_onehot == 1)[0]\n",
    "        # Drop first, last and second to last page of the whole registers\n",
    "        # if they where selected.\n",
    "        last_page = total_pages - 1\n",
    "        n2_pages = n2_pages[\n",
    "            (n2_pages != 0)\n",
    "            & (n2_pages != last_page)\n",
    "            & (n2_pages != last_page - 1)]\n",
    "        n2_pages = n2_pages.astype(np.int16)\n",
    "        return n2_pages\n",
    "    \n",
    "    def get_all_pages(self, signal_length):\n",
    "        total_pages = int(np.ceil(signal_length / (self.fs * self.page_duration)))\n",
    "        all_pages = np.arange(1, total_pages - 2, dtype=np.int16)\n",
    "        return all_pages\n",
    "    \n",
    "    def read_marks(self, file_path):\n",
    "        pred_data = pd.read_csv(file_path, sep='\\t')\n",
    "        # We substract 1 to translate from matlab to numpy indexing system\n",
    "        start_samples = pred_data.start_sample.values - 1\n",
    "        end_samples = pred_data.end_sample.values - 1\n",
    "        marks = np.stack([start_samples, end_samples], axis=1).astype(np.int32)\n",
    "        # Sample-stamps assume 200Hz sampling rate\n",
    "        if self.fs != 200:\n",
    "            print('Correcting marks from 200 Hz to %d Hz' % self.fs)\n",
    "            # We need to transform the marks to the new sampling rate\n",
    "            marks_time = marks.astype(np.float32) / 200.0\n",
    "            # Transform to sample-stamps\n",
    "            marks = np.round(marks_time * self.fs).astype(np.int32)\n",
    "        # Combine marks that are too close according to standards\n",
    "        marks = stamp_correction.combine_close_stamps(marks, self.fs, self.min_ss_duration)\n",
    "        # Fix durations that are outside standards\n",
    "        marks = stamp_correction.filter_duration_stamps(marks, self.fs, self.min_ss_duration, self.max_ss_duration)\n",
    "        return marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CapFullLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = get_id(6)\n",
    "print(\"Subject id %s\" % subject_id)\n",
    "data = loader.read_subject_data(subject_id)\n",
    "print(\"Available keys:\", list(data.keys()))\n",
    "\n",
    "print(\"\")\n",
    "for key in data.keys():\n",
    "    if 'metadata' not in key:\n",
    "        print(key, data[key].shape, data[key].dtype)\n",
    "print(\"Metadata:\")\n",
    "pprint(data['metadata'])\n",
    "\n",
    "print(\"\")\n",
    "spindle1_n2_original = utils.extract_pages_for_stamps(\n",
    "    data['spindle1_all'], \n",
    "    np.where(data['hypnogram_original'] == loader.n2_id)[0], \n",
    "    int(loader.original_page_duration * loader.fs)\n",
    ")\n",
    "print(\"spindle 1 in N2 original:\", spindle1_n2_original.shape)\n",
    "\n",
    "spindle2_n2_original = utils.extract_pages_for_stamps(\n",
    "    data['spindle2_all'], \n",
    "    np.where(data['hypnogram_original'] == loader.n2_id)[0], \n",
    "    int(loader.original_page_duration * loader.fs)\n",
    ")\n",
    "print(\"spindle 2 in N2 original:\", spindle2_n2_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subject %s\" % subject_id)\n",
    "\n",
    "print(\"Signal total duration: %1.2f [s] (%1.2f [h])\" % (\n",
    "    data['signal'].size / loader.fs,\n",
    "    data['signal'].size / loader.fs / 3600,\n",
    "))\n",
    "print(\"Total REM sleep (original): %1.2f [h]\" % (\n",
    "    np.sum(\n",
    "        data['hypnogram_original'] == 'R'\n",
    "    ) * loader.original_page_duration / 3600\n",
    "))\n",
    "print(\"Total NREM sleep (original): %1.2f [h]\" % (\n",
    "    np.sum(\n",
    "        (data['hypnogram_original'] == 'S1')\n",
    "        | (data['hypnogram_original'] == 'S2')\n",
    "        | (data['hypnogram_original'] == 'S3')\n",
    "        | (data['hypnogram_original'] == 'S4')\n",
    "    ) * loader.original_page_duration / 3600\n",
    "))\n",
    "print(\"Total N2 sleep (original): %1.2f [h]\" % (\n",
    "    np.sum(\n",
    "        (data['hypnogram_original'] == 'S2')\n",
    "    ) * loader.original_page_duration / 3600\n",
    "))\n",
    "print(\"Fraction N2 / Sleep (original): %1.2f %%\" % (\n",
    "    100 * np.sum(data['hypnogram_original'] == 'S2') / np.sum(\n",
    "        (data['hypnogram_original'] == 'S1')\n",
    "        | (data['hypnogram_original'] == 'S2')\n",
    "        | (data['hypnogram_original'] == 'S3')\n",
    "        | (data['hypnogram_original'] == 'S4')\n",
    "        | (data['hypnogram_original'] == 'R')\n",
    "    )\n",
    "))\n",
    "print(\"N spindle 1 in N2: %d\" % data['spindle1_n2'].shape[0])\n",
    "print(\"N spindle 2 in N2: %d\" % data['spindle2_n2'].shape[0])\n",
    "duration_1 = (data['spindle1_n2'][:, 1] - data['spindle1_n2'][:, 0] + 1) / loader.fs\n",
    "duration_2 = (data['spindle2_n2'][:, 1] - data['spindle2_n2'][:, 0] + 1) / loader.fs\n",
    "print(\"Min-Mean-Max duration spindle 1 in N2: %1.6fs - %1.6fs - %1.6fs\" % (\n",
    "    np.min(duration_1), np.mean(duration_1), np.max(duration_1)\n",
    "))\n",
    "print(\"Min-Mean-Max duration spindle 2 in N2: %1.6fs - %1.6fs - %1.6fs\" % (\n",
    "    np.min(duration_2), np.mean(duration_2), np.max(duration_2)\n",
    "))\n",
    "print(\"Density of spindle 1 (in N2 original): %1.4f spm\" % (\n",
    "    spindle1_n2_original.shape[0] / (\n",
    "        np.sum((data['hypnogram_original'] == 'S2')) * loader.original_page_duration / 60\n",
    "    )\n",
    "))\n",
    "print(\"Density of spindle 2 (in N2 original): %1.4f spm\" % (\n",
    "    spindle2_n2_original.shape[0] / (\n",
    "        np.sum((data['hypnogram_original'] == 'S2')) * loader.original_page_duration / 60\n",
    "    )\n",
    "))\n",
    "\n",
    "print(\"Range of signal min-mean-max and std: %1.4f, %1.4f, %1.4f, %1.4f\" % (\n",
    "    data['signal'].min(),\n",
    "    data['signal'].mean(),\n",
    "    data['signal'].max(),\n",
    "    data['signal'].std(),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N2 spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrum\n",
    "segmented_signal = utils.extract_pages(data['signal'], data['n2_pages'], int(loader.fs * loader.page_duration))\n",
    "print(\"Segmented signal\", segmented_signal.shape)\n",
    "all_pages_power = []\n",
    "for page_signal in segmented_signal:\n",
    "    power, freq = utils.power_spectrum(page_signal, loader.fs, apply_hanning=True)\n",
    "    all_pages_power.append(power)\n",
    "\n",
    "pages_power = np.stack(all_pages_power, axis=0)\n",
    "min_freq = 0.5\n",
    "max_freq = 30\n",
    "plot_indices = np.where((freq >= min_freq) & (freq <= max_freq))[0]\n",
    "plot_pages_power = pages_power[:, plot_indices]\n",
    "plot_freq = freq[plot_indices]\n",
    "plot_power_min = np.min(plot_pages_power, axis=0)\n",
    "plot_power_max = np.min(plot_pages_power, axis=0)\n",
    "plot_power_mean = np.mean(plot_pages_power, axis=0)\n",
    "plot_power_25, plot_power_50, plot_power_75 = np.percentile(plot_pages_power, (25, 50, 75), axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), dpi=120)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(plot_freq, plot_power_50, color=viz.PALETTE['blue'], linewidth=1, label='median')\n",
    "ax.fill_between(plot_freq, plot_power_25, plot_power_75, facecolor=viz.PALETTE['blue'], alpha=0.3, label='iqr')\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.set_yticks([])\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('Subject %s, N2, channel %s' % (subject_id, data['metadata']['channel']))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(plot_freq, np.log10(plot_power_50), color=viz.PALETTE['blue'], linewidth=1, label='median')\n",
    "ax.fill_between(plot_freq, np.log10(plot_power_25), np.log10(plot_power_75), facecolor=viz.PALETTE['blue'], alpha=0.3, label='iqr')\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.set_yticks([])\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_loc = np.random.RandomState(seed=0).choice(data['spindle1_n2'].mean(axis=1).astype(np.int32))\n",
    "window_size = 15\n",
    "start_sample = int(center_loc - window_size * loader.fs // 2)\n",
    "end_sample = int(start_sample + window_size * loader.fs)\n",
    "\n",
    "time_axis = np.arange(start_sample, end_sample) / loader.fs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 3), dpi=120)\n",
    "ax.plot(time_axis, data['signal'][start_sample:end_sample], linewidth=0.7)\n",
    "ax.set_xlim([start_sample/loader.fs, end_sample/loader.fs])\n",
    "ax.set_ylim([-100, 100])\n",
    "\n",
    "spindle1 = utils.filter_stamps(data['spindle1_n2'], start_sample, end_sample)\n",
    "spindle2 = utils.filter_stamps(data['spindle2_n2'], start_sample, end_sample)\n",
    "for i, mark in enumerate(spindle1):\n",
    "    label = 'Spindle 1' if i == 0 else None\n",
    "    ax.plot(mark / loader.fs, [-50, -50], color=viz.PALETTE['red'], linewidth=4, alpha=0.4, label=label)\n",
    "for i, mark in enumerate(spindle2):\n",
    "    label = 'Spindle 2' if i == 0 else None\n",
    "    ax.plot(mark / loader.fs, [-60, -60], color=viz.PALETTE['blue'], linewidth=4, alpha=0.4, label=label)\n",
    "ax.set_title(\"Subject %s, channel %s\" % (subject_id, data['metadata']['channel']))\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement between spindle1 and spindle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_matching, idx_matching = metrics.matching(spindle1_n2_original, spindle2_n2_original)\n",
    "iou_thr_list = np.linspace(0, 1, 21)[1:-2]\n",
    "recall_vs_iou = metrics.metric_vs_iou(spindle1_n2_original, spindle2_n2_original, iou_thr_list, metric_name=constants.RECALL, iou_matching=iou_matching)\n",
    "precision_vs_iou = metrics.metric_vs_iou(spindle1_n2_original, spindle2_n2_original, iou_thr_list, metric_name=constants.PRECISION, iou_matching=iou_matching)\n",
    "f1score_vs_iou = metrics.metric_vs_iou(spindle1_n2_original, spindle2_n2_original, iou_thr_list, metric_name=constants.F1_SCORE, iou_matching=iou_matching)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), dpi=120)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(iou_thr_list, f1score_vs_iou, linewidth=1, color=viz.PALETTE['blue'], marker='o', markersize=3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel(\"IoU threshold\")\n",
    "ax.set_ylabel(\"F1-score\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid()\n",
    "ax.set_title(\"Subject %s, spindles agreement\" % subject_id)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(recall_vs_iou, precision_vs_iou, linewidth=1, color=viz.PALETTE['blue'], marker='o', markersize=3)\n",
    "iou_02_loc = np.argmin((iou_thr_list - 0.2)**2)\n",
    "ax.plot(recall_vs_iou[iou_02_loc], precision_vs_iou[iou_02_loc], color=viz.PALETTE['red'], marker='o', markersize=5, label=\"IoU > 0.2\")\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid()\n",
    "ax.set_title(\"Spindles-2 as detections\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax = axes[2]\n",
    "iou_of_matchings = iou_matching[idx_matching != -1]\n",
    "bins = np.linspace(0, 1, 11)\n",
    "ax.hist(iou_of_matchings, bins=bins, color=viz.PALETTE['blue'])\n",
    "mean_iou_of_matchings = np.mean(iou_of_matchings)\n",
    "ax.axvline(mean_iou_of_matchings, color='k', linestyle='--', linewidth=2)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, None])\n",
    "ax.set_title(\"mIoU %1.2f %%\" % (100 * mean_iou_of_matchings))\n",
    "ax.set_xlabel(\"IoU of matchings\")\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAP subject's N2 mean spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrum_dict = {}\n",
    "min_freq = 0.5\n",
    "max_freq = 30\n",
    "for subject_id in ALL_IDS:\n",
    "    data = loader.read_subject_data(subject_id)\n",
    "    # spectrum\n",
    "    segmented_signal = utils.extract_pages(data['signal'], data['n2_pages'], int(loader.fs * loader.page_duration))\n",
    "    all_pages_power = []\n",
    "    for page_signal in segmented_signal:\n",
    "        power, freq = utils.power_spectrum(page_signal, loader.fs, apply_hanning=True)\n",
    "        all_pages_power.append(power)\n",
    "    pages_power = np.stack(all_pages_power, axis=0)\n",
    "    plot_indices = np.where((freq >= min_freq) & (freq <= max_freq))[0]\n",
    "    plot_pages_power = pages_power[:, plot_indices]\n",
    "    plot_freq = freq[plot_indices]\n",
    "    plot_power_mean = np.mean(plot_pages_power, axis=0)\n",
    "    spectrum_dict[subject_id] = np.stack([plot_freq, plot_power_mean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_fft(emph_subject=None, sample_from='spindle1_n2', save_plots=False, collapse_background=True):    \n",
    "    if save_plots:\n",
    "        savedir = os.path.join('spectrum_validation', 'subject_%s' % emph_subject)\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "    emph_subject = [] if emph_subject is None else [emph_subject]\n",
    "    \n",
    "    # Show FFT\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4), dpi=120)\n",
    "    ax = axes[0]\n",
    "    ax.set_title('CAP subjects (n=%d), N2 average' % len(ALL_IDS))\n",
    "    for subject_id in ALL_IDS:\n",
    "        if subject_id in emph_subject:\n",
    "            ax.plot(spectrum_dict[subject_id][:, 0], spectrum_dict[subject_id][:, 1], linewidth=1.0, color=viz.PALETTE['blue'], alpha=1, zorder=20, label=subject_id)\n",
    "        elif not collapse_background:\n",
    "            ax.plot(spectrum_dict[subject_id][:, 0], spectrum_dict[subject_id][:, 1], linewidth=0.7, color=viz.GREY_COLORS[5], alpha=0.3)\n",
    "    if collapse_background:\n",
    "        all_subjects_spectrum = np.stack([spectrum_dict[subject_id][:, 1] for subject_id in ALL_IDS], axis=0)\n",
    "        plot_lowest, plot_low, plot_median, plot_high, plot_highest = np.percentile(all_subjects_spectrum, (5, 25, 50, 75, 95), axis=0)\n",
    "        ax.plot(spectrum_dict[ALL_IDS[0]][:, 0], plot_median, linewidth=1, color=viz.GREY_COLORS[5], alpha=1)\n",
    "        ax.fill_between(\n",
    "            spectrum_dict[ALL_IDS[0]][:, 0], plot_low, plot_high,\n",
    "            facecolor=viz.GREY_COLORS[5], alpha=0.3)\n",
    "        ax.fill_between(\n",
    "            spectrum_dict[ALL_IDS[0]][:, 0], plot_lowest, plot_highest,\n",
    "            facecolor=viz.GREY_COLORS[5], alpha=0.3)\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax.set_yticks([])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax = axes[1]\n",
    "    for subject_id in ALL_IDS:\n",
    "        if subject_id in emph_subject:\n",
    "            ax.plot(spectrum_dict[subject_id][:, 0], np.log10(spectrum_dict[subject_id][:, 1]), linewidth=1.0, color=viz.PALETTE['blue'], alpha=1, zorder=20, label=subject_id)\n",
    "        elif not collapse_background:\n",
    "            ax.plot(spectrum_dict[subject_id][:, 0], np.log10(spectrum_dict[subject_id][:, 1]), linewidth=0.7, color=viz.GREY_COLORS[5], alpha=0.3)\n",
    "    if collapse_background:\n",
    "        ax.plot(spectrum_dict[ALL_IDS[0]][:, 0], np.log10(plot_median), linewidth=1, color=viz.GREY_COLORS[5], alpha=1)\n",
    "        ax.fill_between(\n",
    "            spectrum_dict[ALL_IDS[0]][:, 0], np.log10(plot_low), np.log10(plot_high),\n",
    "            facecolor=viz.GREY_COLORS[5], alpha=0.3)\n",
    "        ax.fill_between(\n",
    "            spectrum_dict[ALL_IDS[0]][:, 0], np.log10(plot_lowest), np.log10(plot_highest),\n",
    "            facecolor=viz.GREY_COLORS[5], alpha=0.3)\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax.set_yticks([])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title('log')\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        plt.savefig(os.path.join(savedir, \"spectrum_collapse%s.png\" % collapse_background), dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show some pages\n",
    "    if emph_subject:\n",
    "        data = loader.read_subject_data(emph_subject[0])\n",
    "        center_locs = np.random.RandomState(seed=0).choice(data[sample_from].mean(axis=1).astype(np.int32), size=4, replace=False)\n",
    "        center_locs = np.sort(center_locs)\n",
    "        for segment_id, center_loc in enumerate(center_locs):\n",
    "            window_size = 15\n",
    "            start_sample = int(center_loc - window_size * loader.fs // 2)\n",
    "            end_sample = int(start_sample + window_size * loader.fs)\n",
    "\n",
    "            time_axis = np.arange(start_sample, end_sample) / loader.fs\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 3), dpi=80)\n",
    "            ax.plot(time_axis, data['signal'][start_sample:end_sample], linewidth=0.7)\n",
    "            ax.set_xlim([start_sample/loader.fs, end_sample/loader.fs])\n",
    "            ax.set_ylim([-100, 100])\n",
    "\n",
    "            spindle1 = utils.filter_stamps(data['spindle1_n2'], start_sample, end_sample)\n",
    "            spindle2 = utils.filter_stamps(data['spindle2_n2'], start_sample, end_sample)\n",
    "            for i, mark in enumerate(spindle1):\n",
    "                label = 'Spindle 1' if i == 0 else None\n",
    "                ax.plot(mark / loader.fs, [-50, -50], color=viz.PALETTE['red'], linewidth=4, alpha=0.4, label=label)\n",
    "            for i, mark in enumerate(spindle2):\n",
    "                label = 'Spindle 2' if i == 0 else None\n",
    "                ax.plot(mark / loader.fs, [-60, -60], color=viz.PALETTE['blue'], linewidth=4, alpha=0.4, label=label)\n",
    "            ax.set_title(\"Subject %s, channel %s\" % (emph_subject[0], data['metadata']['channel']))\n",
    "            ax.legend(loc='upper right')\n",
    "            plt.tight_layout()\n",
    "            if save_plots:\n",
    "                plt.savefig(os.path.join(savedir, \"segment_%d.png\" % segment_id), dpi=200)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_fft(\n",
    "    '7-011',# get_id(104), \n",
    "    sample_from='spindle1_n2', save_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whole dataset statistics (N2 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(DATASET_DIR, 'CAP_groups_ids.json')\n",
    "with open(fname, 'r') as handle:\n",
    "    PATHOLOGY_TO_GROUP_DICT = json.load(handle)\n",
    "GROUP_TO_PATHOLOGY_DICT = {v: k for k, v in PATHOLOGY_TO_GROUP_DICT.items()}\n",
    "fname = os.path.join(DATASET_DIR, 'gender-age.xlsx')\n",
    "GENDER_AGE_TABLE = pd.read_excel(fname, header=None, names=['subject_id', 'gender', 'age'])\n",
    "GENDER_AGE_TABLE['subject_id'] = GENDER_AGE_TABLE['subject_id'].apply(lambda x: x.strip())\n",
    "GENDER_AGE_TABLE['subject_id'] = GENDER_AGE_TABLE['subject_id'].apply(lambda x: x.replace('RDB', 'RBD'))\n",
    "GENDER_AGE_TABLE['subject_id'] = GENDER_AGE_TABLE['subject_id'].apply(lambda x: x.replace('SBD', 'SDB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathology(subject_id):\n",
    "    group_id = int(subject_id.split(\"-\")[0])\n",
    "    pathology_abv = GROUP_TO_PATHOLOGY_DICT[group_id]\n",
    "    return pathology_abv\n",
    "\n",
    "\n",
    "def get_old_id(subject_id):\n",
    "    first_part = get_pathology(subject_id)\n",
    "    second_part = str(int(subject_id.split(\"-\")[1]))\n",
    "    return \"\".join([first_part, second_part])\n",
    "\n",
    "\n",
    "def get_age_and_sex(subject_id):\n",
    "    old_id = get_old_id(subject_id).upper()\n",
    "    subject_row = GENDER_AGE_TABLE.loc[GENDER_AGE_TABLE['subject_id'] == old_id]\n",
    "    age = subject_row['age'].values.item()\n",
    "    sex = subject_row['gender'].values.item().lower()\n",
    "    return age, sex\n",
    "\n",
    "\n",
    "def get_mean_spectrum(stack_of_pages, fs):\n",
    "    all_pages_power = []\n",
    "    for page_signal in stack_of_pages:\n",
    "        power, freq = utils.power_spectrum(page_signal, fs, apply_hanning=True)\n",
    "        all_pages_power.append(power)\n",
    "    power_mean = np.stack(all_pages_power, axis=0).mean(axis=0)\n",
    "    return freq, power_mean\n",
    "\n",
    "\n",
    "def listify_dictionaries(list_of_dicts, to_numpy=True):\n",
    "    dict_of_lists = {}\n",
    "    for key in list_of_dicts[0].keys():\n",
    "        dict_of_lists[key] = []\n",
    "        for single_dict in list_of_dicts:\n",
    "            dict_of_lists[key].append(single_dict[key])\n",
    "        if to_numpy:\n",
    "            dict_of_lists[key] = np.asarray(dict_of_lists[key])\n",
    "    return dict_of_lists\n",
    "\n",
    "\n",
    "def analyze_spindle(spindle, fs):\n",
    "    # Power ratio\n",
    "    window_han = np.hanning(spindle.size)\n",
    "    padding = np.zeros(1 * fs)\n",
    "    padded_spindle = np.concatenate([padding, spindle * window_han, padding])\n",
    "    amp, freqs = utils.power_spectrum(padded_spindle, fs, apply_hanning=False)\n",
    "    num_power = amp[(freqs >= 11) & (freqs <= 16)].mean()\n",
    "    den_power = amp[(freqs >= 4.5) & (freqs <= 30)].mean()\n",
    "    power_ratio = num_power / (den_power + 1e-6)\n",
    "    # other params\n",
    "    duration = spindle.size / fs\n",
    "    pp_amplitude = spindle.max() - spindle.min()\n",
    "    rms = np.sqrt(np.mean(spindle ** 2))\n",
    "    # Compute peak frequency by fft\n",
    "    w, h = freqz(spindle)\n",
    "    resp_freq = w * fs / (2*np.pi)\n",
    "    resp_amp = abs(h)\n",
    "    max_loc = np.argmax(resp_amp)\n",
    "    central_freq_fft = resp_freq[max_loc]\n",
    "    results = {\n",
    "        'duration': duration,\n",
    "        'amplitude_pp': pp_amplitude,\n",
    "        'amplitude_rms': rms,\n",
    "        'central_freq_fft': central_freq_fft,\n",
    "        'power_ratio': power_ratio\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_size = int(loader.fs * loader.page_duration)\n",
    "table_list = []\n",
    "print(\"Generating table \", end='')\n",
    "for subject_id in ALL_IDS:\n",
    "    print('.', end='', flush=True)\n",
    "    data = loader.read_subject_data(subject_id)\n",
    "    \n",
    "    age, sex = get_age_and_sex(subject_id)\n",
    "    \n",
    "    # Signal descriptors:\n",
    "    signal_duration_hours = data['signal'].size / loader.fs / 3600\n",
    "    signal_min = data['signal'].min()\n",
    "    signal_mean = data['signal'].mean()\n",
    "    signal_std = data['signal'].std()\n",
    "    signal_max = data['signal'].max()\n",
    "    \n",
    "    # fft -> use original 20s n2 pages here\n",
    "    stack_of_pages = utils.extract_pages(data['signal'], data['n2_pages'], page_size)\n",
    "    freq, mean_power = get_mean_spectrum(stack_of_pages, loader.fs)\n",
    "    mean_power_n2_mean_delta_slow = mean_power[(freq >= 0.5) & (freq <= 2)].mean()\n",
    "    mean_power_n2_mean_delta_fast = mean_power[(freq >= 2) & (freq <= 4)].mean()\n",
    "    mean_power_n2_mean_theta = mean_power[(freq >= 4) & (freq <= 8)].mean()\n",
    "    mean_power_n2_mean_alpha = mean_power[(freq >= 8) & (freq <= 12)].mean()\n",
    "    mean_power_n2_mean_sigma = mean_power[(freq >= 11) & (freq <= 16)].mean()\n",
    "    mean_power_n2_mean_beta = mean_power[(freq >= 13) & (freq <= 30)].mean()\n",
    "    # lacourse (a7) power ratio\n",
    "    num_power = mean_power[(freq >= 11) & (freq <= 16)].mean()\n",
    "    den_power = mean_power[(freq >= 4.5) & (freq <= 30)].mean()\n",
    "    spectrum_power_ratio = num_power / (den_power + 1e-6)\n",
    "    \n",
    "    # original hypnogram descriptors:\n",
    "    rem_duration_hours = np.sum(data['hypnogram_original'] == 'R') * loader.original_page_duration / 3600\n",
    "    n1_duration_hours = np.sum(data['hypnogram_original'] == 'S1') * loader.original_page_duration / 3600\n",
    "    n2_duration_hours = np.sum(data['hypnogram_original'] == 'S2') * loader.original_page_duration / 3600\n",
    "    n3_duration_hours = np.sum(data['hypnogram_original'] == 'S3') * loader.original_page_duration / 3600\n",
    "    n4_duration_hours = np.sum(data['hypnogram_original'] == 'S4') * loader.original_page_duration / 3600\n",
    "    nrem_duration_hours = n1_duration_hours + n2_duration_hours + n3_duration_hours + n4_duration_hours\n",
    "    sleep_duration_hours = rem_duration_hours + nrem_duration_hours\n",
    "    n2_to_sleep_ratio = n2_duration_hours / sleep_duration_hours\n",
    "    \n",
    "    labels_in_hypno = np.unique(data['hypnogram_original'])\n",
    "    valid_labels_list = ['?', 'R', 'S1', 'S2', 'S3', 'S4', 'W']\n",
    "    flag_valid_hypno_labels = np.all([l in valid_labels_list for l in labels_in_hypno])\n",
    "    \n",
    "    # Spindles descriptors: -> use original n2 pages here\n",
    "    spindles_1 = utils.extract_pages_for_stamps(\n",
    "        data['spindle1_all'], \n",
    "        np.where(data['hypnogram_original'] == loader.n2_id)[0], \n",
    "        int(loader.original_page_duration * loader.fs))\n",
    "    spindles_2 = utils.extract_pages_for_stamps(\n",
    "        data['spindle2_all'], \n",
    "        np.where(data['hypnogram_original'] == loader.n2_id)[0], \n",
    "        int(loader.original_page_duration * loader.fs))\n",
    "    \n",
    "    spindles1_n2_number = spindles_1.shape[0]\n",
    "    spindles1_n2_density = spindles1_n2_number / (n2_duration_hours * 60)\n",
    "    spindles1_n2_mean_duration = np.mean(spindles_1[:, 1] - spindles_1[:, 0] + 1) / loader.fs\n",
    "    \n",
    "    spindles2_n2_number = spindles_2.shape[0]\n",
    "    spindles2_n2_density = spindles2_n2_number / (n2_duration_hours * 60)\n",
    "    spindles2_n2_mean_duration = np.mean(spindles_2[:, 1] - spindles_2[:, 0] + 1) / loader.fs\n",
    "               \n",
    "    iou_matching, idx_matching = metrics.matching(spindles_1, spindles_2)\n",
    "    spindle_1_to_2_recall = metrics.metric_vs_iou(spindles_1, spindles_2, [0.2], metric_name=constants.RECALL, iou_matching=iou_matching)[0]\n",
    "    spindle_1_to_2_precision = metrics.metric_vs_iou(spindles_1, spindles_2, [0.2], metric_name=constants.PRECISION, iou_matching=iou_matching)[0]\n",
    "    spindle_1_to_2_f1score = metrics.metric_vs_iou(spindles_1, spindles_2, [0.2], metric_name=constants.F1_SCORE, iou_matching=iou_matching)[0]\n",
    "    spindle_1_to_2_miou = np.mean(iou_matching[idx_matching != -1])\n",
    "    \n",
    "    # Spindle + signal descriptors: -> use original n2 pages here\n",
    "    sigma_signal = utils.broad_filter(data['signal'], loader.fs, lowcut=10, highcut=17)\n",
    "    # Spindle 1 parameters\n",
    "    spindles = [sigma_signal[s_start:s_end] for (s_start, s_end) in spindles_1]\n",
    "    analysis = [analyze_spindle(spindle, loader.fs) for spindle in spindles]\n",
    "    if len(analysis) > 0:\n",
    "        analysis = listify_dictionaries(analysis)\n",
    "        spindles1_n2_freq = np.mean(analysis['central_freq_fft'])\n",
    "        spindles1_n2_amplitude_pp = np.mean(analysis['amplitude_pp'])\n",
    "        spindles1_n2_amplitude_rms = np.mean(analysis['amplitude_rms'])\n",
    "        spindles1_n2_power_ratio = np.mean(analysis['power_ratio'])\n",
    "    else:\n",
    "        spindles1_n2_freq = np.nan\n",
    "        spindles1_n2_amplitude_pp = np.nan\n",
    "        spindles1_n2_amplitude_rms = np.nan\n",
    "        spindles1_n2_power_ratio = np.nan\n",
    "    # Spindle 2 parameters\n",
    "    spindles = [sigma_signal[s_start:s_end] for (s_start, s_end) in spindles_2]\n",
    "    analysis = [analyze_spindle(spindle, loader.fs) for spindle in spindles] \n",
    "    if len(analysis) > 0:\n",
    "        analysis = listify_dictionaries(analysis)\n",
    "        spindles2_n2_freq = np.mean(analysis['central_freq_fft'])\n",
    "        spindles2_n2_amplitude_pp = np.mean(analysis['amplitude_pp'])\n",
    "        spindles2_n2_amplitude_rms = np.mean(analysis['amplitude_rms'])\n",
    "        spindles2_n2_power_ratio = np.mean(analysis['power_ratio'])\n",
    "    else:\n",
    "        spindles2_n2_freq = np.nan\n",
    "        spindles2_n2_amplitude_pp = np.nan\n",
    "        spindles2_n2_amplitude_rms = np.nan\n",
    "        spindles2_n2_power_ratio = np.nan\n",
    "    \n",
    "    # Collect results\n",
    "    tmp_dict = {\n",
    "        'subject_id': subject_id,\n",
    "        'channel': data['metadata']['channel'],\n",
    "        'pathology': get_pathology(subject_id),\n",
    "        'age': age,\n",
    "        'sex': sex,\n",
    "        'signal_hours': signal_duration_hours,\n",
    "        'signal_min': signal_min,\n",
    "        'signal_mean': signal_mean,\n",
    "        'signal_std': signal_std,\n",
    "        'signal_max': signal_max,\n",
    "        'power_delta_slow': mean_power_n2_mean_delta_slow,\n",
    "        'power_delta_fast': mean_power_n2_mean_delta_fast,\n",
    "        'power_theta': mean_power_n2_mean_theta,\n",
    "        'power_alpha': mean_power_n2_mean_alpha,\n",
    "        'power_sigma': mean_power_n2_mean_sigma,\n",
    "        'power_beta': mean_power_n2_mean_beta,\n",
    "        'power_ratio': spectrum_power_ratio,\n",
    "        'hypno_rem_hours': rem_duration_hours,\n",
    "        'hypno_n1_hours': n1_duration_hours,\n",
    "        'hypno_n2_hours': n2_duration_hours,\n",
    "        'hypno_n3_hours': n3_duration_hours,\n",
    "        'hypno_n4_hours': n4_duration_hours,\n",
    "        'hypno_nrem_hours': nrem_duration_hours,\n",
    "        'hypno_sleep_hours': sleep_duration_hours,\n",
    "        'hypno_n2_to_sleep': n2_to_sleep_ratio,\n",
    "        'hypno_flag_sanity': flag_valid_hypno_labels,\n",
    "        's1_number': spindles1_n2_number,\n",
    "        's1_density': spindles1_n2_density,\n",
    "        's1_duration': spindles1_n2_mean_duration,\n",
    "        's2_number': spindles2_n2_number,\n",
    "        's2_density': spindles2_n2_density,\n",
    "        's2_duration': spindles2_n2_mean_duration,\n",
    "        'match12_recall': spindle_1_to_2_recall,\n",
    "        'match12_precision': spindle_1_to_2_precision,\n",
    "        'match12_f1score': spindle_1_to_2_f1score,\n",
    "        'match12_miou': spindle_1_to_2_miou, \n",
    "        's1_freq': spindles1_n2_freq,\n",
    "        's1_amp_pp': spindles1_n2_amplitude_pp,\n",
    "        's1_amp_rms': spindles1_n2_amplitude_rms,\n",
    "        's1_power_ratio': spindles1_n2_power_ratio,\n",
    "        's2_freq': spindles2_n2_freq,\n",
    "        's2_amp_pp': spindles2_n2_amplitude_pp,\n",
    "        's2_amp_rms': spindles2_n2_amplitude_rms,\n",
    "        's2_power_ratio': spindles2_n2_power_ratio\n",
    "    }\n",
    "    table_list.append(tmp_dict)\n",
    "print(\" Done\")\n",
    "big_table = pd.DataFrame(table_list)\n",
    "big_table.to_csv(os.path.join(DATASET_DIR, 'big_table.csv'), index = False)\n",
    "big_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with shape (104, 44)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>pathology</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>signal_hours</th>\n",
       "      <th>signal_min</th>\n",
       "      <th>signal_mean</th>\n",
       "      <th>signal_std</th>\n",
       "      <th>signal_max</th>\n",
       "      <th>...</th>\n",
       "      <th>match12_f1score</th>\n",
       "      <th>match12_miou</th>\n",
       "      <th>s1_freq</th>\n",
       "      <th>s1_amp_pp</th>\n",
       "      <th>s1_amp_rms</th>\n",
       "      <th>s1_power_ratio</th>\n",
       "      <th>s2_freq</th>\n",
       "      <th>s2_amp_pp</th>\n",
       "      <th>s2_amp_rms</th>\n",
       "      <th>s2_power_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-001</td>\n",
       "      <td>C4-A1</td>\n",
       "      <td>n</td>\n",
       "      <td>37</td>\n",
       "      <td>f</td>\n",
       "      <td>9.616667</td>\n",
       "      <td>-693.722351</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>19.938787</td>\n",
       "      <td>449.330200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651224</td>\n",
       "      <td>0.584456</td>\n",
       "      <td>13.607422</td>\n",
       "      <td>40.123288</td>\n",
       "      <td>10.956236</td>\n",
       "      <td>4.042940</td>\n",
       "      <td>13.638067</td>\n",
       "      <td>33.662973</td>\n",
       "      <td>8.008141</td>\n",
       "      <td>4.180594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-002</td>\n",
       "      <td>C4-A1</td>\n",
       "      <td>n</td>\n",
       "      <td>34</td>\n",
       "      <td>m</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>-361.908936</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>18.905073</td>\n",
       "      <td>411.918854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753452</td>\n",
       "      <td>0.739252</td>\n",
       "      <td>13.354577</td>\n",
       "      <td>47.737720</td>\n",
       "      <td>12.227225</td>\n",
       "      <td>3.866156</td>\n",
       "      <td>13.289328</td>\n",
       "      <td>43.194544</td>\n",
       "      <td>10.440300</td>\n",
       "      <td>3.854735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-003</td>\n",
       "      <td>C4-A1</td>\n",
       "      <td>n</td>\n",
       "      <td>35</td>\n",
       "      <td>f</td>\n",
       "      <td>9.183611</td>\n",
       "      <td>-802.269592</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>23.741087</td>\n",
       "      <td>643.161072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441905</td>\n",
       "      <td>0.534438</td>\n",
       "      <td>13.393315</td>\n",
       "      <td>41.703911</td>\n",
       "      <td>11.184649</td>\n",
       "      <td>3.668290</td>\n",
       "      <td>13.157181</td>\n",
       "      <td>33.592781</td>\n",
       "      <td>7.940702</td>\n",
       "      <td>3.724424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-004</td>\n",
       "      <td>C4-A1</td>\n",
       "      <td>n</td>\n",
       "      <td>25</td>\n",
       "      <td>f</td>\n",
       "      <td>9.927778</td>\n",
       "      <td>-629.273132</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>20.362926</td>\n",
       "      <td>435.637604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.699981</td>\n",
       "      <td>13.227227</td>\n",
       "      <td>43.666849</td>\n",
       "      <td>11.506620</td>\n",
       "      <td>4.102276</td>\n",
       "      <td>13.214290</td>\n",
       "      <td>39.587462</td>\n",
       "      <td>9.673125</td>\n",
       "      <td>4.136671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-005</td>\n",
       "      <td>C4-A1</td>\n",
       "      <td>n</td>\n",
       "      <td>35</td>\n",
       "      <td>f</td>\n",
       "      <td>8.733611</td>\n",
       "      <td>-947.683472</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>22.980350</td>\n",
       "      <td>543.786194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764783</td>\n",
       "      <td>0.680081</td>\n",
       "      <td>13.328699</td>\n",
       "      <td>46.857011</td>\n",
       "      <td>12.177717</td>\n",
       "      <td>4.137713</td>\n",
       "      <td>13.311427</td>\n",
       "      <td>40.210202</td>\n",
       "      <td>9.496020</td>\n",
       "      <td>4.175193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id channel pathology  age sex  signal_hours  signal_min  \\\n",
       "0      1-001   C4-A1         n   37   f      9.616667 -693.722351   \n",
       "1      1-002   C4-A1         n   34   m     12.250000 -361.908936   \n",
       "2      1-003   C4-A1         n   35   f      9.183611 -802.269592   \n",
       "3      1-004   C4-A1         n   25   f      9.927778 -629.273132   \n",
       "4      1-005   C4-A1         n   35   f      8.733611 -947.683472   \n",
       "\n",
       "   signal_mean  signal_std  signal_max  ...  match12_f1score  match12_miou  \\\n",
       "0     0.000761   19.938787  449.330200  ...         0.651224      0.584456   \n",
       "1     0.001033   18.905073  411.918854  ...         0.753452      0.739252   \n",
       "2    -0.000113   23.741087  643.161072  ...         0.441905      0.534438   \n",
       "3     0.000532   20.362926  435.637604  ...         0.789157      0.699981   \n",
       "4     0.000358   22.980350  543.786194  ...         0.764783      0.680081   \n",
       "\n",
       "     s1_freq  s1_amp_pp  s1_amp_rms  s1_power_ratio    s2_freq  s2_amp_pp  \\\n",
       "0  13.607422  40.123288   10.956236        4.042940  13.638067  33.662973   \n",
       "1  13.354577  47.737720   12.227225        3.866156  13.289328  43.194544   \n",
       "2  13.393315  41.703911   11.184649        3.668290  13.157181  33.592781   \n",
       "3  13.227227  43.666849   11.506620        4.102276  13.214290  39.587462   \n",
       "4  13.328699  46.857011   12.177717        4.137713  13.311427  40.210202   \n",
       "\n",
       "   s2_amp_rms  s2_power_ratio  \n",
       "0    8.008141        4.180594  \n",
       "1   10.440300        3.854735  \n",
       "2    7.940702        3.724424  \n",
       "3    9.673125        4.136671  \n",
       "4    9.496020        4.175193  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load table from memory if already exists\n",
    "big_table = pd.read_csv(os.path.join(DATASET_DIR, 'big_table.csv'))\n",
    "print(\"Table with shape\", big_table.shape)\n",
    "big_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
