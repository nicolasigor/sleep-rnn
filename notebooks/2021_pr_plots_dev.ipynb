{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.helpers import reader, misc\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.common.optimal_thresholds import OPTIMAL_THR_FOR_CKPT_DICT\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_folder = '20210411_bsf_update_exp1'\n",
    "# You may specify certain runs within that ckpt_folder in grid_folder_list.\n",
    "# If None then all runs are returned\n",
    "grid_folder_list = None\n",
    "# You may specify certain seeds within the experiment to plot.\n",
    "# If None, then all available seeds are plotted\n",
    "selected_seeds = None\n",
    "\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "train_fraction = 0.75\n",
    "fs = 200\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "set_list = [constants.VAL_SUBSET]\n",
    "verbose = False\n",
    "\n",
    "# Plot settings\n",
    "legend_fontsize = 9  # 7\n",
    "show_splits_id = True\n",
    "show_subject_id = True\n",
    "show_grid = True\n",
    "show_desired_attractor = True\n",
    "show_mean = True\n",
    "show_quadrants = True\n",
    "skip_subjects = []  # [11, 14, 19]\n",
    "iou_to_show = 0.2\n",
    "marker_size = 7\n",
    "marker_alpha = 1.0\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "# Identifier\n",
    "result_id = '%s-%s-E%d-%s' % (\n",
    "    dataset_name.split('_')[0].upper(),\n",
    "    dataset_name.split('_')[1].upper(),\n",
    "    which_expert,\n",
    "    task_mode.upper())\n",
    "\n",
    "# Define paths\n",
    "full_ckpt_folder = '%s_%s_train_%s' % (ckpt_folder, task_mode, dataset_name)\n",
    "if grid_folder_list is None:\n",
    "    grid_folder_list = os.listdir(os.path.join(\n",
    "        RESULTS_PATH, 'predictions_%s' % dataset_name, full_ckpt_folder))\n",
    "grid_folder_list.sort()\n",
    "print(\"Checkpoint folder:\\n%s\" % full_ckpt_folder)\n",
    "print(\"Considered settings:\")\n",
    "pprint(grid_folder_list)\n",
    "save_dir = os.path.join(\n",
    "    RESULTS_PATH,\n",
    "    'auto_pr_figs',\n",
    "    full_ckpt_folder)\n",
    "# Find available seeds\n",
    "if selected_seeds is None:\n",
    "    available_seed_folders = os.listdir(os.path.abspath(os.path.join(\n",
    "        RESULTS_PATH,\n",
    "        'predictions_%s' % dataset_name,\n",
    "        full_ckpt_folder, grid_folder_list[0]\n",
    "    )))\n",
    "    seeds_to_show = [int(f[4:]) for f in available_seed_folders]\n",
    "    seeds_to_show.sort()\n",
    "else:\n",
    "    seeds_to_show = selected_seeds\n",
    "print(\"Seeds to plot: %s\" % seeds_to_show)\n",
    "print(\"\")\n",
    "\n",
    "# Load data and predictions\n",
    "dataset = reader.load_dataset(dataset_name, params={pkeys.FS: fs}, verbose=verbose)\n",
    "ids_dict = {constants.ALL_TRAIN_SUBSET: dataset.train_ids}\n",
    "ids_dict.update(misc.get_splits_dict(dataset, seeds_to_show, use_test_set=False, train_fraction=train_fraction))\n",
    "predictions_dict = {}\n",
    "for grid_folder in grid_folder_list:\n",
    "    full_grid_path = os.path.join(full_ckpt_folder, grid_folder)\n",
    "    predictions_dict[grid_folder] = reader.read_prediction_with_seeds(\n",
    "        full_grid_path, dataset_name, task_mode, seeds_to_show, set_list=set_list, parent_dataset=dataset,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "# Plot\n",
    "print(\"\\nProcessing %s\" % full_ckpt_folder, flush=True)\n",
    "# if save_figs:\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     print(\"Saving directory: %s\" % save_dir)\n",
    "color_dict = {\n",
    "    constants.TRAIN_SUBSET: {\n",
    "        i: viz.GREY_COLORS[4] for i in range(4)},\n",
    "    constants.VAL_SUBSET: {\n",
    "        0: viz.PALETTE[constants.RED],\n",
    "        1: viz.PALETTE[constants.BLUE],\n",
    "        2: viz.PALETTE[constants.GREEN],\n",
    "        3: viz.PALETTE[constants.DARK],\n",
    "        4: viz.PALETTE[constants.CYAN],\n",
    "        5: viz.PALETTE[constants.PURPLE],\n",
    "        6: viz.PALETTE[constants.GREY],\n",
    "    }\n",
    "}\n",
    "axis_markers = np.arange(0, 1.1, 0.1)\n",
    "for grid_folder in grid_folder_list:\n",
    "    full_grid_path = os.path.join(full_ckpt_folder, grid_folder)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=100)\n",
    "    tmp_all_recall = []\n",
    "    tmp_all_precision = []\n",
    "    tmp_all_mean_iou = []\n",
    "    for seed_id in seeds_to_show:\n",
    "        # ---------------- Compute performance\n",
    "        pre_vs_iou_subject_dict = {}\n",
    "        rec_vs_iou_subject_dict = {}\n",
    "        mean_iou_subject_dict = {}\n",
    "        for set_name in set_list:\n",
    "            # Prepare expert labels\n",
    "            data_inference = FeederDataset(dataset, ids_dict[seed_id][set_name], task_mode, which_expert)\n",
    "            this_ids = data_inference.get_ids()\n",
    "            this_events_list = data_inference.get_stamps()\n",
    "            # Prepare model predictions\n",
    "            prediction_obj = predictions_dict[grid_folder][seed_id][set_name]\n",
    "            prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[full_grid_path][seed_id])\n",
    "            this_detections_list = prediction_obj.get_stamps()\n",
    "            for i, single_id in enumerate(this_ids):\n",
    "                single_events = this_events_list[i]\n",
    "                single_detections = this_detections_list[i]\n",
    "                this_iou_matching, _ = metrics.matching(single_events, single_detections)\n",
    "                this_mean_iou = np.mean(this_iou_matching[this_iou_matching > 0])\n",
    "                this_precision = metrics.metric_vs_iou(\n",
    "                    single_events, single_detections, [iou_to_show], metric_name=constants.PRECISION,\n",
    "                    iou_matching=this_iou_matching)\n",
    "                this_recall = metrics.metric_vs_iou(\n",
    "                    single_events, single_detections, [iou_to_show], metric_name=constants.RECALL,\n",
    "                    iou_matching=this_iou_matching)\n",
    "                pre_vs_iou_subject_dict[single_id] = this_precision[0]\n",
    "                rec_vs_iou_subject_dict[single_id] = this_recall[0]\n",
    "                mean_iou_subject_dict[single_id] = this_mean_iou\n",
    "\n",
    "        # -------------------- P L O T ----------------------\n",
    "        for set_name in set_list:\n",
    "            for i, single_id in enumerate(ids_dict[seed_id][set_name]):\n",
    "                if single_id in skip_subjects:\n",
    "                    continue\n",
    "                this_rec = rec_vs_iou_subject_dict[single_id]\n",
    "                this_pre = pre_vs_iou_subject_dict[single_id]\n",
    "                this_iou = mean_iou_subject_dict[single_id]\n",
    "                tmp_all_recall.append(this_rec)\n",
    "                tmp_all_precision.append(this_pre)\n",
    "                tmp_all_mean_iou.append(this_iou)\n",
    "                label = 'Split %d' % seed_id if i == 0 else None\n",
    "                if show_splits_id:\n",
    "                    color = color_dict[set_name][seed_id]\n",
    "                else:\n",
    "                    color = viz.PALETTE['blue']\n",
    "                ax.plot(\n",
    "                    this_rec, this_pre, color=color, alpha=marker_alpha, markeredgewidth=0.0,\n",
    "                    marker='o', markersize=marker_size, label=label, linestyle='None')\n",
    "                if show_subject_id:\n",
    "                    if isinstance(single_id, str):\n",
    "                        single_id_to_show = int(single_id[0] + single_id[3:])\n",
    "                        subject_id_fontsize = 3\n",
    "                    else:\n",
    "                        single_id_to_show = single_id\n",
    "                        subject_id_fontsize = 4\n",
    "                    ax.annotate(\n",
    "                        single_id_to_show, (this_rec, this_pre),\n",
    "                        horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                        fontsize=subject_id_fontsize, color=\"w\")\n",
    "    tmp_all_precision = np.array(tmp_all_precision)\n",
    "    tmp_all_recall = np.array(tmp_all_recall)\n",
    "    tmp_all_f1_score = 2 * tmp_all_precision * tmp_all_recall / (tmp_all_precision + tmp_all_recall)\n",
    "    print(\"Precision %1.1f \\u00B1 %4.1f -- Recall %1.1f \\u00B1 %4.1f -- F1-score %1.1f \\u00B1 %1.1f -- mIoU %1.1f \\u00B1 %1.1f for %s\" % (\n",
    "        100 * np.mean(tmp_all_precision), 100 * np.std(tmp_all_precision),\n",
    "        100 * np.mean(tmp_all_recall), 100 * np.std(tmp_all_recall),\n",
    "        100 * np.mean(tmp_all_f1_score), 100 * np.std(tmp_all_f1_score),\n",
    "        100 * np.mean(tmp_all_mean_iou), 100 * np.std(tmp_all_mean_iou),\n",
    "        grid_folder\n",
    "    ))\n",
    "    perf_str = \"F1: %1.1f\\u00B1%1.1f, IoU: %1.1f\\u00B1%1.1f\\nP: %1.1f\\u00B1%1.1f, R: %1.1f\\u00B1%1.1f\" % (\n",
    "        100 * np.mean(tmp_all_f1_score), 100 * np.std(tmp_all_f1_score),\n",
    "        100 * np.mean(tmp_all_mean_iou), 100 * np.std(tmp_all_mean_iou),\n",
    "        100 * np.mean(tmp_all_precision), 100 * np.std(tmp_all_precision),\n",
    "        100 * np.mean(tmp_all_recall), 100 * np.std(tmp_all_recall),\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], zorder=1, linewidth=1, color=viz.GREY_COLORS[4])\n",
    "    ax.set_title('%s\\nValidation, %s\\n%s' % (grid_folder, result_id, perf_str), fontsize=9)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_yticks(axis_markers)\n",
    "    ax.set_xticks(axis_markers)\n",
    "    if show_grid:\n",
    "        ax.set_xticks(np.arange(0, 1, 0.1), minor=True)\n",
    "        ax.set_yticks(np.arange(0, 1, 0.1), minor=True)\n",
    "        ax.grid(which=\"minor\")\n",
    "    if show_quadrants:\n",
    "        ax.axhline(0.5, color=viz.GREY_COLORS[5], linewidth=2)\n",
    "        ax.axvline(0.5, color=viz.GREY_COLORS[5], linewidth=2)\n",
    "    if show_desired_attractor:\n",
    "        ax.fill_between([0.80, 0.9], 0.8, 0.9, facecolor=viz.GREY_COLORS[2], zorder=1)\n",
    "    if show_mean:\n",
    "        ax.plot(\n",
    "            np.mean(tmp_all_recall), np.mean(tmp_all_precision),\n",
    "            marker='o', markersize=marker_size / 2, linestyle=\"None\",\n",
    "            color=viz.GREY_COLORS[6]\n",
    "        )\n",
    "    ax.tick_params(labelsize=8.5)\n",
    "    ax.set_ylabel('Precision (IoU>%1.1f)' % iou_to_show, fontsize=9)\n",
    "    ax.set_xlabel('Recall (IoU>%1.1f)' % iou_to_show, fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "    if show_splits_id:\n",
    "        ax.legend(loc='lower left', fontsize=legend_fontsize)\n",
    "    plt.tight_layout()\n",
    "    # if save_figs:\n",
    "    #     fname = os.path.join(save_dir, \"pr_seeds_%s.png\" % grid_folder)\n",
    "    #     plt.savefig(fname, dpi=viz.DPI, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New implementation - By fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_precision_recall_plot(\n",
    "    ax,\n",
    "    axis_markers=None, \n",
    "    show_quadrants=True,\n",
    "    show_grid=True, \n",
    "    minor_axis_markers=None\n",
    "):\n",
    "    if axis_markers is None:\n",
    "        axis_markers = np.arange(0, 1 + 0.001, 0.1)\n",
    "    if minor_axis_markers is None:\n",
    "        minor_axis_markers = axis_markers\n",
    "    # Diagonal\n",
    "    ax.plot([0, 1], [0, 1], zorder=1, linewidth=1, color=viz.GREY_COLORS[4])\n",
    "    # Square basis\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_aspect('equal')\n",
    "    # Ticks\n",
    "    ax.set_yticks(axis_markers)\n",
    "    ax.set_xticks(axis_markers)\n",
    "    ax.set_xticks(minor_axis_markers, minor=True)\n",
    "    ax.set_yticks(minor_axis_markers, minor=True)\n",
    "    if show_grid:    \n",
    "        ax.grid(which=\"minor\")\n",
    "    if show_quadrants:\n",
    "        ax.axhline(0.5, color=viz.GREY_COLORS[5], linewidth=2)\n",
    "        ax.axvline(0.5, color=viz.GREY_COLORS[5], linewidth=2)\n",
    "\n",
    "\n",
    "def get_fold_colors():\n",
    "    color_dict = {\n",
    "        0: viz.PALETTE[constants.RED],\n",
    "        1: viz.PALETTE[constants.BLUE],\n",
    "        2: viz.PALETTE[constants.GREEN],\n",
    "        3: viz.PALETTE[constants.DARK],\n",
    "        5: viz.PALETTE[constants.PURPLE],\n",
    "        4: viz.PALETTE[constants.CYAN],\n",
    "        6: viz.PALETTE[constants.GREY],\n",
    "    }\n",
    "    return color_dict\n",
    "\n",
    "\n",
    "def get_performance_string(outputs):\n",
    "    perf_str = \"F1: %1.1f\\u00B1%1.1f, IoU: %1.1f\\u00B1%1.1f\\nP: %1.1f\\u00B1%1.1f, R: %1.1f\\u00B1%1.1f\" % (\n",
    "        100 * np.mean(outputs['f1']), 100 * np.std(outputs['f1']),\n",
    "        100 * np.mean(outputs['miou']), 100 * np.std(outputs['miou']),\n",
    "        100 * np.mean(outputs['prec']), 100 * np.std(outputs['prec']),\n",
    "        100 * np.mean(outputs['rec']), 100 * np.std(outputs['rec']),\n",
    "    )\n",
    "    return perf_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = constants.MASS_SS_NAME\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "dataset_params = {pkeys.FS: 200}\n",
    "load_dataset_from_ckpt = True\n",
    "dataset = reader.load_dataset(\n",
    "    dataset_name, params=dataset_params, load_checkpoint=load_dataset_from_ckpt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_folder_prefix = '20210411_bsf_update_exp1'\n",
    "# You may specify certain runs within that ckpt_folder in grid_folder_list.\n",
    "# If None then all runs are returned\n",
    "grid_folder_list = None\n",
    "# You may specify certain folds within the experiment to plot.\n",
    "# If None, then all available folds are used\n",
    "selected_folds = None\n",
    "\n",
    "# Data settings\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "dataset_params = {pkeys.FS: 200}\n",
    "load_dataset_from_ckpt = True\n",
    "\n",
    "# Evaluation settings\n",
    "evaluation_set = constants.VAL_SUBSET\n",
    "average_mode = constants.MACRO_AVERAGE\n",
    "iou_threshold_report = 0.2\n",
    "\n",
    "# Plot settings\n",
    "title_fontsize = 9\n",
    "general_fontsize = 9\n",
    "marker_size = 7\n",
    "marker_alpha = 1.0\n",
    "fold_monocolor = False\n",
    "show_fold_id = True\n",
    "show_grid = True\n",
    "show_mean = True\n",
    "show_quadrants = True\n",
    "axis_markers = np.arange(0, 1 + 0.001, 0.2)\n",
    "minor_axis_markers = np.arange(0, 1 + 0.001, 0.1)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "color_dict = get_fold_colors()\n",
    "dataset = reader.load_dataset(\n",
    "    dataset_name, params=dataset_params, load_checkpoint=load_dataset_from_ckpt, verbose=False)\n",
    "ckpt_folder = '%s_%s_train_%s' % (ckpt_folder_prefix, task_mode, dataset_name)\n",
    "if grid_folder_list is None:\n",
    "    experiment_path = os.path.join(RESULTS_PATH, 'predictions_%s' % dataset_name, ckpt_folder)\n",
    "    grid_folder_list = os.listdir(experiment_path)\n",
    "    grid_folder_list.sort()\n",
    "print('Grid settings to be used from %s:' % ckpt_folder)\n",
    "pprint(grid_folder_list)\n",
    "predictions_dict = {}\n",
    "for grid_folder in grid_folder_list:\n",
    "    grid_folder_complete = os.path.join(ckpt_folder, grid_folder)\n",
    "    predictions_dict[grid_folder] = reader.read_predictions_crossval(\n",
    "        grid_folder_complete, dataset, task_mode)\n",
    "if selected_folds is None:\n",
    "    selected_folds = list(predictions_dict[grid_folder_list[0]].keys())\n",
    "    selected_folds.sort()\n",
    "print(\"Folds to plot: %s\" % selected_folds)\n",
    "print(\"\")\n",
    "result_id = '%s-%s-E%d-%s' % (\n",
    "    dataset_name.split('_')[0].upper(),\n",
    "    dataset_name.split('_')[1].upper(),\n",
    "    which_expert,\n",
    "    task_mode.upper())\n",
    "metric_vs_iou_fn_dict = {\n",
    "    constants.MACRO_AVERAGE: metrics.metric_vs_iou_macro_average,\n",
    "    constants.MICRO_AVERAGE: metrics.metric_vs_iou_micro_average}\n",
    "for grid_folder in grid_folder_list:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=viz.DPI)\n",
    "    opt_thr_list = OPTIMAL_THR_FOR_CKPT_DICT[os.path.join(ckpt_folder, grid_folder)]\n",
    "    outputs = {'f1': [], 'miou': [], 'prec': [], 'rec': []}\n",
    "    for k in selected_folds:\n",
    "        # Retrieve relevant data\n",
    "        eval_predictions = predictions_dict[grid_folder][k][evaluation_set]\n",
    "        subject_ids = eval_predictions.get_ids()\n",
    "        feed_d = FeederDataset(dataset, subject_ids, task_mode, which_expert=which_expert)\n",
    "        events_list = feed_d.get_stamps()\n",
    "        eval_predictions.set_probability_threshold(opt_thr_list[k])\n",
    "        detections_list = eval_predictions.get_stamps()\n",
    "        # Compute performance\n",
    "        iou_matching_list, _ = metrics.matching_with_list(events_list, detections_list)\n",
    "        f1_score = metric_vs_iou_fn_dict[average_mode](\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.F1_SCORE)\n",
    "        recall = metric_vs_iou_fn_dict[average_mode](\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.RECALL)\n",
    "        precision = metric_vs_iou_fn_dict[average_mode](\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.PRECISION)\n",
    "        nonzero_iou_list = [iou_matching[iou_matching > 0] for iou_matching in iou_matching_list]\n",
    "        if average_mode == constants.MACRO_AVERAGE:\n",
    "            miou_list = [np.mean(nonzero_iou) for nonzero_iou in nonzero_iou_list]\n",
    "            miou = np.mean(miou_list)\n",
    "        elif average_mode == constants.MICRO_AVERAGE:\n",
    "            miou = np.concatenate(nonzero_iou_list).mean()\n",
    "        else:\n",
    "            raise ValueError(\"Average mode %s invalid\" % average_mode)\n",
    "        outputs['f1'].append(f1_score[0])\n",
    "        outputs['prec'].append(precision[0])\n",
    "        outputs['rec'].append(recall[0])\n",
    "        outputs['miou'].append(miou)\n",
    "        # Plot\n",
    "        color = viz.PALETTE['blue'] if fold_monocolor else color_dict[k]\n",
    "        ax.plot(\n",
    "            recall[0], precision[0], \n",
    "            color=color, linestyle='None', alpha=marker_alpha, \n",
    "            markeredgewidth=0.0, marker='o', markersize=marker_size, \n",
    "            label='Fold %d' % k)\n",
    "    if show_mean:\n",
    "        ax.plot(\n",
    "            np.mean(outputs['rec']), np.mean(outputs['prec']),\n",
    "            marker='o', markersize=marker_size / 2, linestyle=\"None\",\n",
    "            color=viz.GREY_COLORS[6])\n",
    "    perf_str = get_performance_string(outputs)\n",
    "    eval_str = \"%s-%s\" % (average_mode.split(\"_\")[0].upper(), evaluation_set.upper())\n",
    "    ax.set_title(\n",
    "        '%s\\n%s, %s\\n%s' % (\n",
    "            grid_folder, eval_str, result_id, perf_str), \n",
    "        fontsize=title_fontsize)\n",
    "    ax.tick_params(labelsize=general_fontsize)\n",
    "    ax.set_xlabel('Recall (IoU>%1.1f)' % iou_threshold_report, fontsize=general_fontsize)\n",
    "    ax.set_ylabel('Precision (IoU>%1.1f)' % iou_threshold_report, fontsize=general_fontsize)\n",
    "    if show_fold_id:\n",
    "        ax.legend(loc='lower left', fontsize=general_fontsize)\n",
    "    format_precision_recall_plot(\n",
    "        ax, axis_markers=axis_markers, show_quadrants=show_quadrants,\n",
    "        show_grid=show_grid, minor_axis_markers=minor_axis_markers)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New implementation - By subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ckpt_folder_prefix = '20210411_bsf_update_exp1'\n",
    "# You may specify certain runs within that ckpt_folder in grid_folder_list.\n",
    "# If None then all runs are returned\n",
    "grid_folder_list = None\n",
    "# You may specify certain folds within the experiment to plot.\n",
    "# If None, then all available folds are used\n",
    "selected_folds = None\n",
    "\n",
    "# Data settings\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "dataset_params = {pkeys.FS: 200}\n",
    "load_dataset_from_ckpt = True\n",
    "\n",
    "# Evaluation settings\n",
    "evaluation_set = constants.VAL_SUBSET\n",
    "iou_threshold_report = 0.2\n",
    "\n",
    "# Plot settings\n",
    "title_fontsize = 9\n",
    "general_fontsize = 9\n",
    "marker_size = 7\n",
    "marker_alpha = 0.6\n",
    "show_subject_id = True\n",
    "group_by_subject = False\n",
    "subject_to_highlight = [14, 5]\n",
    "fold_monocolor = True\n",
    "show_fold_id = False\n",
    "show_grid = False\n",
    "show_mean = True\n",
    "show_quadrants = False\n",
    "axis_markers = np.arange(0, 1 + 0.001, 0.2)\n",
    "minor_axis_markers = np.arange(0, 1 + 0.001, 0.1)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "color_dict = get_fold_colors()\n",
    "# dataset = reader.load_dataset(\n",
    "#     dataset_name, params=dataset_params, load_checkpoint=load_dataset_from_ckpt, verbose=False)\n",
    "ckpt_folder = '%s_%s_train_%s' % (ckpt_folder_prefix, task_mode, dataset_name)\n",
    "if grid_folder_list is None:\n",
    "    experiment_path = os.path.join(RESULTS_PATH, 'predictions_%s' % dataset_name, ckpt_folder)\n",
    "    grid_folder_list = os.listdir(experiment_path)\n",
    "    grid_folder_list.sort()\n",
    "print('Grid settings to be used from %s:' % ckpt_folder)\n",
    "pprint(grid_folder_list)\n",
    "predictions_dict = {}\n",
    "for grid_folder in grid_folder_list:\n",
    "    grid_folder_complete = os.path.join(ckpt_folder, grid_folder)\n",
    "    predictions_dict[grid_folder] = reader.read_predictions_crossval(\n",
    "        grid_folder_complete, dataset, task_mode)\n",
    "if selected_folds is None:\n",
    "    selected_folds = list(predictions_dict[grid_folder_list[0]].keys())\n",
    "    selected_folds.sort()\n",
    "print(\"Folds to plot: %s\" % selected_folds)\n",
    "print(\"\")\n",
    "result_id = '%s-%s-E%d-%s' % (\n",
    "    dataset_name.split('_')[0].upper(),\n",
    "    dataset_name.split('_')[1].upper(),\n",
    "    which_expert,\n",
    "    task_mode.upper())\n",
    "for grid_folder in grid_folder_list:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=viz.DPI)\n",
    "    opt_thr_list = OPTIMAL_THR_FOR_CKPT_DICT[os.path.join(ckpt_folder, grid_folder)]\n",
    "    outputs = {'f1': [], 'miou': [], 'prec': [], 'rec': [], 'fold': [], 'subjects': []}\n",
    "    for k in selected_folds:\n",
    "        # Retrieve relevant data\n",
    "        eval_predictions = predictions_dict[grid_folder][k][evaluation_set]\n",
    "        subject_ids = eval_predictions.get_ids()\n",
    "        feed_d = FeederDataset(dataset, subject_ids, task_mode, which_expert=which_expert)\n",
    "        events_list = feed_d.get_stamps()\n",
    "        eval_predictions.set_probability_threshold(opt_thr_list[k])\n",
    "        detections_list = eval_predictions.get_stamps()\n",
    "        # Compute performance\n",
    "        iou_matching_list, _ = metrics.matching_with_list(events_list, detections_list)\n",
    "        f1_score = metrics.metric_vs_iou_macro_average(\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.F1_SCORE, collapse_values=False)\n",
    "        recall = metrics.metric_vs_iou_macro_average(\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.RECALL, collapse_values=False)\n",
    "        precision = metrics.metric_vs_iou_macro_average(\n",
    "            events_list, detections_list, [iou_threshold_report], \n",
    "            iou_matching_list=iou_matching_list, metric_name=constants.PRECISION, collapse_values=False)\n",
    "        nonzero_iou_list = [iou_matching[iou_matching > 0] for iou_matching in iou_matching_list]\n",
    "        miou_list = [np.mean(nonzero_iou) for nonzero_iou in nonzero_iou_list]\n",
    "        outputs['f1'].append(f1_score[:, 0])\n",
    "        outputs['prec'].append(precision[:, 0])\n",
    "        outputs['rec'].append(recall[:, 0])\n",
    "        outputs['miou'].append(miou_list)\n",
    "        outputs['subjects'].append(subject_ids)\n",
    "        outputs['fold'].append([k] * len(subject_ids))\n",
    "    if group_by_subject:\n",
    "        subject_ids = np.unique(np.concatenate(outputs['subjects']))\n",
    "        grouped_outputs = {\n",
    "            s: {key: [] for key in outputs}\n",
    "            for s in subject_ids}\n",
    "        for k in selected_folds:\n",
    "            for i, s in enumerate(outputs['subjects'][k]):\n",
    "                for key in outputs.keys():\n",
    "                    grouped_outputs[s][key].append(outputs[key][k][i])\n",
    "        outputs = {key: [] for key in grouped_outputs[subject_ids[0]].keys()}\n",
    "        for s in subject_ids:\n",
    "            for key in grouped_outputs[s].keys():\n",
    "                if key in ['fold', 'subjects']:\n",
    "                    # Assign to the first fold in which it appears\n",
    "                    outputs[key].append(grouped_outputs[s][key][0])\n",
    "                else:\n",
    "                    outputs[key].append(np.mean(grouped_outputs[s][key]))\n",
    "    else:\n",
    "        for key in outputs.keys():\n",
    "            outputs[key] = np.concatenate(outputs[key])\n",
    "    sorted_loc = np.argsort(outputs['fold'])\n",
    "    for key in outputs.keys():\n",
    "        outputs[key] = np.asarray(outputs[key])[sorted_loc]\n",
    "    # Plot\n",
    "    folds_shown = []\n",
    "    for i in range(len(outputs['f1'])):\n",
    "        subject_id = outputs['subjects'][i]\n",
    "        k = outputs['fold'][i]\n",
    "        if fold_monocolor:\n",
    "            color = viz.PALETTE['green'] if subject_id in subject_to_highlight else viz.PALETTE['blue']\n",
    "        else:\n",
    "            color = color_dict[k]\n",
    "        label = 'Fold %d' % k if k not in folds_shown else None\n",
    "        folds_shown.append(k)\n",
    "        ax.plot(\n",
    "            outputs['rec'][i], outputs['prec'][i], \n",
    "            color=color, linestyle='None', alpha=marker_alpha, \n",
    "            markeredgewidth=0.0, marker='o', markersize=marker_size, \n",
    "            label=label)\n",
    "        if show_subject_id:\n",
    "            if isinstance(subject_id, str):\n",
    "                single_id_to_show = int(subject_id[0] + subject_id[3:])\n",
    "                subject_id_fontsize = 3\n",
    "            else:\n",
    "                single_id_to_show = subject_id\n",
    "                subject_id_fontsize = 4\n",
    "            ax.annotate(\n",
    "                single_id_to_show, (outputs['rec'][i], outputs['prec'][i]),\n",
    "                horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                fontsize=subject_id_fontsize, color=\"w\")\n",
    "    if show_mean:\n",
    "        ax.plot(\n",
    "            np.mean(outputs['rec']), np.mean(outputs['prec']),\n",
    "            marker='o', markersize=marker_size / 2, linestyle=\"None\",\n",
    "            color=viz.GREY_COLORS[6])\n",
    "    perf_str = get_performance_string(outputs)\n",
    "    eval_str = \"%s-%s\" % (\"SUBJECT\", evaluation_set.upper())\n",
    "    ax.set_title(\n",
    "        '%s\\n%s, %s\\n%s' % (\n",
    "            grid_folder, eval_str, result_id, perf_str), \n",
    "        fontsize=title_fontsize)\n",
    "    ax.tick_params(labelsize=general_fontsize)\n",
    "    ax.set_xlabel('Recall (IoU>%1.1f)' % iou_threshold_report, fontsize=general_fontsize)\n",
    "    ax.set_ylabel('Precision (IoU>%1.1f)' % iou_threshold_report, fontsize=general_fontsize)\n",
    "    if show_fold_id:\n",
    "        ax.legend(loc='lower left', fontsize=general_fontsize)\n",
    "    format_precision_recall_plot(\n",
    "        ax, axis_markers=axis_markers, show_quadrants=show_quadrants,\n",
    "        show_grid=show_grid, minor_axis_markers=minor_axis_markers)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
