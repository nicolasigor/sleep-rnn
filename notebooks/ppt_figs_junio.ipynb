{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm as CM\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.data.loader import load_dataset, RefactorUnpickler\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.common import constants, pkeys\n",
    "\n",
    "SEED_LIST = [123, 234, 345, 456]\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "COMPARISON_PATH = os.path.join(project_root, 'resources', 'comparison_data')\n",
    "DPI = 200\n",
    "CUSTOM_COLOR = {'red': '#c62828', 'grey': '#455a64', 'blue': '#0277bd', 'green': '#43a047'} \n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: BSF Mixed\n",
    "ckpt_folder = os.path.join(\n",
    "        '20190605_grid_v15_v16_n2_train_mass_ss',\n",
    "        'v15_timef_64_128_256_cwtf_32_32_fb_0.5')\n",
    "id_try = 0\n",
    "optimal_thr_list = [0.46, 0.52, 0.62, 0.42]\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "task_mode = constants.N2_RECORD\n",
    "which_expert = 1\n",
    "\n",
    "# ---- Load stuff\n",
    "scaling_optimal_thr = optimal_thr_list[id_try]\n",
    "prediction_folder = os.path.join(RESULTS_PATH, 'scaling_results', ckpt_folder, 'seed%d' % id_try)\n",
    "filenames = os.listdir(prediction_folder)\n",
    "filenames.sort()\n",
    "scale_list = [float(single_filename[-8:-4]) for single_filename in filenames]\n",
    "scale_list = np.array(scale_list)\n",
    "predictions_test = {}\n",
    "for scale, file in zip(scale_list, filenames):\n",
    "    print('Loading scale %1.2f on file %s' % (scale, file))\n",
    "    with open(os.path.join(prediction_folder, file), 'rb') as handle:\n",
    "        predictions_test[scale] = RefactorUnpickler(handle).load()\n",
    "print('Optimal thr for scaling experiment %1.2f (seed %d)' % (scaling_optimal_thr, id_try))\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "test_ids = dataset.test_ids\n",
    "data_test = FeederDataset(\n",
    "    dataset, test_ids, task_mode, which_expert=which_expert)\n",
    "this_events = data_test.get_stamps()\n",
    "\n",
    "# Measure performance\n",
    "print('Measuring performance', flush=True)\n",
    "iou_thr = 0.3\n",
    "scaling_stats = {}\n",
    "for scale in scale_list:\n",
    "    prediction_set = predictions_test[scale]\n",
    "    prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "    this_detections = prediction_set.get_stamps()\n",
    "    scaling_stats[scale] = [\n",
    "        metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "        for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=DPI)\n",
    "\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "markersize = 20\n",
    "alpha = 0.4\n",
    "text_space = 0.01\n",
    "axis_lims = [0.5, 1.0]\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "\n",
    "CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax.clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "marker_list = ['o', 's', '<', 'd']\n",
    "for i in range(4):\n",
    "    subject_id = test_ids[i]\n",
    "    this_recall = [scaling_stats[scale][i]['recall'] for scale in scale_list]\n",
    "    this_precision = [scaling_stats[scale][i]['precision'] for scale in scale_list]\n",
    "    scat = ax.scatter(this_recall, this_precision, c=scale_list, marker=marker_list[i], cmap='viridis', zorder=10, s=markersize)\n",
    "    ax.plot(this_recall, this_precision, linewidth=1, color='k', zorder=7)\n",
    "    recall_at_1 = scaling_stats[1.0][i]['recall']\n",
    "    precision_at_1 = scaling_stats[1.0][i]['precision']\n",
    "    ax.scatter(recall_at_1, precision_at_1, color='k', marker=marker_list[i], s=2*markersize, zorder=15, label='S%02d with scale=1' % subject_id)\n",
    "ax.axis('square')\n",
    "ax.set_title('PR by subject after scaling input signal\\n Scaling step %1.2f, IoU$>$%1.1f, %s' % (scale_list[1]-scale_list[0], iou_thr, dataset_name.upper()), fontsize=title_fontsize)\n",
    "ax.set_xlabel('Recall', fontsize=other_fontsize)\n",
    "ax.set_ylabel('Precision', fontsize=other_fontsize)\n",
    "ax.set_xlim(axis_lims)\n",
    "ax.set_ylim(axis_lims)\n",
    "\n",
    "lg = ax.legend(loc='lower left', labelspacing=1, fontsize=other_fontsize)\n",
    "for lh in lg.legendHandles:\n",
    "    lh.set_alpha(1.0)\n",
    "\n",
    "ax.tick_params(labelsize=other_fontsize)\n",
    "ax.grid()\n",
    "cbar = plt.colorbar(scat)\n",
    "cbar.ax.tick_params(labelsize=other_fontsize) \n",
    "cbar.set_label('Scaling factor', fontsize=other_fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Results - Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "ckpt_dict['v11'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.56, 0.54, 0.54, 0.48],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.6, 0.58, 0.5, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.48, 0.6, 0.68, 0.6],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.54, 0.62, 0.56, 0.58],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.46, 0.5, 0.5, 0.44],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.44, 0.52, 0.48, 0.46],\n",
    "}\n",
    "ckpt_dict['v12'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.66, 0.4, 0.56, 0.52],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.58, 0.46, 0.38, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.46, 0.52, 0.48, 0.54],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.52, 0.58, 0.52, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.46, 0.46, 0.48, 0.5],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.48, 0.48, 0.48, 0.46],\n",
    "}\n",
    "ckpt_dict['v15'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.62, 0.62, 0.4, 0.42],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.64, 0.54, 0.6, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.68, 0.54, 0.66, 0.52], \n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.72, 0.54, 0.44, 0.56],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.48, 0.52, 0.5, 0.5],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.48, 0.52, 0.54, 0.46],\n",
    "}\n",
    "task_mode = constants.N2_RECORD\n",
    "which_expert = 1\n",
    "dataset_name = constants.INTA_SS_NAME\n",
    "n_seeds = 4\n",
    "model_name_list = ['v12', 'v11', 'v15']\n",
    "model_name_display_dict = {\n",
    "    'v11': 'Time Domain Model',\n",
    "    'v12': 'CWT Domain Model',\n",
    "    'v15': 'Combined Model'\n",
    "}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "test_ids = dataset.test_ids\n",
    "data_test = FeederDataset(\n",
    "    dataset, test_ids, task_mode, which_expert=which_expert)\n",
    "this_events = data_test.get_stamps()\n",
    "\n",
    "predictions_dict = {}\n",
    "optimal_thr_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    predictions_dict[model_name] = []\n",
    "    optimal_thr_dict[model_name] = []\n",
    "    this_ckpt_dict = ckpt_dict[model_name]\n",
    "    useful_keys = [key for key in this_ckpt_dict.keys() if dataset_name in key]\n",
    "    # Read predictions\n",
    "    for key in useful_keys:\n",
    "        for k in range(n_seeds):\n",
    "            ckpt_path = os.path.abspath(os.path.join(RESULTS_PATH, 'predictions_%s' % dataset_name, key, 'seed%d' % k))\n",
    "            filename = os.path.join(ckpt_path, 'prediction_%s_test.pkl' % task_mode)\n",
    "            with open(filename, 'rb') as handle:\n",
    "                this_pred = RefactorUnpickler(handle).load()\n",
    "            predictions_dict[model_name].append(this_pred)\n",
    "            optimal_thr_dict[model_name].append(this_ckpt_dict[key][k])\n",
    "print('Predictions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure AF1 and F1 vs IoU\n",
    "print('Measuring F1')\n",
    "af1_dict = {}\n",
    "f1_vs_iou_dict = {}\n",
    "iou_list = np.arange(1, 10) * 0.1\n",
    "for model_name in model_name_list:\n",
    "    print('Measuring model %s' % model_name, flush=True)\n",
    "    af1_dict[model_name] = []\n",
    "    f1_vs_iou_dict[model_name] = []\n",
    "    for optimal_thr, predictions in zip(optimal_thr_dict[model_name], predictions_dict[model_name]):\n",
    "        predictions.set_probability_threshold(optimal_thr)\n",
    "        this_detections = predictions.get_stamps()\n",
    "        af1_at_thr = metrics.average_metric_with_list(this_events, this_detections, verbose=False)\n",
    "        af1_dict[model_name].append(af1_at_thr)\n",
    "        this_f1_vs_iou = metrics.metric_vs_iou_with_list(this_events, this_detections, iou_list)\n",
    "        f1_vs_iou_dict[model_name].append(this_f1_vs_iou)\n",
    "# Mean results\n",
    "mean_af1_dict = {}\n",
    "mean_f1_vs_iou_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    mean_af1 = np.mean(af1_dict[model_name])\n",
    "    std_af1 = np.std(af1_dict[model_name])\n",
    "    \n",
    "    mean_curve = np.stack(f1_vs_iou_dict[model_name], axis=1).mean(axis=1)\n",
    "    std_curve = np.stack(f1_vs_iou_dict[model_name], axis=1).std(axis=1)\n",
    "    \n",
    "    mean_af1_dict[model_name] = (mean_af1, std_af1)\n",
    "    mean_f1_vs_iou_dict[model_name] = (mean_curve, std_curve)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_precision = 2\n",
    "print('Test AF1 for %s' % dataset_name)\n",
    "for model_name in model_name_list:\n",
    "    print('%s: %1.2f +- %1.2f' \n",
    "          % (\n",
    "              model_name_display_dict[model_name].ljust(20), \n",
    "              np.round(100*mean_af1_dict[model_name][0], decimals=round_precision), \n",
    "              np.round(100*mean_af1_dict[model_name][1], decimals=round_precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "color_list = {'v15': CUSTOM_COLOR['red'] , 'expert': CUSTOM_COLOR['grey'], 'v11': CUSTOM_COLOR['blue'], 'v12': CUSTOM_COLOR['green']}\n",
    "linewidth = 1.5\n",
    "markersize = 7\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "show_std = True\n",
    "\n",
    "# -------------------- P L O T ----------------------    \n",
    "\n",
    "# Comparison data\n",
    "compare_expert = dataset.event_name == constants.SPINDLE\n",
    "if compare_expert:\n",
    "    expert_f1_curve_mean = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_mean.csv'), delimiter=',')\n",
    "    expert_f1_curve_std = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_std.csv'), delimiter=',')\n",
    "    expert_f1_curve_mean = expert_f1_curve_mean[1:, :]\n",
    "    expert_f1_curve_std = expert_f1_curve_std[1:, :]\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "\n",
    "# Complete plot\n",
    "if compare_expert:\n",
    "    ax.plot(expert_f1_curve_mean[:, 0], expert_f1_curve_mean[:, 1], linewidth=linewidth, \n",
    "               label='Expert Performance\\nPrivate Dataset\\nWarby et al. 2014', color=color_list['expert'])\n",
    "    ax.plot(expert_f1_curve_mean[:, 0], expert_f1_curve_mean[:, 1], linestyle='none', \n",
    "               markersize=markersize, marker='.', color=color_list['expert'])\n",
    "    ax.fill_between(\n",
    "        expert_f1_curve_mean[:, 0], \n",
    "        expert_f1_curve_mean[:, 1] - expert_f1_curve_std[:, 1], \n",
    "        expert_f1_curve_mean[:, 1] + expert_f1_curve_std[:, 1], \n",
    "        alpha=alpha, facecolor=color_list['expert'])\n",
    "\n",
    "for model_name in model_name_list:\n",
    "    this_name = model_name_display_dict[model_name]\n",
    "    ax.plot(iou_list, mean_f1_vs_iou_dict[model_name][0], \n",
    "               linewidth=linewidth,\n",
    "               label='%s' % this_name, color=color_list[model_name])\n",
    "    ax.plot(iou_list, mean_f1_vs_iou_dict[model_name][0], \n",
    "               linestyle='none', markersize=markersize, marker='.', \n",
    "               color=color_list[model_name])\n",
    "    if show_std:\n",
    "        ax.fill_between(iou_list, \n",
    "                        mean_f1_vs_iou_dict[model_name][0] - mean_f1_vs_iou_dict[model_name][1], \n",
    "                        mean_f1_vs_iou_dict[model_name][0] + mean_f1_vs_iou_dict[model_name][1], alpha=alpha, facecolor=color_list[model_name])\n",
    "\n",
    "\n",
    "    \n",
    "ax.set_title('Test Performance (%s)' % dataset_name.upper(), fontsize=title_fontsize)\n",
    "ax.set_xlim([0.1 - 0.02, 0.9 + 0.02])\n",
    "ax.set_ylim([0.1 - 0.02, 0.9 + 0.02])\n",
    "# ax.set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax.set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax.tick_params(labelsize=other_fontsize)\n",
    "ax.set_xlabel('IoU Threshold', fontsize=other_fontsize)\n",
    "ax.set_ylabel('F1-score', fontsize=other_fontsize)\n",
    "ax.yaxis.grid()\n",
    "ax.legend(loc='lower left', labelspacing=1.5, fontsize=other_fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR\n",
    "iou_thr = 0.3\n",
    "res_thr = 0.02\n",
    "start_thr = 0.1\n",
    "end_thr = 0.9\n",
    "n_thr = int(np.round((end_thr - start_thr) / res_thr + 1))\n",
    "thr_list = np.array([start_thr + res_thr * i for i in range(n_thr)])\n",
    "\n",
    "pr_curve_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    print('Measuring model %s' % model_name, flush=True)\n",
    "    pr_curve_dict[model_name] = []\n",
    "    for k, predictions in enumerate(predictions_dict[model_name]):\n",
    "        pr_curve_dict[model_name].append(np.zeros((n_thr, 2)))\n",
    "        for i, thr in enumerate(thr_list):\n",
    "            predictions.set_probability_threshold(thr)\n",
    "            this_detections = predictions.get_stamps()\n",
    "            this_stats = [metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "                        for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "            this_recall = np.mean([m[constants.RECALL] for m in this_stats])\n",
    "            this_precision = np.mean([m[constants.PRECISION] for m in this_stats])\n",
    "            pr_curve_dict[model_name][k][i, 0] = this_recall\n",
    "            pr_curve_dict[model_name][k][i, 1] = this_precision\n",
    "print('Done', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "markersize = 6\n",
    "alpha = 0.3\n",
    "text_space = 0.01\n",
    "compare_expert = dataset.event_name == constants.SPINDLE\n",
    "axis_lims = [0.5, 1.0]\n",
    "color_list = {'v15': CUSTOM_COLOR['red'] , 'expert': CUSTOM_COLOR['grey'], 'v11': CUSTOM_COLOR['blue'], 'v12': CUSTOM_COLOR['green']}\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax.clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "    \n",
    "if compare_expert:\n",
    "    expert_pr_mean = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_pr_expert_mean.csv'), delimiter=',')\n",
    "    ax.scatter(expert_pr_mean[0], expert_pr_mean[1], s=50, c=color_list['expert'], zorder=10, label='Expert Performance\\nPrivate Dataset\\nWarby et al. 2014')\n",
    "    \n",
    "for model_name in model_name_list:\n",
    "    mean_precision = []\n",
    "    mean_recall = []\n",
    "    for k, pr_curve in enumerate(pr_curve_dict[model_name]):\n",
    "        if k == 0:\n",
    "            label = model_name_display_dict[model_name]\n",
    "        else:\n",
    "            label = None\n",
    "        thr_run = optimal_thr_dict[model_name][k]\n",
    "        chosen_thr_idx = np.where(np.isclose(thr_list, thr_run))[0].item()\n",
    "        ax.plot(pr_curve[:, 0], pr_curve[:, 1], linewidth=1, color=color_list[model_name], zorder=7, alpha=alpha, label=label)\n",
    "        mean_precision.append(pr_curve[chosen_thr_idx, 1])\n",
    "        mean_recall.append(pr_curve[chosen_thr_idx, 0])\n",
    "    mean_precision = np.mean(mean_precision)\n",
    "    mean_recall = np.mean(mean_recall)\n",
    "    ax.scatter(mean_recall, mean_precision, s=50, c=color_list[model_name], zorder=10)\n",
    "\n",
    "ax.set_title('Test PR Curve with IoU$>$%1.1f (%s)' % (iou_thr, dataset_name.upper()), fontsize=10)\n",
    "ax.set_xlabel('Recall', fontsize=8.5)\n",
    "ax.set_ylabel('Precision', fontsize=8.5)\n",
    "ax.set_xlim(axis_lims)\n",
    "ax.set_ylim(axis_lims)\n",
    "\n",
    "lg = ax.legend(loc='lower left', labelspacing=1, fontsize=6.5)\n",
    "for lh in lg.legendHandles:\n",
    "    lh.set_alpha(1.0)\n",
    "\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
