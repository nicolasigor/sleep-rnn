{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm as CM\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.data.loader import load_dataset\n",
    "from sleeprnn.helpers.reader import RefactorUnpickler\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.detection.postprocessor import PostProcessor\n",
    "from sleeprnn.common import constants, pkeys\n",
    "\n",
    "SEED_LIST = [123, 234, 345, 456]\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "COMPARISON_PATH = os.path.join(project_root, 'resources', 'comparison_data')\n",
    "DPI = 200\n",
    "CUSTOM_COLOR = {'red': '#c62828', 'grey': '#455a64', 'blue': '#0277bd', 'green': '#43a047', 'dark':'#1b2631'} \n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/scaling_results/20190617_grid_normalization_n2_train_mass_ss/norm_global/seed0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4cdaa44e72c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscaling_optimal_thr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_thr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_try\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprediction_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaling_results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seed%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mid_try\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/scaling_results/20190617_grid_normalization_n2_train_mass_ss/norm_global/seed0'"
     ]
    }
   ],
   "source": [
    "# Model: BSF Mixed\n",
    "ckpt_folder = os.path.join(\n",
    "        '20190617_grid_normalization_n2_train_mass_ss',\n",
    "        'norm_global')\n",
    "id_try = 0\n",
    "optimal_thr_list = [0.58, 0.42, 0.4, 0.5]\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "task_mode = constants.N2_RECORD\n",
    "which_expert = 1\n",
    "set_name = constants.TRAIN_SUBSET\n",
    "\n",
    "# ---- Load stuff\n",
    "scaling_optimal_thr = optimal_thr_list[id_try]\n",
    "prediction_folder = os.path.join(RESULTS_PATH, 'scaling_results', ckpt_folder, 'seed%d' % id_try)\n",
    "filenames = os.listdir(prediction_folder)\n",
    "filenames = [fn for fn in filenames if set_name in fn]\n",
    "filenames.sort()\n",
    "scale_list = [float(single_filename[-8:-4]) for single_filename in filenames]\n",
    "scale_list = np.array(scale_list)\n",
    "predictions_dict = {}\n",
    "for scale, file in zip(scale_list, filenames):\n",
    "    print('Loading scale %1.2f on file %s' % (scale, file))\n",
    "    with open(os.path.join(prediction_folder, file), 'rb') as handle:\n",
    "        predictions_dict[scale] = RefactorUnpickler(handle).load()\n",
    "print('Optimal thr for scaling experiment %1.2f (seed %d)' % (scaling_optimal_thr, id_try))\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "ids_dict = {\n",
    "    constants.TRAIN_SUBSET: dataset.train_ids,\n",
    "    constants.TEST_SUBSET: dataset.test_ids\n",
    "}\n",
    "\n",
    "data_inference = FeederDataset(\n",
    "    dataset, ids_dict[set_name], task_mode, which_expert=which_expert)\n",
    "this_events = data_inference.get_stamps()\n",
    "\n",
    "# Measure performance\n",
    "print('Measuring performance', flush=True)\n",
    "iou_thr = 0.3\n",
    "scaling_stats = {}\n",
    "for scale in scale_list:\n",
    "    prediction_set = predictions_dict[scale]\n",
    "    prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "    this_detections = prediction_set.get_stamps()\n",
    "    scaling_stats[scale] = [\n",
    "        metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "        for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "if set_name == constants.TEST_SUBSET:\n",
    "    n_plots = 1\n",
    "else:\n",
    "    n_plots = 3\n",
    "\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "markersize = 20\n",
    "alpha = 0.4\n",
    "text_space = 0.01\n",
    "axis_lims = [0.5, 1.0]\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "\n",
    "for id_plot in range(n_plots):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=DPI)\n",
    "    CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "    ax.clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "    marker_list = ['o', 's', '<', 'd']\n",
    "    end_range = min(4 * (id_plot+1), len(ids_dict[set_name]))\n",
    "    for i_inner, i in enumerate(range(id_plot*4, end_range)):\n",
    "        subject_id = ids_dict[set_name][i]\n",
    "        this_recall = [scaling_stats[scale][i]['recall'] for scale in scale_list]\n",
    "        this_precision = [scaling_stats[scale][i]['precision'] for scale in scale_list]\n",
    "        scat = ax.scatter(this_recall, this_precision, c=scale_list, marker=marker_list[i_inner], cmap='viridis', zorder=10, s=markersize)\n",
    "        ax.plot(this_recall, this_precision, linewidth=1, color='k', zorder=7)\n",
    "        recall_at_1 = scaling_stats[1.0][i]['recall']\n",
    "        precision_at_1 = scaling_stats[1.0][i]['precision']\n",
    "        ax.scatter(recall_at_1, precision_at_1, color='k', marker=marker_list[i_inner], s=2*markersize, zorder=15, label='S%02d with scale=1' % subject_id)\n",
    "    ax.axis('square')\n",
    "    ax.set_title('PR by subject after scaling input signal\\n Scaling step %1.2f, IoU$>$%1.1f, %s' % (scale_list[1]-scale_list[0], iou_thr, dataset_name.upper()), fontsize=title_fontsize)\n",
    "    ax.set_xlabel('Recall', fontsize=other_fontsize)\n",
    "    ax.set_ylabel('Precision', fontsize=other_fontsize)\n",
    "    ax.set_xlim(axis_lims)\n",
    "    ax.set_ylim(axis_lims)\n",
    "\n",
    "    lg = ax.legend(loc='lower left', labelspacing=1, fontsize=other_fontsize)\n",
    "    for lh in lg.legendHandles:\n",
    "        lh.set_alpha(1.0)\n",
    "\n",
    "    ax.tick_params(labelsize=other_fontsize)\n",
    "    ax.grid()\n",
    "    cbar = plt.colorbar(scat)\n",
    "    cbar.ax.tick_params(labelsize=other_fontsize) \n",
    "    cbar.set_label('Scaling factor', fontsize=other_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current performance\n",
    "prediction_set = predictions_dict[1.0]\n",
    "prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "this_detections = prediction_set.get_stamps()\n",
    "af1_list = [metrics.average_metric(y, y_hat, verbose=False, metric_name=constants.F1_SCORE) for (y, y_hat) in zip(this_events, this_detections)]\n",
    "ap_list = [metrics.average_metric(y, y_hat, verbose=False, metric_name=constants.PRECISION) for (y, y_hat) in zip(this_events, this_detections)]\n",
    "ar_list = [metrics.average_metric(y, y_hat, verbose=False, metric_name=constants.RECALL) for (y, y_hat) in zip(this_events, this_detections)]\n",
    "\n",
    "print('AF1: %1.2f +- %1.2f' % (np.mean(af1_list)*100, np.std(af1_list)*100))\n",
    "print('AP : %1.2f +- %1.2f' % (np.mean(ap_list)*100, np.std(ap_list)*100))\n",
    "print('AR : %1.2f +- %1.2f' % (np.mean(ar_list)*100, np.std(ar_list)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search optimal scale\n",
    "best_scale_dict = {}\n",
    "for subject_id in ids_dict[set_name]:\n",
    "    subj_af1 = []\n",
    "    subj_events = data_inference.get_subject_stamps(subject_id=subject_id)\n",
    "    for scale in scale_list:\n",
    "        prediction_set = predictions_dict[scale]\n",
    "        prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "        subj_detections = prediction_set.get_subject_stamps(subject_id=subject_id)\n",
    "        this_af1 = metrics.average_metric(subj_events, subj_detections, verbose=False)\n",
    "        subj_af1.append(this_af1)\n",
    "    max_idx = np.argmax(subj_af1).item()\n",
    "    max_af1 = subj_af1[max_idx]\n",
    "    max_scale = scale_list[max_idx]\n",
    "    print('S%02d, best scale %1.2f with AF1 %1.2f' % (subject_id, max_scale, max_af1))\n",
    "    best_scale_dict[subject_id] = max_scale\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best scale performance\n",
    "best_af1_list = []\n",
    "best_ap_list = []\n",
    "best_ar_list = []\n",
    "for subject_id in ids_dict[set_name]:\n",
    "    subj_events = data_inference.get_subject_stamps(subject_id=subject_id)\n",
    "    prediction_set = predictions_dict[best_scale_dict[subject_id]]\n",
    "    prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "    subj_detections = prediction_set.get_subject_stamps(subject_id=subject_id)\n",
    "    this_af1 = metrics.average_metric(subj_events, subj_detections, verbose=False)\n",
    "    this_ap = metrics.average_metric(subj_events, subj_detections, verbose=False, metric_name=constants.PRECISION)\n",
    "    this_ar = metrics.average_metric(subj_events, subj_detections, verbose=False, metric_name=constants.RECALL)\n",
    "    best_af1_list.append(this_af1)\n",
    "    best_ap_list.append(this_ap)\n",
    "    best_ar_list.append(this_ar)\n",
    "\n",
    "print('AF1: %1.2f +- %1.2f' % (np.mean(best_af1_list)*100, np.std(best_af1_list)*100))\n",
    "print('AP : %1.2f +- %1.2f' % (np.mean(best_ap_list)*100, np.std(best_ap_list)*100))\n",
    "print('AR : %1.2f +- %1.2f' % (np.mean(best_ar_list)*100, np.std(best_ar_list)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject colors\n",
    "subject_color_dict = {}\n",
    "train_split, val_split = utils.split_ids_list(dataset.train_ids, seed=SEED_LIST[id_try])\n",
    "for subject_id in ids_dict[set_name]:\n",
    "    if subject_id in dataset.test_ids:\n",
    "        subject_color_dict[subject_id] = CUSTOM_COLOR['red']\n",
    "    else:\n",
    "        if subject_id in train_split:\n",
    "            subject_color_dict[subject_id] = CUSTOM_COLOR['green']\n",
    "        else:\n",
    "            subject_color_dict[subject_id] = CUSTOM_COLOR['grey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison before / after\n",
    "show_ids = True\n",
    "\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "markersize = 20\n",
    "alpha = 0.4\n",
    "text_space = 0.01\n",
    "axis_lims = [0.5, 1.0]\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, dpi=200, figsize=(6, 3))\n",
    "\n",
    "CS = ax[0].contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax[0].clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "CS = ax[1].contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax[1].clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "[single_ax.set_xlabel('Recall', fontsize=other_fontsize) for single_ax in ax]\n",
    "[single_ax.set_ylabel('Precision', fontsize=other_fontsize) for single_ax in ax]\n",
    "[single_ax.set_xlim(axis_lims) for single_ax in ax]\n",
    "[single_ax.set_ylim(axis_lims) for single_ax in ax]\n",
    "[single_ax.tick_params(labelsize=other_fontsize) for single_ax in ax]\n",
    "[single_ax.grid() for single_ax in ax]\n",
    "\n",
    "# Original scale\n",
    "ax[0].set_title('Original scale (IoU > %1.1f)' % iou_thr, fontsize=title_fontsize)\n",
    "for i in range(len(ids_dict[set_name])):\n",
    "    subject_id = ids_dict[set_name][i]\n",
    "    this_recall = scaling_stats[1.0][i]['recall']\n",
    "    this_precision = scaling_stats[1.0][i]['precision']\n",
    "    ax[0].scatter(this_recall, this_precision, c=subject_color_dict[subject_id], marker='s', s=markersize, zorder=10)\n",
    "    if show_ids:\n",
    "        ax[0].annotate(\n",
    "            subject_id, \n",
    "            (this_recall+text_space, this_precision+text_space), \n",
    "            fontsize=7, color=CUSTOM_COLOR['dark'], zorder=20)\n",
    "\n",
    "# Best scale\n",
    "ax[1].set_title('Best scale (IoU > %1.1f)' % iou_thr, fontsize=title_fontsize)\n",
    "for i in range(len(ids_dict[set_name])):\n",
    "    subject_id = ids_dict[set_name][i]\n",
    "    this_recall = scaling_stats[best_scale_dict[subject_id]][i]['recall']\n",
    "    this_precision = scaling_stats[best_scale_dict[subject_id]][i]['precision']\n",
    "    ax[1].scatter(this_recall, this_precision, c=subject_color_dict[subject_id], marker='s', s=markersize, zorder=10)\n",
    "    if show_ids:\n",
    "        ax[1].annotate(\n",
    "            subject_id, \n",
    "            (this_recall+text_space, this_precision+text_space), \n",
    "            fontsize=7, color=CUSTOM_COLOR['dark'], zorder=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Compute performance\n",
    "f1_vs_iou_subject_dict = {}\n",
    "pre_vs_iou_subject_dict = {}\n",
    "rec_vs_iou_subject_dict = {}\n",
    "iou_list = np.arange(21) * 0.05\n",
    "\n",
    "# original performance\n",
    "print('Processing original performance', flush=True)\n",
    "f1_vs_iou_dict = {}\n",
    "pre_vs_iou_dict = {}\n",
    "rec_vs_iou_dict = {}\n",
    "for subject_id in ids_dict[set_name]:\n",
    "    subj_events = data_inference.get_subject_stamps(subject_id=subject_id)\n",
    "    prediction_set = predictions_dict[1.0]\n",
    "    prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "    subj_detections = prediction_set.get_subject_stamps(subject_id=subject_id)\n",
    "    this_precision = metrics.metric_vs_iou(subj_events, subj_detections, iou_list, metric_name=constants.PRECISION)\n",
    "    this_recall = metrics.metric_vs_iou(subj_events, subj_detections, iou_list, metric_name=constants.RECALL)\n",
    "    this_f1 = 2 * this_precision * this_recall / (this_precision + this_recall + 1e-8)\n",
    "    f1_vs_iou_dict[subject_id] = this_f1\n",
    "    pre_vs_iou_dict[subject_id] = this_precision\n",
    "    rec_vs_iou_dict[subject_id] = this_recall\n",
    "print('Done', flush=True)\n",
    "\n",
    "# best performance\n",
    "print('Processing best scale performance', flush=True)\n",
    "best_f1_vs_iou_dict = {}\n",
    "best_pre_vs_iou_dict = {}\n",
    "best_rec_vs_iou_dict = {}\n",
    "for subject_id in ids_dict[set_name]:\n",
    "    subj_events = data_inference.get_subject_stamps(subject_id=subject_id)\n",
    "    prediction_set = predictions_dict[best_scale_dict[subject_id]]\n",
    "    prediction_set.set_probability_threshold(scaling_optimal_thr)\n",
    "    subj_detections = prediction_set.get_subject_stamps(subject_id=subject_id)\n",
    "    this_precision = metrics.metric_vs_iou(subj_events, subj_detections, iou_list, metric_name=constants.PRECISION)\n",
    "    this_recall = metrics.metric_vs_iou(subj_events, subj_detections, iou_list, metric_name=constants.RECALL)\n",
    "    this_f1 = 2 * this_precision * this_recall / (this_precision + this_recall + 1e-8)\n",
    "    best_f1_vs_iou_dict[subject_id] = this_f1\n",
    "    best_pre_vs_iou_dict[subject_id] = this_precision\n",
    "    best_rec_vs_iou_dict[subject_id] = this_recall\n",
    "print('Done', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_list = ['.', 's', '^', 'x', '*', 'd', 'v', 'p', '<', '>', '+']\n",
    "linewidth_model = 1\n",
    "markersize_model = 3\n",
    "\n",
    "# -------------------- P L O T ----------------------    \n",
    "    \n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 8), dpi=DPI)\n",
    "\n",
    "# F1\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[0, 0].plot(iou_list, f1_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "ax[0, 0].set_title('Original scale', fontsize=title_fontsize)\n",
    "ax[0, 0].set_xlim([0, 1])\n",
    "ax[0, 0].set_ylim([0, 1])\n",
    "ax[0, 0].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 0].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 0].tick_params(labelsize=8.5)\n",
    "ax[0, 0].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[0, 0].set_ylabel('F1-score', fontsize=8.5)\n",
    "ax[0, 0].yaxis.grid()\n",
    "ax[0, 0].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "# Precision\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[0, 1].plot(iou_list, pre_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "ax[0, 1].set_title('Original scale', fontsize=title_fontsize)\n",
    "ax[0, 1].set_xlim([0, 1])\n",
    "ax[0, 1].set_ylim([0, 1])\n",
    "ax[0, 1].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 1].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 1].tick_params(labelsize=8.5)\n",
    "ax[0, 1].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[0, 1].set_ylabel('Precision', fontsize=8.5)\n",
    "ax[0, 1].yaxis.grid()\n",
    "ax[0, 1].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "# Recall\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[0, 2].plot(iou_list, rec_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "    \n",
    "ax[0, 2].set_title('Original scale', fontsize=title_fontsize)\n",
    "ax[0, 2].set_xlim([0, 1])\n",
    "ax[0, 2].set_ylim([0, 1])\n",
    "ax[0, 2].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 2].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[0, 2].tick_params(labelsize=8.5)\n",
    "ax[0, 2].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[0, 2].set_ylabel('Recall', fontsize=8.5)\n",
    "ax[0, 2].yaxis.grid()\n",
    "ax[0, 2].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "# Best scale\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[1, 0].plot(iou_list, best_f1_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "ax[1, 0].set_title('Best scale', fontsize=title_fontsize)\n",
    "ax[1, 0].set_xlim([0, 1])\n",
    "ax[1, 0].set_ylim([0, 1])\n",
    "ax[1, 0].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 0].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 0].tick_params(labelsize=8.5)\n",
    "ax[1, 0].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[1, 0].set_ylabel('F1-score', fontsize=8.5)\n",
    "ax[1, 0].yaxis.grid()\n",
    "ax[1, 0].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "# Precision\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[1, 1].plot(iou_list, best_pre_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "ax[1, 1].set_title('Best scale', fontsize=title_fontsize)\n",
    "ax[1, 1].set_xlim([0, 1])\n",
    "ax[1, 1].set_ylim([0, 1])\n",
    "ax[1, 1].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 1].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 1].tick_params(labelsize=8.5)\n",
    "ax[1, 1].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[1, 1].set_ylabel('Precision', fontsize=8.5)\n",
    "ax[1, 1].yaxis.grid()\n",
    "ax[1, 1].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "# Recall\n",
    "for i, subject_id in enumerate(ids_dict[set_name]):\n",
    "    ax[1, 2].plot(iou_list, best_rec_vs_iou_dict[subject_id], \n",
    "               linewidth=linewidth_model, markersize=markersize_model, marker=marker_list[i], \n",
    "               label='S%02d' % subject_id, color=subject_color_dict[subject_id])\n",
    "\n",
    "    \n",
    "ax[1, 2].set_title('Best scale', fontsize=title_fontsize)\n",
    "ax[1, 2].set_xlim([0, 1])\n",
    "ax[1, 2].set_ylim([0, 1])\n",
    "ax[1, 2].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 2].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[1, 2].tick_params(labelsize=8.5)\n",
    "ax[1, 2].set_xlabel('IoU Threshold', fontsize=8.5)\n",
    "ax[1, 2].set_ylabel('Recall', fontsize=8.5)\n",
    "ax[1, 2].yaxis.grid()\n",
    "ax[1, 2].legend(loc='lower left', fontsize=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Results - Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "ckpt_dict['v11'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.56, 0.54, 0.54, 0.48],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.6, 0.58, 0.5, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.48, 0.6, 0.68, 0.6],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.54, 0.62, 0.56, 0.58],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v11_tf_64-128-256_cwtf_None-None/rep0'): [0.46, 0.5, 0.5, 0.44],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v11_tf_64-128-256_cwtf_None-None/rep1'): [0.44, 0.52, 0.48, 0.46],\n",
    "}\n",
    "ckpt_dict['v12'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.66, 0.4, 0.56, 0.52],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.58, 0.46, 0.38, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.46, 0.52, 0.48, 0.54],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.52, 0.58, 0.52, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v12_tf_None-None-None_cwtf_32-64/rep0'): [0.46, 0.46, 0.48, 0.5],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v12_tf_None-None-None_cwtf_32-64/rep1'): [0.48, 0.48, 0.48, 0.46],\n",
    "}\n",
    "ckpt_dict['v15'] = {\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.62, 0.62, 0.4, 0.42],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_ss', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.64, 0.54, 0.6, 0.46],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.68, 0.54, 0.66, 0.52], \n",
    "    os.path.join('20190608_bsf_ablation_n2_train_mass_kc', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.72, 0.54, 0.44, 0.56],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v15_tf_64-128-256_cwtf_32-32/rep0'): [0.48, 0.52, 0.5, 0.5],\n",
    "    os.path.join('20190608_bsf_ablation_n2_train_inta_ss', 'v15_tf_64-128-256_cwtf_32-32/rep1'): [0.48, 0.52, 0.54, 0.46],\n",
    "}\n",
    "task_mode = constants.N2_RECORD\n",
    "which_expert = 1\n",
    "dataset_name = constants.INTA_SS_NAME\n",
    "n_seeds = 4\n",
    "model_name_list = ['v12', 'v11', 'v15']\n",
    "model_name_display_dict = {\n",
    "    'v11': 'Time Domain Model',\n",
    "    'v12': 'CWT Domain Model',\n",
    "    'v15': 'Combined Model'\n",
    "}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "test_ids = dataset.test_ids\n",
    "data_test = FeederDataset(\n",
    "    dataset, test_ids, task_mode, which_expert=which_expert)\n",
    "this_events = data_test.get_stamps()\n",
    "\n",
    "predictions_dict = {}\n",
    "optimal_thr_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    predictions_dict[model_name] = []\n",
    "    optimal_thr_dict[model_name] = []\n",
    "    this_ckpt_dict = ckpt_dict[model_name]\n",
    "    useful_keys = [key for key in this_ckpt_dict.keys() if dataset_name in key]\n",
    "    # Read predictions\n",
    "    for key in useful_keys:\n",
    "        for k in range(n_seeds):\n",
    "            ckpt_path = os.path.abspath(os.path.join(RESULTS_PATH, 'predictions_%s' % dataset_name, key, 'seed%d' % k))\n",
    "            filename = os.path.join(ckpt_path, 'prediction_%s_test.pkl' % task_mode)\n",
    "            with open(filename, 'rb') as handle:\n",
    "                this_pred = RefactorUnpickler(handle).load()\n",
    "            predictions_dict[model_name].append(this_pred)\n",
    "            optimal_thr_dict[model_name].append(this_ckpt_dict[key][k])\n",
    "print('Predictions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure AF1 and F1 vs IoU\n",
    "print('Measuring F1')\n",
    "af1_dict = {}\n",
    "f1_vs_iou_dict = {}\n",
    "iou_list = np.arange(1, 10) * 0.1\n",
    "for model_name in model_name_list:\n",
    "    print('Measuring model %s' % model_name, flush=True)\n",
    "    af1_dict[model_name] = []\n",
    "    f1_vs_iou_dict[model_name] = []\n",
    "    for optimal_thr, predictions in zip(optimal_thr_dict[model_name], predictions_dict[model_name]):\n",
    "        predictions.set_probability_threshold(optimal_thr)\n",
    "        this_detections = predictions.get_stamps()\n",
    "        af1_at_thr = metrics.average_metric_with_list(this_events, this_detections, verbose=False)\n",
    "        af1_dict[model_name].append(af1_at_thr)\n",
    "        this_f1_vs_iou = metrics.metric_vs_iou_with_list(this_events, this_detections, iou_list)\n",
    "        f1_vs_iou_dict[model_name].append(this_f1_vs_iou)\n",
    "# Mean results\n",
    "mean_af1_dict = {}\n",
    "mean_f1_vs_iou_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    mean_af1 = np.mean(af1_dict[model_name])\n",
    "    std_af1 = np.std(af1_dict[model_name])\n",
    "    \n",
    "    mean_curve = np.stack(f1_vs_iou_dict[model_name], axis=1).mean(axis=1)\n",
    "    std_curve = np.stack(f1_vs_iou_dict[model_name], axis=1).std(axis=1)\n",
    "    \n",
    "    mean_af1_dict[model_name] = (mean_af1, std_af1)\n",
    "    mean_f1_vs_iou_dict[model_name] = (mean_curve, std_curve)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_precision = 2\n",
    "print('Test AF1 for %s' % dataset_name)\n",
    "for model_name in model_name_list:\n",
    "    print('%s: %1.2f +- %1.2f' \n",
    "          % (\n",
    "              model_name_display_dict[model_name].ljust(20), \n",
    "              np.round(100*mean_af1_dict[model_name][0], decimals=round_precision), \n",
    "              np.round(100*mean_af1_dict[model_name][1], decimals=round_precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "color_list = {'v15': CUSTOM_COLOR['red'] , 'expert': CUSTOM_COLOR['grey'], 'v11': CUSTOM_COLOR['blue'], 'v12': CUSTOM_COLOR['green']}\n",
    "linewidth = 1.5\n",
    "markersize = 7\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "show_std = True\n",
    "\n",
    "# -------------------- P L O T ----------------------    \n",
    "\n",
    "# Comparison data\n",
    "compare_expert = dataset.event_name == constants.SPINDLE\n",
    "if compare_expert:\n",
    "    expert_f1_curve_mean = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_mean.csv'), delimiter=',')\n",
    "    expert_f1_curve_std = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_f1_vs_iou_expert_std.csv'), delimiter=',')\n",
    "    expert_f1_curve_mean = expert_f1_curve_mean[1:, :]\n",
    "    expert_f1_curve_std = expert_f1_curve_std[1:, :]\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "\n",
    "# Complete plot\n",
    "if compare_expert:\n",
    "    ax.plot(expert_f1_curve_mean[:, 0], expert_f1_curve_mean[:, 1], linewidth=linewidth, \n",
    "               label='Expert Performance\\nPrivate Dataset\\nWarby et al. 2014', color=color_list['expert'])\n",
    "    ax.plot(expert_f1_curve_mean[:, 0], expert_f1_curve_mean[:, 1], linestyle='none', \n",
    "               markersize=markersize, marker='.', color=color_list['expert'])\n",
    "    ax.fill_between(\n",
    "        expert_f1_curve_mean[:, 0], \n",
    "        expert_f1_curve_mean[:, 1] - expert_f1_curve_std[:, 1], \n",
    "        expert_f1_curve_mean[:, 1] + expert_f1_curve_std[:, 1], \n",
    "        alpha=alpha, facecolor=color_list['expert'])\n",
    "\n",
    "for model_name in model_name_list:\n",
    "    this_name = model_name_display_dict[model_name]\n",
    "    ax.plot(iou_list, mean_f1_vs_iou_dict[model_name][0], \n",
    "               linewidth=linewidth,\n",
    "               label='%s' % this_name, color=color_list[model_name])\n",
    "    ax.plot(iou_list, mean_f1_vs_iou_dict[model_name][0], \n",
    "               linestyle='none', markersize=markersize, marker='.', \n",
    "               color=color_list[model_name])\n",
    "    if show_std:\n",
    "        ax.fill_between(iou_list, \n",
    "                        mean_f1_vs_iou_dict[model_name][0] - mean_f1_vs_iou_dict[model_name][1], \n",
    "                        mean_f1_vs_iou_dict[model_name][0] + mean_f1_vs_iou_dict[model_name][1], alpha=alpha, facecolor=color_list[model_name])\n",
    "\n",
    "\n",
    "    \n",
    "ax.set_title('Test Performance (%s)' % dataset_name.upper(), fontsize=title_fontsize)\n",
    "ax.set_xlim([0.1 - 0.02, 0.9 + 0.02])\n",
    "ax.set_ylim([0.1 - 0.02, 0.9 + 0.02])\n",
    "# ax.set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax.set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax.tick_params(labelsize=other_fontsize)\n",
    "ax.set_xlabel('IoU Threshold', fontsize=other_fontsize)\n",
    "ax.set_ylabel('F1-score', fontsize=other_fontsize)\n",
    "ax.yaxis.grid()\n",
    "ax.legend(loc='lower left', labelspacing=1.5, fontsize=other_fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR\n",
    "iou_thr = 0.3\n",
    "res_thr = 0.02\n",
    "start_thr = 0.1\n",
    "end_thr = 0.9\n",
    "n_thr = int(np.round((end_thr - start_thr) / res_thr + 1))\n",
    "thr_list = np.array([start_thr + res_thr * i for i in range(n_thr)])\n",
    "\n",
    "pr_curve_dict = {}\n",
    "for model_name in model_name_list:\n",
    "    print('Measuring model %s' % model_name, flush=True)\n",
    "    pr_curve_dict[model_name] = []\n",
    "    for k, predictions in enumerate(predictions_dict[model_name]):\n",
    "        pr_curve_dict[model_name].append(np.zeros((n_thr, 2)))\n",
    "        for i, thr in enumerate(thr_list):\n",
    "            predictions.set_probability_threshold(thr)\n",
    "            this_detections = predictions.get_stamps()\n",
    "            this_stats = [metrics.by_event_confusion(this_y, this_y_pred, iou_thr=iou_thr) \n",
    "                        for (this_y, this_y_pred) in zip(this_events, this_detections)]\n",
    "            this_recall = np.mean([m[constants.RECALL] for m in this_stats])\n",
    "            this_precision = np.mean([m[constants.PRECISION] for m in this_stats])\n",
    "            pr_curve_dict[model_name][k][i, 0] = this_recall\n",
    "            pr_curve_dict[model_name][k][i, 1] = this_precision\n",
    "print('Done', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=DPI)\n",
    "markersize = 6\n",
    "alpha = 0.3\n",
    "text_space = 0.01\n",
    "compare_expert = dataset.event_name == constants.SPINDLE\n",
    "axis_lims = [0.5, 1.0]\n",
    "color_list = {'v15': CUSTOM_COLOR['red'] , 'expert': CUSTOM_COLOR['grey'], 'v11': CUSTOM_COLOR['blue'], 'v12': CUSTOM_COLOR['green']}\n",
    "title_fontsize = 10\n",
    "other_fontsize = 8\n",
    "\n",
    "# F1 score levels\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "levels = [level for level in levels if (level>axis_lims[0] and level<axis_lims[1])]\n",
    "CS = ax.contour(X, Y, Z, colors='k', alpha=0.3, levels=levels)\n",
    "ax.clabel(CS, fontsize=7.5, fmt='%1.2f')\n",
    "    \n",
    "if compare_expert:\n",
    "    expert_pr_mean = np.loadtxt(os.path.join(COMPARISON_PATH, 'expert', 'ss_pr_expert_mean.csv'), delimiter=',')\n",
    "    ax.scatter(expert_pr_mean[0], expert_pr_mean[1], s=50, c=color_list['expert'], zorder=10, label='Expert Performance\\nPrivate Dataset\\nWarby et al. 2014')\n",
    "    \n",
    "for model_name in model_name_list:\n",
    "    mean_precision = []\n",
    "    mean_recall = []\n",
    "    for k, pr_curve in enumerate(pr_curve_dict[model_name]):\n",
    "        if k == 0:\n",
    "            label = model_name_display_dict[model_name]\n",
    "        else:\n",
    "            label = None\n",
    "        thr_run = optimal_thr_dict[model_name][k]\n",
    "        chosen_thr_idx = np.where(np.isclose(thr_list, thr_run))[0].item()\n",
    "        ax.plot(pr_curve[:, 0], pr_curve[:, 1], linewidth=1, color=color_list[model_name], zorder=7, alpha=alpha, label=label)\n",
    "        mean_precision.append(pr_curve[chosen_thr_idx, 1])\n",
    "        mean_recall.append(pr_curve[chosen_thr_idx, 0])\n",
    "    mean_precision = np.mean(mean_precision)\n",
    "    mean_recall = np.mean(mean_recall)\n",
    "    ax.scatter(mean_recall, mean_precision, s=50, c=color_list[model_name], zorder=10)\n",
    "\n",
    "ax.set_title('Test PR Curve with IoU$>$%1.1f (%s)' % (iou_thr, dataset_name.upper()), fontsize=10)\n",
    "ax.set_xlabel('Recall', fontsize=8.5)\n",
    "ax.set_ylabel('Precision', fontsize=8.5)\n",
    "ax.set_xlim(axis_lims)\n",
    "ax.set_ylim(axis_lims)\n",
    "\n",
    "lg = ax.legend(loc='lower left', labelspacing=1, fontsize=6.5)\n",
    "for lh in lg.legendHandles:\n",
    "    lh.set_alpha(1.0)\n",
    "\n",
    "ax.tick_params(labelsize=8.5)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SS and KC example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11. Test size: 4\n",
      "Train subjects: \n",
      " [1, 3, 5, 7, 9, 10, 11, 14, 17, 18, 19]\n",
      "Test subjects: \n",
      " [2, 6, 12, 13]\n",
      "Dataset mass_ss with 15 patients.\n",
      "Loading from checkpoint... Loaded\n",
      "Global STD: 16.482037\n",
      "Train size: 11. Test size: 4\n",
      "Train subjects: \n",
      " [1, 3, 5, 7, 9, 10, 11, 14, 17, 18, 19]\n",
      "Test subjects: \n",
      " [2, 6, 12, 13]\n",
      "Dataset mass_kc with 15 patients.\n",
      "Loading from checkpoint... Loaded\n",
      "Global STD: 16.482037\n",
      "1044 SS stamps.\n",
      "409 KC stamps.\n"
     ]
    }
   ],
   "source": [
    "subject_id = 1\n",
    "dataset = load_dataset(constants.MASS_SS_NAME)\n",
    "signal = dataset.get_subject_signal(subject_id=subject_id, normalize_clip=False)\n",
    "stamps_ss = dataset.get_subject_stamps(subject_id=subject_id)\n",
    "dataset = load_dataset(constants.MASS_KC_NAME)\n",
    "stamps_kc = dataset.get_subject_stamps(subject_id=subject_id)\n",
    "fs = dataset.fs\n",
    "\n",
    "print('%d SS stamps.' % stamps_ss.shape[0])\n",
    "print('%d KC stamps.' % stamps_kc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stamps(stamps, start_sample, end_sample):\n",
    "    pages_list = []\n",
    "    for i in range(stamps.shape[0]):\n",
    "        start_inside = (stamps[i, 0] > start_sample) and (stamps[i, 0] < end_sample)\n",
    "        end_inside = (stamps[i, 1] > start_sample) and (stamps[i, 1] < end_sample)\n",
    "\n",
    "        if start_inside or end_inside:\n",
    "            pages_list.append(stamps[i, :])\n",
    "    return pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segment(which_kc_stamp):\n",
    "    left_context = 3\n",
    "    right_context = 7\n",
    "    segment_size = left_context + right_context\n",
    "    central_sample = stamps_kc[which_kc_stamp, :].mean()\n",
    "    start_sample = int(central_sample - fs * left_context)\n",
    "    end_sample = int(central_sample + fs * right_context)\n",
    "\n",
    "    segment_stamps_ss = filter_stamps(stamps_ss, start_sample, end_sample)\n",
    "    segment_stamps_kc = filter_stamps(stamps_kc, start_sample, end_sample)\n",
    "    segment_signal = signal[start_sample:end_sample]\n",
    "    time_axis = np.arange(start_sample, end_sample) / fs\n",
    "\n",
    "    title_fontsize = 10\n",
    "    other_fontsize = 8\n",
    "    fig, ax = plt.subplots(1, 1, dpi=DPI, figsize=(5, 2))\n",
    "\n",
    "    y_max = 150                   \n",
    "    ax.plot(\n",
    "        time_axis, segment_signal, \n",
    "        linewidth=1, color=CUSTOM_COLOR['grey'], label='EEG C3-CLE')\n",
    "    stamp_label_used = False\n",
    "    for stamp in segment_stamps_ss:\n",
    "        if stamp_label_used:\n",
    "            label = None\n",
    "        else:\n",
    "            label = 'SS Mark'\n",
    "            stamp_label_used = True\n",
    "        ax.fill_between(\n",
    "            stamp / fs, y_max, -y_max, \n",
    "            facecolor=CUSTOM_COLOR['red'], alpha=0.3, label=label,\n",
    "            edgecolor='k', linewidth=1.5, \n",
    "        )\n",
    "    stamp_label_used = False\n",
    "    for stamp in segment_stamps_kc:\n",
    "        if stamp_label_used:\n",
    "            label = None\n",
    "        else:\n",
    "            label = 'KC Mark'\n",
    "            stamp_label_used = True\n",
    "        ax.fill_between(\n",
    "            stamp / fs, y_max, -y_max, \n",
    "            facecolor=CUSTOM_COLOR['blue'], alpha=0.3, label=label,\n",
    "            edgecolor='k', linewidth=1.5, \n",
    "        )\n",
    "    ax.set_xlabel('Time [s]', fontsize=other_fontsize)\n",
    "    ax.set_ylabel('Voltage [$\\mu$V]', fontsize=other_fontsize)\n",
    "    # ax.set_yticks([-y_max, 0, y_max])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([time_axis[0], time_axis[-1]])\n",
    "    ax.set_ylim([-y_max, y_max])\n",
    "    # ticks_array = time_axis[0] + np.arange(0, segment_size+1, 5)\n",
    "    # ax.set_xticks(ticks_array)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticks(np.arange(time_axis[0], time_axis[-1], 0.5), minor=True)\n",
    "    ax.grid(b=True, axis='x', which='minor')\n",
    "    lg = ax.legend(loc='upper right', fontsize=other_fontsize-1)\n",
    "    ax.tick_params(labelsize=other_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167731aa99ed41d79683a89475a3fc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=195, continuous_update=False, description='which_kc_stamp', max=408), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interact(\n",
    "    lambda which_kc_stamp: plot_segment(which_kc_stamp),\n",
    "    which_kc_stamp=widgets.IntSlider(min=0,max=stamps_kc.shape[0]-1,step=1,value=195, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
