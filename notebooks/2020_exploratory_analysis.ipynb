{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, gridspec\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import find_peaks, freqz\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "from sleeprnn.common.optimal_thresholds import OPTIMAL_THR_FOR_CKPT_DICT\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.helpers import reader, plotter, printer, misc, performer\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "\n",
    "%matplotlib inline\n",
    "viz.notebook_full_width()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_spindle(spindle, fs):\n",
    "    duration = spindle.size / fs\n",
    "    pp_amplitude = spindle.max() - spindle.min()\n",
    "    rms = np.sqrt(np.mean(spindle ** 2))\n",
    "    central_freq_count = find_peaks(spindle)[0].size / duration\n",
    "    # Compute peak frequency by fft\n",
    "    w, h = freqz(spindle)\n",
    "    resp_freq = w * fs / (2*np.pi)\n",
    "    resp_amp = abs(h)\n",
    "    max_loc = np.argmax(resp_amp)\n",
    "    central_freq_fft = resp_freq[max_loc]\n",
    "    results = {\n",
    "        'duration': duration,\n",
    "        'pp_amplitude': pp_amplitude,\n",
    "        'rms': rms,\n",
    "        'central_freq_count': central_freq_count,\n",
    "        'central_freq_fft': central_freq_fft\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_band_weights(freq_vals, lim_low, lim_high):\n",
    "    band_low = (freq_vals >= lim_low).astype(np.float32)\n",
    "    band_high = (freq_vals <= lim_high).astype(np.float32)\n",
    "    band_weights = band_low * band_high\n",
    "    return band_weights\n",
    "\n",
    "        \n",
    "def get_band_power(power_vals, freq_vals, lim_low, lim_high, mode):\n",
    "    if mode not in [\"mean\", \"max\"]:\n",
    "        raise ValueError()\n",
    "    w = get_band_weights(freq_vals, lim_low, lim_high)\n",
    "    weighted_power = w * power_vals\n",
    "    if mode == \"mean\":\n",
    "        band_power = np.sum(weighted_power) / np.sum(w)\n",
    "    else:\n",
    "        band_power =  np.max(weighted_power)\n",
    "    return band_power\n",
    "\n",
    "\n",
    "def analyze_spindle_window(signal_window, fs):\n",
    "    window_han = np.hanning(signal_window.size)\n",
    "    signal_window = signal_window * window_han\n",
    "    padding = np.zeros(1 * fs)\n",
    "    signal_window_extended = np.concatenate([padding, signal_window, padding])\n",
    "    amp, freqs = utils.power_spectrum(signal_window_extended, fs)\n",
    "    \n",
    "    # Kulkarni (SpindleNet) power ratio\n",
    "    num_power = get_band_power(amp, freqs, 9, 16, mode=\"mean\")\n",
    "    den_power = get_band_power(amp, freqs, 2, 8, mode=\"mean\")\n",
    "    pr_spindlenet = num_power / (den_power + 1e-6) \n",
    "\n",
    "    # Lacourse (A7) power ratio\n",
    "    num_power = get_band_power(amp, freqs, 11, 16, mode=\"mean\")\n",
    "    den_power = get_band_power(amp, freqs, 4.5, 30, mode=\"mean\")\n",
    "    pr_a7 = num_power / (den_power + 1e-6)\n",
    "\n",
    "    # Huupponen (sigma index) power ratio\n",
    "    num_power = get_band_power(amp, freqs, 10.5, 16, mode=\"max\")\n",
    "    den_power_1 = get_band_power(amp, freqs, 4, 10, mode=\"mean\")\n",
    "    den_power_2 = get_band_power(amp, freqs, 20, 40, mode=\"mean\")\n",
    "    alpha_power = get_band_power(amp, freqs, 7.5, 10, mode=\"max\")\n",
    "    pr_huupp = 2.0 * num_power / (den_power_1 + den_power_2 + 1e-6)\n",
    "    pr_huupp_alfa = num_power / (alpha_power + 1e-6)\n",
    "    \n",
    "    results = {\n",
    "        'pr_spindlenet': pr_spindlenet,\n",
    "        'pr_a7': pr_a7,\n",
    "        'pr_huupp': pr_huupp,\n",
    "        'pr_huupp_alfa': pr_huupp_alfa,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_spindle_window_custom(signal_window, fs, bands_list):\n",
    "    window_han = np.hanning(signal_window.size)\n",
    "    signal_window = signal_window * window_han\n",
    "    padding = np.zeros(1 * fs)\n",
    "    signal_window_extended = np.concatenate([padding, signal_window, padding])\n",
    "    amp, freqs = utils.power_spectrum(signal_window_extended, fs)\n",
    "    sigma_power = get_band_power(amp, freqs, 11, 16, mode=\"mean\")\n",
    "    power_list = []\n",
    "    for band in bands_list:\n",
    "        band_power = get_band_power(amp, freqs, band[0], band[1], mode=\"mean\")\n",
    "        power_list.append(sigma_power / band_power)\n",
    "    results = {'band_%d' % i: power_list[i] for i in range(len(bands_list))}\n",
    "    return results\n",
    "\n",
    "def analyze_spindle_window_spectrum(signal_window, fs):\n",
    "    window_han = np.hanning(signal_window.size)\n",
    "    signal_window = signal_window * window_han\n",
    "    padding = np.zeros(1 * fs)\n",
    "    signal_window_extended = np.concatenate([padding, signal_window, padding])\n",
    "    amp, freqs = utils.power_spectrum(signal_window_extended, fs)\n",
    "    sigma_power = get_band_power(amp, freqs, 11, 16, mode=\"max\")\n",
    "    sigma_norm_amp = amp / sigma_power\n",
    "    results = {\n",
    "        'spectrum': np.stack([freqs, amp], axis=1), \n",
    "        'spectrum_sigma_norm': np.stack([freqs, sigma_norm_amp], axis=1)}\n",
    "    return results\n",
    "\n",
    "\n",
    "def listify_dictionaries(list_of_dicts):\n",
    "    dict_of_lists = {}\n",
    "    for key in list_of_dicts[0].keys():\n",
    "        dict_of_lists[key] = []\n",
    "        for single_dict in list_of_dicts:\n",
    "            dict_of_lists[key].append(single_dict[key])\n",
    "    return dict_of_lists\n",
    "\n",
    "\n",
    "def fft_based_scaling(signal, fs, pages, page_size, band=None):\n",
    "    # Using FFT on whole page\n",
    "    amp_all = []\n",
    "    freq_all = []\n",
    "    window_han = np.hanning(page_size)\n",
    "    for page in pages:\n",
    "        start_page = page * page_size\n",
    "        end_page = start_page + page_size\n",
    "        amp, freq = utils.power_spectrum(window_han * signal[start_page:end_page], fs)\n",
    "        amp_all.append(amp)\n",
    "        freq_all.append(freq)\n",
    "    amp_all = np.stack(amp_all, axis=0).mean(axis=0)\n",
    "    freq_all = np.stack(freq_all, axis=0).mean(axis=0)\n",
    "    if band is None:\n",
    "        scaling_factor = 1 / np.mean(amp_all)\n",
    "    else:\n",
    "        first_freq_loc = np.argmin((freq_all - band[0]) ** 2)\n",
    "        last_freq_loc = np.argmin((freq_all - band[1]) ** 2)\n",
    "        scaling_factor = 1 / np.mean(amp_all[first_freq_loc:(last_freq_loc+1)])\n",
    "    return scaling_factor\n",
    "\n",
    "\n",
    "def raw_spectrum(signal, fs, pages, page_size):\n",
    "    # Using FFT on whole page\n",
    "    amp_all = []\n",
    "    freq_all = []\n",
    "    window_han = np.hanning(page_size)\n",
    "    for page in pages:\n",
    "        start_page = page * page_size\n",
    "        end_page = start_page + page_size\n",
    "        amp, freq = utils.power_spectrum(window_han * signal[start_page:end_page], fs)\n",
    "        amp_all.append(amp)\n",
    "        freq_all.append(freq)\n",
    "    amp_all = np.stack(amp_all, axis=0).mean(axis=0)\n",
    "    freq_all = np.stack(freq_all, axis=0).mean(axis=0)\n",
    "    spectrum_page = np.stack([freq_all, amp_all], axis=1)\n",
    "    \n",
    "    # Using FFT on 2-seconds segments\n",
    "    amp_all = []\n",
    "    freq_all = []\n",
    "    window_size = 2 * fs\n",
    "    window_han = np.hanning(window_size)\n",
    "    n_windows = int(2 * (page_size / window_size) - 1)\n",
    "    for page in pages:\n",
    "        start_page = page * page_size\n",
    "        end_page = start_page + page_size\n",
    "        segment_signal = signal[start_page:end_page]\n",
    "        for i in range(n_windows):\n",
    "            start_window = int(i * window_size / 2)\n",
    "            end_window = start_window + window_size\n",
    "            window_signal = segment_signal[start_window:end_window] * window_han\n",
    "            padding = np.zeros(1 * fs)\n",
    "            window_signal_extended = np.concatenate([padding, window_signal, padding])\n",
    "            amp, freq = utils.power_spectrum(window_signal_extended, fs)\n",
    "            amp_all.append(amp)\n",
    "            freq_all.append(freq)\n",
    "    amp_all = np.stack(amp_all, axis=0).mean(axis=0)\n",
    "    freq_all = np.stack(freq_all, axis=0).mean(axis=0)\n",
    "    spectrum_window = np.stack([freq_all, amp_all], axis=1)\n",
    "    \n",
    "    results = {\n",
    "        'spectrum_page': spectrum_page,\n",
    "        'spectrum_window': spectrum_window\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def center_probabilities(probabilities, center):\n",
    "    \"\"\"input: probas with class 1 iff proba > center.\n",
    "    output: probas with class 1 iff proba > 0.5\n",
    "    \"\"\"\n",
    "    bias_center = np.log(center / (1-center))\n",
    "    eps = 1e-6\n",
    "    probabilities = np.clip(probabilities, eps, 1-eps)\n",
    "    logits = np.log(probabilities / (1 - probabilities))\n",
    "    logits = logits - bias_center\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED: K-Complex Performance Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = constants.MASS_KC_NAME\n",
    "fs = 200\n",
    "dataset = reader.load_dataset(dataset_name, params={pkeys.FS: fs})\n",
    "\n",
    "# Settings\n",
    "ref_ckpt_folder = '20200724_reproduce_red_n2_train_mass_kc/v19_rep1'\n",
    "seed_id_list = [0, 1, 2, 3]\n",
    "\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "set_list = [constants.VAL_SUBSET, constants.TRAIN_SUBSET]\n",
    "iou_thr = 0.2\n",
    "iou_hist_bins = np.linspace(0, 1, 21)\n",
    "iou_curve_axis = misc.custom_linspace(0.05, 0.95, 0.05)\n",
    "ids_dict = {constants.ALL_TRAIN_SUBSET: dataset.train_ids, constants.TEST_SUBSET: dataset.test_ids}\n",
    "ids_dict.update(misc.get_splits_dict(dataset, seed_id_list))\n",
    "\n",
    "# Load predictions\n",
    "ref_predictions_dict = reader.read_prediction_with_seeds(\n",
    "    ref_ckpt_folder, dataset_name, task_mode, seed_id_list, set_list=set_list, parent_dataset=dataset)\n",
    "\n",
    "# Compute performance by subject\n",
    "ref_precision_dict = {}\n",
    "ref_recall_dict = {}\n",
    "for seed_id in seed_id_list:\n",
    "    data_inference = FeederDataset(dataset, ids_dict[seed_id][constants.VAL_SUBSET], task_mode, which_expert)\n",
    "    this_ids = data_inference.get_ids()\n",
    "    this_events_list = data_inference.get_stamps()\n",
    "    prediction_obj = ref_predictions_dict[seed_id][constants.VAL_SUBSET]\n",
    "    prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[ref_ckpt_folder][seed_id])\n",
    "    this_detections_list = prediction_obj.get_stamps()\n",
    "    for i, single_id in enumerate(this_ids):\n",
    "        single_events = this_events_list[i]\n",
    "        single_detections = this_detections_list[i]\n",
    "        this_precision = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.PRECISION)\n",
    "        this_recall = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.RECALL)\n",
    "        ref_precision_dict[single_id] = this_precision[0]\n",
    "        ref_recall_dict[single_id] = this_recall[0]\n",
    "print(\"Done.\")\n",
    "color_recall = viz.GREY_COLORS[5]\n",
    "color_precision = viz.PALETTE['dark']\n",
    "color_difference = viz.PALETTE['blue']\n",
    "markersize = 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5), dpi=200)\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    rec_label = \"Recall\" if i==0 else None\n",
    "    pre_label = \"Precision\" if i==0 else None\n",
    "    diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "    difference = ref_recall_dict[single_id] - ref_precision_dict[single_id]\n",
    "    ax.plot(\n",
    "        i, ref_recall_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, ref_precision_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, difference, markersize=markersize, marker='o', \n",
    "        color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "    ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "ax.set_ylim([-0.55, 1.05])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.set_xlabel(\"Subject\")\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_title(\"RED-CWT, KC Validation at IoU>%1.1f\" % iou_thr, loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = constants.MASS_SS_NAME\n",
    "fs = 200\n",
    "dataset = reader.load_dataset(dataset_name, params={pkeys.FS: fs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spindle annotations across the night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_pages = 0  #6\n",
    "thr_n_pages = 32  #30\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3), dpi=200)\n",
    "\n",
    "ax = axes[0]\n",
    "for subject_id in dataset.train_ids:\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "    n2_pages = np.where(hypno==\"2\")[0]\n",
    "    n2_pages_end = (n2_pages + 1) * dataset.page_size\n",
    "    stamps_center = stamps.mean(axis=1)\n",
    "    cumulative_spindles = []\n",
    "    for page_end in n2_pages_end:\n",
    "        count = np.sum(stamps_center <= page_end)\n",
    "        cumulative_spindles.append(count)\n",
    "    cumulative_spindles = np.array(cumulative_spindles)\n",
    "    # Drop pages\n",
    "    if drop_pages > 0:\n",
    "        spindles_until_drop = cumulative_spindles[drop_pages-1]\n",
    "        cumulative_spindles = cumulative_spindles - spindles_until_drop\n",
    "        cumulative_spindles = cumulative_spindles[drop_pages:]\n",
    "    pages_axis_seconds = np.arange(1, n2_pages_end.size + 1 - drop_pages) * 20 \n",
    "    pages_axis_minutes = pages_axis_seconds / 60\n",
    "    pages_axis_hours = pages_axis_minutes / 60\n",
    "    print(\"S%02d: %02d spindles in first %s N2 pages (%d pages dropped)\" % (\n",
    "        subject_id, cumulative_spindles[thr_n_pages-1], thr_n_pages, drop_pages))\n",
    "    ax.plot(pages_axis_hours, cumulative_spindles, linewidth=1.1, color=viz.GREY_COLORS[6])\n",
    "thr_n_pages_hours = thr_n_pages * 20 / 3600\n",
    "ax.axvline(thr_n_pages_hours)\n",
    "ax.set_xlabel(\"Hours in N2 Sleep\")\n",
    "ax.set_ylabel(\"Accumulated Spindles\")\n",
    "ax.set_title(\"Spindle count in validation subjects\", loc=\"left\")\n",
    "\n",
    "ax = axes[1]\n",
    "for subject_id in dataset.train_ids:\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "    n2_pages = np.where(hypno==\"2\")[0]\n",
    "    n2_pages_end = (n2_pages + 1) * dataset.page_size\n",
    "    stamps_center = stamps.mean(axis=1)\n",
    "    cumulative_spindles = []\n",
    "    for page_end in n2_pages_end:\n",
    "        count = np.sum(stamps_center <= page_end)\n",
    "        cumulative_spindles.append(count)\n",
    "    cumulative_spindles = np.array(cumulative_spindles)\n",
    "    # Drop pages\n",
    "    if drop_pages > 0:\n",
    "        spindles_until_drop = cumulative_spindles[drop_pages-1]\n",
    "        cumulative_spindles = cumulative_spindles - spindles_until_drop\n",
    "        cumulative_spindles = cumulative_spindles[drop_pages:]\n",
    "    pages_axis_seconds = np.arange(1, n2_pages_end.size + 1 - drop_pages) * 20 \n",
    "    pages_axis_minutes = pages_axis_seconds / 60\n",
    "    print(\"S%02d: %02d spindles in first %s N2 pages (%d pages dropped)\" % (\n",
    "        subject_id, cumulative_spindles[thr_n_pages-1], thr_n_pages, drop_pages))\n",
    "    ax.plot(pages_axis_minutes, cumulative_spindles, linewidth=1.1, color=viz.GREY_COLORS[6])\n",
    "thr_n_pages_minutes = thr_n_pages * 20 / 60\n",
    "ax.axvline(thr_n_pages_minutes, linewidth=2, label=\"%d N2 pages\" % thr_n_pages)\n",
    "ax.set_xlabel(\"Minutes in N2 Sleep\")\n",
    "ax.set_ylabel(\"Accumulated Spindles\")\n",
    "ax.set_title(\"Zoom-in\", loc=\"left\")\n",
    "ax.set_xlim([0, 30])\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xticks([0, thr_n_pages_minutes, 30])\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED Validation Performance (MASS-SS-E1-N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dates = [20200724, None]\n",
    "printer.print_available_ckpt(OPTIMAL_THR_FOR_CKPT_DICT, filter_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "ref_ckpt_folder = '20200724_reproduce_red_n2_train_mass_ss/v19_rep1'\n",
    "seed_id_list = [0, 1, 2, 3]\n",
    "\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "set_list = [constants.VAL_SUBSET, constants.TRAIN_SUBSET]\n",
    "iou_thr = 0.2\n",
    "iou_hist_bins = np.linspace(0, 1, 21)\n",
    "iou_curve_axis = misc.custom_linspace(0.05, 0.95, 0.05)\n",
    "ids_dict = {constants.ALL_TRAIN_SUBSET: dataset.train_ids, constants.TEST_SUBSET: dataset.test_ids}\n",
    "ids_dict.update(misc.get_splits_dict(dataset, seed_id_list))\n",
    "\n",
    "# Load predictions\n",
    "ref_predictions_dict = reader.read_prediction_with_seeds(\n",
    "    ref_ckpt_folder, dataset_name, task_mode, seed_id_list, set_list=set_list, parent_dataset=dataset)\n",
    "\n",
    "# Compute performance by subject\n",
    "ref_precision_dict = {}\n",
    "ref_recall_dict = {}\n",
    "for seed_id in seed_id_list:\n",
    "    data_inference = FeederDataset(dataset, ids_dict[seed_id][constants.VAL_SUBSET], task_mode, which_expert)\n",
    "    this_ids = data_inference.get_ids()\n",
    "    this_events_list = data_inference.get_stamps()\n",
    "    prediction_obj = ref_predictions_dict[seed_id][constants.VAL_SUBSET]\n",
    "    prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[ref_ckpt_folder][seed_id])\n",
    "    this_detections_list = prediction_obj.get_stamps()\n",
    "    for i, single_id in enumerate(this_ids):\n",
    "        single_events = this_events_list[i]\n",
    "        single_detections = this_detections_list[i]\n",
    "        this_precision = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.PRECISION)\n",
    "        this_recall = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.RECALL)\n",
    "        ref_precision_dict[single_id] = this_precision[0]\n",
    "        ref_recall_dict[single_id] = this_recall[0]\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_recall = viz.GREY_COLORS[5]\n",
    "color_precision = viz.PALETTE['dark']\n",
    "color_difference = viz.PALETTE['blue']\n",
    "markersize = 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5), dpi=200)\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    rec_label = \"Recall\" if i==0 else None\n",
    "    pre_label = \"Precision\" if i==0 else None\n",
    "    diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "    difference = ref_recall_dict[single_id] - ref_precision_dict[single_id]\n",
    "    ax.plot(\n",
    "        i, ref_recall_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, ref_precision_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, difference, markersize=markersize, marker='o', \n",
    "        color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "    ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "ax.set_ylim([-0.55, 1.05])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.set_xlabel(\"Subject\")\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_title(\"RED-CWT, SS Validation at IoU>%1.1f\" % iou_thr, loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance by subject on training set\n",
    "train_precision_dict = {}\n",
    "train_recall_dict = {}\n",
    "for single_id in dataset.train_ids:\n",
    "    train_precision_dict[single_id] = []\n",
    "    train_recall_dict[single_id] = []\n",
    "\n",
    "for seed_id in seed_id_list:\n",
    "    data_inference = FeederDataset(dataset, ids_dict[seed_id][constants.TRAIN_SUBSET], task_mode, which_expert)\n",
    "    this_ids = data_inference.get_ids()\n",
    "    this_events_list = data_inference.get_stamps()\n",
    "    prediction_obj = ref_predictions_dict[seed_id][constants.TRAIN_SUBSET]\n",
    "    prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[ref_ckpt_folder][seed_id])\n",
    "    this_detections_list = prediction_obj.get_stamps()\n",
    "    for i, single_id in enumerate(this_ids):\n",
    "        single_events = this_events_list[i]\n",
    "        single_detections = this_detections_list[i]\n",
    "        this_precision = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.PRECISION)\n",
    "        this_recall = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.RECALL)\n",
    "        train_precision_dict[single_id].append(this_precision[0])\n",
    "        train_recall_dict[single_id].append(this_recall[0])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_recall = viz.GREY_COLORS[5]\n",
    "color_precision = viz.PALETTE['dark']\n",
    "color_difference = viz.PALETTE['blue']\n",
    "markersize = 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5), dpi=200)\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    rec_label = \"Recall\" if i==0 else None\n",
    "    pre_label = \"Precision\" if i==0 else None\n",
    "    diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "    recall = np.mean(train_recall_dict[single_id])\n",
    "    precision = np.mean(train_precision_dict[single_id])\n",
    "    recall_std = np.std(train_recall_dict[single_id])\n",
    "    precision_std = np.std(train_precision_dict[single_id])\n",
    "    difference = recall - precision\n",
    "    ax.plot(\n",
    "        i, recall, markersize=markersize, marker='o', \n",
    "        color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, precision, markersize=markersize, marker='o', \n",
    "        color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, difference, markersize=markersize, marker='o', \n",
    "        color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "    ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "ax.set_ylim([-0.55, 1.05])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.set_xlabel(\"Subject\")\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_title(\"RED-CWT, SS Training at IoU>%1.1f\" % iou_thr, loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in performance after FFT-based normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other predictions\n",
    "new_ckpt_folder = '20200914_fft_based_norm_n2_train_mass_ss/v19_rep1'\n",
    "band = [0, 100]\n",
    "\n",
    "# new_ckpt_folder = '20200918_fft_based_norm_slow_n2_train_mass_ss/v19_rep1'\n",
    "# band = [2, 6]\n",
    "\n",
    "new_predictions_dict = reader.read_prediction_with_seeds(\n",
    "    new_ckpt_folder, dataset_name, task_mode, seed_id_list, set_list=set_list, parent_dataset=dataset)\n",
    "\n",
    "# Compute performance by subject\n",
    "new_precision_dict = {}\n",
    "new_recall_dict = {}\n",
    "for seed_id in seed_id_list:\n",
    "    data_inference = FeederDataset(dataset, ids_dict[seed_id][constants.VAL_SUBSET], task_mode, which_expert)\n",
    "    this_ids = data_inference.get_ids()\n",
    "    this_events_list = data_inference.get_stamps()\n",
    "    prediction_obj = new_predictions_dict[seed_id][constants.VAL_SUBSET]\n",
    "    prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[new_ckpt_folder][seed_id])\n",
    "    this_detections_list = prediction_obj.get_stamps()\n",
    "    for i, single_id in enumerate(this_ids):\n",
    "        single_events = this_events_list[i]\n",
    "        single_detections = this_detections_list[i]\n",
    "        this_precision = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.PRECISION)\n",
    "        this_recall = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.RECALL)\n",
    "        new_precision_dict[single_id] = this_precision[0]\n",
    "        new_recall_dict[single_id] = this_recall[0]\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_recall = viz.GREY_COLORS[5]\n",
    "color_precision = viz.PALETTE['dark']\n",
    "color_difference = viz.PALETTE['blue']\n",
    "color_positive = viz.PALETTE['green']\n",
    "color_negative = viz.PALETTE['red']\n",
    "markersize = 5\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(5, 6), dpi=200)\n",
    "ax = axes[0]\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    rec_label = \"Recall\" if i==0 else None\n",
    "    pre_label = \"Precision\" if i==0 else None\n",
    "    diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "    difference = ref_recall_dict[single_id] - ref_precision_dict[single_id]\n",
    "    ax.plot(\n",
    "        i, ref_recall_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, ref_precision_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, difference, markersize=markersize, marker='o', \n",
    "        color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "    ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "ax.set_ylim([-0.55, 1.05])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_title(\"RED-CWT Validation at IoU>%1.1f\" % iou_thr, loc=\"left\")\n",
    "\n",
    "ax = axes[1]\n",
    "subjects_scaling_dict = {}\n",
    "mean_fft_scaling = 0\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "    n2_pages = np.where(hypno==\"2\")[0]\n",
    "    scaling_factor = fft_based_scaling(signal, fs, n2_pages, dataset.page_size, band=band)\n",
    "    mean_fft_scaling += scaling_factor\n",
    "    subjects_scaling_dict[subject_id] = scaling_factor\n",
    "mean_fft_scaling /= len(dataset.train_ids)\n",
    "factors_list = []\n",
    "upscale_shown = False\n",
    "downscale_shown = False\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    factor = subjects_scaling_dict[single_id] / mean_fft_scaling\n",
    "    factors_list.append(factor)\n",
    "    color = color_positive if factor > 1 else color_negative\n",
    "    marker = \"^\" if factor > 1 else \"v\" \n",
    "    if factor > 1 and not upscale_shown:\n",
    "        scale_label = \"Upscaling\"\n",
    "        upscale_shown = True\n",
    "    elif factor < 1 and not downscale_shown:\n",
    "        scale_label = \"Downscaling\"\n",
    "        downscale_shown = True\n",
    "    else:\n",
    "        scale_label = None\n",
    "    ax.plot(\n",
    "        i, factor, markersize=markersize, marker=marker, \n",
    "        color=color, linestyle=\"none\", label=scale_label)\n",
    "    ax.plot([i, i], [1, factor], color=color, linewidth=2)\n",
    "max_delta = np.max(np.abs(factors_list) - 1) + 0.1\n",
    "ax.set_ylim([1 - max_delta, 1 + max_delta])\n",
    "ax.set_yticks([0.8, 1.0, 1.2])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.grid()\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.set_title(\"FFT-based scaling factor (%d-%d Hz)\" % (band[0], band[1]), loc=\"left\")\n",
    "\n",
    "ax = axes[2]\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    rec_label = \"Recall\" if i==0 else None\n",
    "    pre_label = \"Precision\" if i==0 else None\n",
    "    diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "    difference = new_recall_dict[single_id] - new_precision_dict[single_id]\n",
    "    ax.plot(\n",
    "        i, new_recall_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, new_precision_dict[single_id], markersize=markersize, marker='o', \n",
    "        color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "    ax.plot(\n",
    "        i, difference, markersize=markersize, marker='o', \n",
    "        color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "    ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "ax.set_ylim([-0.55, 1.05])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.set_xlabel(\"Subject\")\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_title(\"After re-scaling\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom scaling (Cheating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_factor_list = [0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "ckpt_folder_dict = {\n",
    "    0.25: '20200920_custom_scaling_n2_train_mass_ss/v19_prop0.25',\n",
    "    0.50: '20200920_custom_scaling_n2_train_mass_ss/v19_prop0.50',\n",
    "    0.75: '20200920_custom_scaling_n2_train_mass_ss/v19_prop0.75',\n",
    "    1.00: '20200920_custom_scaling_n2_train_mass_ss/v19_prop1.00',\n",
    "}\n",
    "\n",
    "custom_scaling_recall_dict = {}\n",
    "custom_scaling_precision_dict = {}\n",
    "for prop_factor in prop_factor_list:\n",
    "    prop_predictions_dict = reader.read_prediction_with_seeds(\n",
    "        ckpt_folder_dict[prop_factor], dataset_name, task_mode, seed_id_list, set_list=set_list, parent_dataset=dataset)\n",
    "    # Compute performance by subject\n",
    "    prop_precision_dict = {}\n",
    "    prop_recall_dict = {}\n",
    "    for seed_id in seed_id_list:\n",
    "        data_inference = FeederDataset(dataset, ids_dict[seed_id][constants.VAL_SUBSET], task_mode, which_expert)\n",
    "        this_ids = data_inference.get_ids()\n",
    "        this_events_list = data_inference.get_stamps()\n",
    "        prediction_obj = prop_predictions_dict[seed_id][constants.VAL_SUBSET]\n",
    "        prediction_obj.set_probability_threshold(OPTIMAL_THR_FOR_CKPT_DICT[ckpt_folder_dict[prop_factor]][seed_id])\n",
    "        this_detections_list = prediction_obj.get_stamps()\n",
    "        for i, single_id in enumerate(this_ids):\n",
    "            single_events = this_events_list[i]\n",
    "            single_detections = this_detections_list[i]\n",
    "            this_precision = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.PRECISION)\n",
    "            this_recall = metrics.metric_vs_iou(single_events, single_detections, [iou_thr], metric_name=constants.RECALL)\n",
    "            prop_precision_dict[single_id] = this_precision[0]\n",
    "            prop_recall_dict[single_id] = this_recall[0]\n",
    "    custom_scaling_recall_dict[prop_factor] = prop_recall_dict\n",
    "    custom_scaling_precision_dict[prop_factor] = prop_precision_dict\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_recall = viz.GREY_COLORS[5]\n",
    "color_precision = viz.PALETTE['dark']\n",
    "color_difference = viz.PALETTE['blue']\n",
    "color_positive = viz.PALETTE['green']\n",
    "color_negative = viz.PALETTE['red']\n",
    "markersize = 5\n",
    "\n",
    "# Best Scaling\n",
    "prop_factor = prop_factor_list[1]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2), dpi=200)\n",
    "subjects_scaling_dict = {}\n",
    "mean_scaling_factor = 0\n",
    "for subject_id in dataset.train_ids:\n",
    "    recall = ref_recall_dict[subject_id]\n",
    "    precision = ref_precision_dict[subject_id]\n",
    "    difference = recall - precision\n",
    "    subjects_scaling_dict[subject_id] = 1 - prop_factor * difference\n",
    "    mean_scaling_factor += subjects_scaling_dict[subject_id]\n",
    "mean_scaling_factor /= len(dataset.train_ids)\n",
    "factors_list = []\n",
    "upscale_shown = False\n",
    "downscale_shown = False\n",
    "for i, single_id in enumerate(dataset.train_ids):\n",
    "    factor = subjects_scaling_dict[single_id] / mean_scaling_factor\n",
    "    factors_list.append(factor)\n",
    "    color = color_positive if factor > 1 else color_negative\n",
    "    marker = \"^\" if factor > 1 else \"v\" \n",
    "    if factor > 1 and not upscale_shown:\n",
    "        scale_label = \"Upscaling\"\n",
    "        upscale_shown = True\n",
    "    elif factor < 1 and not downscale_shown:\n",
    "        scale_label = \"Downscaling\"\n",
    "        downscale_shown = True\n",
    "    else:\n",
    "        scale_label = None\n",
    "    ax.plot(\n",
    "        i, factor, markersize=markersize, marker=marker, \n",
    "        color=color, linestyle=\"none\", label=scale_label)\n",
    "    ax.plot([i, i], [1, factor], color=color, linewidth=2)\n",
    "max_delta = np.max(np.abs(factors_list) - 1) + 0.1\n",
    "ax.set_ylim([1 - max_delta, 1 + max_delta])\n",
    "ax.set_yticks([0.8, 1.0, 1.2])\n",
    "ax.set_xticks(range(len(dataset.train_ids)))\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.grid()\n",
    "ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "ax.set_title(r\"Custom scaling factor ($\\alpha$ = %1.2f)\" % prop_factor, loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# All scaling effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 4.5), dpi=200)\n",
    "for k in range(4):\n",
    "    prop_factor = prop_factor_list[k]\n",
    "    row = k // 2\n",
    "    col = k % 2\n",
    "    ax = axes[row, col]\n",
    "    for i, single_id in enumerate(dataset.train_ids):\n",
    "        rec_label = \"Recall\" if i==0 else None\n",
    "        pre_label = \"Precision\" if i==0 else None\n",
    "        diff_label = \"Difference\\n(Rec. - Pre.)\" if i==0 else None\n",
    "        difference = custom_scaling_recall_dict[prop_factor][single_id] - custom_scaling_precision_dict[prop_factor][single_id]\n",
    "        ax.plot(\n",
    "            i, custom_scaling_recall_dict[prop_factor][single_id], markersize=markersize, marker='o', \n",
    "            color=color_recall, label=rec_label, linestyle=\"none\")\n",
    "        ax.plot(\n",
    "            i, custom_scaling_precision_dict[prop_factor][single_id], markersize=markersize, marker='o', \n",
    "            color=color_precision, label=pre_label, linestyle=\"none\")\n",
    "        ax.plot(\n",
    "            i, difference, markersize=markersize, marker='o', \n",
    "            color=color_difference, label=diff_label, linestyle=\"none\")\n",
    "        ax.plot([i, i], [0, difference], color=color_difference, linewidth=1.5)\n",
    "    ax.axhline(0, color=\"k\", linewidth=0.8, linestyle=\"--\", zorder=1)\n",
    "    ax.set_ylim([-0.55, 1.05])\n",
    "    ax.set_xticks(range(len(dataset.train_ids)))\n",
    "    ax.set_xticklabels(dataset.train_ids)\n",
    "    if k == 1:\n",
    "        ax.legend(loc=\"upper left\", ncol=1, handlelength=0.5, framealpha=1, bbox_to_anchor=(1, 1))\n",
    "    ax.grid(axis=\"x\")\n",
    "    ax.set_title(r\"After using $\\alpha$ = %1.2f\" % prop_factor, loc=\"left\")\n",
    "    if k in [2, 3]:\n",
    "        ax.set_xlabel(\"Subject\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling sensitivy to frequency band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_list = [\n",
    "    [0, 100, None, None],\n",
    "    [2, 30, None, None],\n",
    "    [4, 30, None, None],\n",
    "    [2, 4, None, None],\n",
    "    [2, 6, None, None],\n",
    "    [8, 16, None, None],\n",
    "    [11, 16, None, None],\n",
    "    [11, 16, 4, 12],\n",
    "    [11, 16, 4, 30]\n",
    "]\n",
    "\n",
    "scaling_factor_dict = {}\n",
    "for band in bands_list:\n",
    "    if band[2] is None:\n",
    "        band_id = \"%d-%d\" % (band[0], band[1])\n",
    "    else:\n",
    "        band_id = \"%d-%d / %d-%d\" % (band[0], band[1], band[2], band[3])\n",
    "    scaling_factor_dict[band_id] = []\n",
    "    for subject_id in dataset.train_ids:\n",
    "        signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "        hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "        n2_pages = np.where(hypno==\"2\")[0]\n",
    "        if band[2] is None:\n",
    "            scaling_factor = fft_based_scaling(signal, fs, n2_pages, dataset.page_size, band=band[:2])\n",
    "        else:\n",
    "            scaling_factor_num = fft_based_scaling(signal, fs, n2_pages, dataset.page_size, band=band[:2])\n",
    "            scaling_factor_den = fft_based_scaling(signal, fs, n2_pages, dataset.page_size, band=band[2:])\n",
    "            scaling_factor = scaling_factor_num / scaling_factor_den\n",
    "        scaling_factor_dict[band_id].append(scaling_factor)\n",
    "    scaling_factor_dict[band_id] = np.array(scaling_factor_dict[band_id])\n",
    "    scaling_factor_dict[band_id] /= np.mean(scaling_factor_dict[band_id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emphasize_subjects = [3, 11, 18, 19]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=200)\n",
    "x_axis = np.arange(len(dataset.train_ids))\n",
    "max_delta = 0.5\n",
    "for i, band in enumerate(bands_list):\n",
    "    if band[2] is None:\n",
    "        band_id = \"%d-%d\" % (band[0], band[1])\n",
    "    else:\n",
    "        band_id = \"%d-%d / %d-%d\" % (band[0], band[1], band[2], band[3])\n",
    "    offset = i * 0.5 / (len(bands_list) - 1)\n",
    "    line = ax.plot(\n",
    "        x_axis + offset, scaling_factor_dict[band_id], label=\"%s Hz\" % band_id, \n",
    "        marker='o', markersize=4, linestyle=\"none\")\n",
    "    line_color = line[0].get_color()\n",
    "    for single_x, single_bar in zip(x_axis, scaling_factor_dict[band_id]):\n",
    "        ax.plot([single_x + offset, single_x + offset], [1, single_bar], linewidth=1.5, color=line_color)\n",
    "for emphasis in emphasize_subjects:\n",
    "    idx_subject = dataset.train_ids.index(emphasis)\n",
    "    ax.fill_between(\n",
    "        [idx_subject-0.1, idx_subject+0.6], 1 - max_delta, 1 + max_delta,\n",
    "        facecolor=viz.GREY_COLORS[6], alpha=0.3)\n",
    "    \n",
    "ax.set_xticks(x_axis)\n",
    "ax.set_xticklabels(dataset.train_ids)\n",
    "ax.axhline(1, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), handlelength=0.5)\n",
    "ax.set_ylim([1 - max_delta, 1 + max_delta])\n",
    "ax.set_xlabel(\"Subject\")\n",
    "ax.set_ylabel(\"Scaling factor\")\n",
    "ax.set_title(\"FFT-based scaling factor using different frequency ranges\", loc=\"left\")\n",
    "ax.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spindle parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = []\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    sigma_signal = utils.broad_filter(signal, fs, lowcut=9, highcut=17)\n",
    "    spindles = [sigma_signal[s_start:s_end] for (s_start, s_end) in stamps]\n",
    "    analysis = []\n",
    "    for spindle in spindles:\n",
    "        analysis_results = analyze_spindle(spindle, fs)\n",
    "        analysis.append(analysis_results)\n",
    "    analysis = listify_dictionaries(analysis)\n",
    "    train_analysis.append(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = {\n",
    "    \"labels\": dataset.train_ids, \"showfliers\": False, \"flierprops\": {'markersize': 2}, \n",
    "    \"medianprops\": dict(linewidth=2, color=viz.PALETTE['blue'], zorder=1)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5), dpi=200)\n",
    "axes[0, 0].set_title(\"Spindle Parameters\", fontweight='bold', loc=\"left\", fontsize=14)\n",
    "# Duration\n",
    "ax = axes[0, 0]\n",
    "ax.set_ylabel(\"Duration [s]\")\n",
    "ax.boxplot([analysis['duration'] for analysis in train_analysis], **common_args)\n",
    "ax.set_yticks([0.6, 0.8, 1.0, 1.2, 1.4])\n",
    "# Frequency\n",
    "ax = axes[1, 0]\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "ax.boxplot([analysis['central_freq_fft'] for analysis in train_analysis], **common_args)\n",
    "ax.set_yticks([12, 13, 14, 15])\n",
    "# Amplitude PP\n",
    "ax = axes[0, 1]\n",
    "ax.set_ylabel(\"Amplitude PP [V]\")\n",
    "ax.boxplot([analysis['pp_amplitude'] for analysis in train_analysis], **common_args)\n",
    "ax.set_yticks([20, 40, 60, 80])\n",
    "# Amplitude RMS\n",
    "ax = axes[1, 1]\n",
    "ax.set_ylabel(\"Amplitude RMS [V]\")\n",
    "ax.boxplot([analysis['rms'] for analysis in train_analysis], **common_args)\n",
    "ax.set_yticks([5, 10, 15, 20])\n",
    "for ax in axes.flatten():\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_scaling = [0, 100]\n",
    "raw_train_analysis = []\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "    n2_pages = np.where(hypno==\"2\")[0]\n",
    "    analysis_results = raw_spectrum(signal, fs, n2_pages, dataset.page_size)    \n",
    "    raw_train_analysis.append(analysis_results)\n",
    "scaled_train_analysis = []\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    hypno = dataset.get_subject_hypnogram(subject_id)\n",
    "    n2_pages = np.where(hypno==\"2\")[0]\n",
    "    scaling_factor = fft_based_scaling(signal, fs, n2_pages, dataset.page_size, band=band_scaling)\n",
    "    analysis_results = raw_spectrum(signal * scaling_factor, fs, n2_pages, dataset.page_size)    \n",
    "    scaled_train_analysis.append(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chosen_key = \"spectrum_window\"\n",
    "first_freq = 2\n",
    "last_freq = 25\n",
    "fit_lowcut = 1\n",
    "fit_highcut = 25\n",
    "show_freqs = [4, 8, 12, 16, 20]\n",
    "emphasize_subjects = [7, 18]  # [7, 14] # None # [5, 19] # [14, 18]\n",
    "show_power_law = False\n",
    "scaled_id = \"scaled %d-%d\" % (band_scaling[0], band_scaling[1])\n",
    "for train_analysis, analysis_id in zip([raw_train_analysis, scaled_train_analysis], [\"raw\", scaled_id]):\n",
    "    # Find tendency\n",
    "    coeffs = []\n",
    "    for analysis in train_analysis:\n",
    "        freq_full = analysis[chosen_key][:, 0]\n",
    "        first_freq_loc = np.argmin((freq_full - fit_lowcut) ** 2)\n",
    "        last_freq_loc = np.argmin((freq_full - fit_highcut) ** 2)\n",
    "        freq = analysis[chosen_key][first_freq_loc:last_freq_loc, 0]\n",
    "        power = analysis[chosen_key][first_freq_loc:last_freq_loc, 1]        \n",
    "        fit_x = np.log(freq)\n",
    "        fit_y = np.log(power)\n",
    "        coeff = np.polyfit(fit_x, fit_y, 1)\n",
    "        coeffs.append(coeff)\n",
    "    coeffs = np.stack(coeffs, axis=0).mean(axis=0)\n",
    "    def tendency(frequencies):\n",
    "        result = np.exp(coeffs[1]) * (frequencies ** coeffs[0])\n",
    "        return result\n",
    "    def flatten(frequencies, power):\n",
    "        result = power / (frequencies ** coeffs[0])\n",
    "        return result\n",
    "\n",
    "    # Plots\n",
    "    fig = plt.figure(figsize=(5.5, 6), dpi=200)\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 2.5])\n",
    "    \n",
    "    # True spectrum\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    subjects_spectrum_raw = []\n",
    "    for i, analysis in enumerate(train_analysis):\n",
    "        freq_full = analysis[chosen_key][:, 0]\n",
    "        first_freq_loc = np.argmin((freq_full - first_freq) ** 2)\n",
    "        last_freq_loc = np.argmin((freq_full - last_freq) ** 2)\n",
    "        freq = analysis[chosen_key][first_freq_loc:last_freq_loc, 0]\n",
    "        power = analysis[chosen_key][first_freq_loc:last_freq_loc, 1]\n",
    "        spectrum_raw = np.stack([freq, power], axis=1)\n",
    "        subjects_spectrum_raw.append(spectrum_raw)\n",
    "        if emphasize_subjects is None:\n",
    "            ax.plot(freq, power, linewidth=1)\n",
    "        else:\n",
    "            if dataset.train_ids[i] in emphasize_subjects:\n",
    "                ax.plot(freq, power, linewidth=1, color=viz.PALETTE['red'], zorder=30)\n",
    "            else:\n",
    "                ax.plot(freq, power, linewidth=1, color=viz.GREY_COLORS[5])\n",
    "    if show_power_law:\n",
    "        regr_freq = np.linspace(first_freq, last_freq)\n",
    "        ax.plot(regr_freq, tendency(regr_freq), '--k', linewidth=1.5, label=\"Power Law\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(show_freqs)\n",
    "    ax.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax.set_title(\"Spectrum (%s)\\n$\\propto f^{\\ a}$, $a=$%1.2f\" % (analysis_id, coeffs[0]), loc=\"left\")\n",
    "    \n",
    "    # True spectrum per subject\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    subjects_spectrum_raw = np.stack(subjects_spectrum_raw, axis=0)\n",
    "    min_val = subjects_spectrum_raw[:, :, 1].min()\n",
    "    max_val = subjects_spectrum_raw[:, :, 1].max()\n",
    "    for i, subject_spectrum in enumerate(subjects_spectrum_raw):\n",
    "        spectrum = i + (subject_spectrum[:, 1] - min_val) / (max_val - min_val)\n",
    "        freq_axis = subject_spectrum[:, 0]\n",
    "        if emphasize_subjects is None:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5)\n",
    "        else:\n",
    "            if dataset.train_ids[i] in emphasize_subjects:\n",
    "                line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.PALETTE['red'], zorder=30)\n",
    "            else:\n",
    "                line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.GREY_COLORS[5])\n",
    "        line_color = line[0].get_color()\n",
    "        ax.fill_between(freq_axis, spectrum, i, facecolor=line_color, linewidth=0.5)\n",
    "    ax.set_xticks(show_freqs)\n",
    "    ax.set_xlabel(\"Frequency [Hz]\")\n",
    "    ytick_locs = np.arange(len(dataset.train_ids)) + 0.3\n",
    "    ax.set_yticks(ytick_locs)\n",
    "    ax.set_yticklabels(dataset.train_ids)\n",
    "    ax.set_ylabel(\"Subject\")\n",
    "    ax.set_ylim([0, subjects_spectrum_raw.shape[0]])\n",
    "    \n",
    "    # Flat spectrum\n",
    "    ax = fig.add_subplot(gs[0, 1])\n",
    "    subjects_spectrum_flat = []\n",
    "    for i, analysis in enumerate(train_analysis):\n",
    "        freq_full = analysis[chosen_key][:, 0]\n",
    "        first_freq_loc = np.argmin((freq_full - first_freq) ** 2)\n",
    "        last_freq_loc = np.argmin((freq_full - last_freq) ** 2)\n",
    "        freq = analysis[chosen_key][first_freq_loc:last_freq_loc, 0]\n",
    "        power = analysis[chosen_key][first_freq_loc:last_freq_loc, 1]\n",
    "        power = flatten(freq, power)\n",
    "        spectrum_flat = np.stack([freq, power], axis=1)\n",
    "        subjects_spectrum_flat.append(spectrum_flat)\n",
    "        if emphasize_subjects is None:\n",
    "            ax.plot(freq, power, linewidth=1)\n",
    "        else:\n",
    "            if dataset.train_ids[i] in emphasize_subjects:\n",
    "                ax.plot(freq, power, linewidth=1, color=viz.PALETTE['red'], zorder=30)\n",
    "            else:\n",
    "                ax.plot(freq, power, linewidth=1, color=viz.GREY_COLORS[5])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(show_freqs)\n",
    "    ax.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax.set_title(\"Flattened (%s) \\n$power\\ /\\ f^{\\ a}$, $a=$%1.2f\" % (analysis_id, coeffs[0]), loc=\"left\")\n",
    "    \n",
    "    # Flat spectrum per subject\n",
    "    ax = fig.add_subplot(gs[1, 1])\n",
    "    subjects_spectrum_flat = np.stack(subjects_spectrum_flat, axis=0)\n",
    "    min_val = subjects_spectrum_flat[:, :, 1].min()\n",
    "    max_val = subjects_spectrum_flat[:, :, 1].max()\n",
    "    for i, subject_spectrum in enumerate(subjects_spectrum_flat):\n",
    "        spectrum = i + (subject_spectrum[:, 1] - min_val) / (max_val - min_val)\n",
    "        freq_axis = subject_spectrum[:, 0]\n",
    "        if emphasize_subjects is None:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5)\n",
    "        else:\n",
    "            if dataset.train_ids[i] in emphasize_subjects:\n",
    "                line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.PALETTE['red'], zorder=30)\n",
    "            else:\n",
    "                line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.GREY_COLORS[5])\n",
    "        line_color = line[0].get_color()\n",
    "        ax.fill_between(freq_axis, spectrum, i, facecolor=line_color, linewidth=0.5)\n",
    "    ax.set_xticks(show_freqs)\n",
    "    ax.set_xlabel(\"Frequency [Hz]\")\n",
    "    ytick_locs = np.arange(len(dataset.train_ids)) + 0.3\n",
    "    ax.set_yticks(ytick_locs)\n",
    "    ax.set_yticklabels(dataset.train_ids)\n",
    "    # ax.set_ylabel(\"Subject\")\n",
    "    ax.set_ylim([0, subjects_spectrum_flat.shape[0]])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spindle spectral parameters\n",
    "Like power ratios and spectrum of 2s centered on spindle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = []\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    stamps_center = stamps.mean(axis=1)\n",
    "    stamps[:, 0] = stamps_center - fs\n",
    "    stamps[:, 1] = stamps_center + fs\n",
    "    signal_on_spindles = [signal[s_start:s_end] for (s_start, s_end) in stamps]\n",
    "    analysis = []\n",
    "    for signal_window in signal_on_spindles:\n",
    "        analysis_results = analyze_spindle_window(signal_window, fs)\n",
    "        analysis.append(analysis_results)\n",
    "    analysis = listify_dictionaries(analysis)\n",
    "    train_analysis.append(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = {\n",
    "    \"labels\": dataset.train_ids, \"showfliers\": False, \"flierprops\": {'markersize': 2}, \n",
    "    \"medianprops\": dict(linewidth=2, color=viz.PALETTE['blue'], zorder=1)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5), dpi=200)\n",
    "axes[0, 0].set_title(\"Spindle Power Ratios \\n(used in previous detectors)\", fontweight='bold', loc=\"left\", fontsize=14)\n",
    "ax = axes[0, 0]\n",
    "ax.set_ylabel(\"Kulkarni PR\")\n",
    "ax.boxplot([analysis['pr_spindlenet'] for analysis in train_analysis], **common_args)\n",
    "ax = axes[0, 1]\n",
    "ax.set_ylabel(\"Lacourse PR\")\n",
    "ax.boxplot([analysis['pr_a7'] for analysis in train_analysis], **common_args)\n",
    "ax = axes[1, 0]\n",
    "ax.set_ylabel(\"Huupponen PR\")\n",
    "ax.boxplot([analysis['pr_huupp'] for analysis in train_analysis], **common_args)\n",
    "ax = axes[1, 1]\n",
    "ax.set_ylabel(\"Huupponen Alpha PR\")\n",
    "ax.boxplot([analysis['pr_huupp_alfa'] for analysis in train_analysis], **common_args)\n",
    "for ax in axes.flatten():\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = []\n",
    "bands_list = [\n",
    "    [0.5, 2],\n",
    "    [2, 4],\n",
    "    [4, 6],\n",
    "    [6, 8],\n",
    "    [8, 10],\n",
    "    [10, 12],\n",
    "    [12, 14],\n",
    "    [14, 16],\n",
    "    [16, 18],\n",
    "    [18, 20]\n",
    "]\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    stamps_center = stamps.mean(axis=1)\n",
    "    stamps[:, 0] = stamps_center - fs\n",
    "    stamps[:, 1] = stamps_center + fs\n",
    "    signal_on_spindles = [signal[s_start:s_end] for (s_start, s_end) in stamps]\n",
    "    analysis = []\n",
    "    for signal_window in signal_on_spindles:\n",
    "        analysis_results = analyze_spindle_window_custom(signal_window, fs, bands_list)\n",
    "        analysis.append(analysis_results)\n",
    "    analysis = listify_dictionaries(analysis)\n",
    "    train_analysis.append(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_args = {\n",
    "    \"labels\": dataset.train_ids, \"showfliers\": False, \"flierprops\": {'markersize': 2}, \n",
    "    \"medianprops\": dict(linewidth=2, color=viz.PALETTE['blue'], zorder=1)\n",
    "}\n",
    "n_bands = len(bands_list)\n",
    "\n",
    "fig, axes = plt.subplots(n_bands, 1, figsize=(4, 2 * n_bands), dpi=100)\n",
    "axes[0].set_title(\"Spindle Power Ratios\\n(simple)\", fontweight='bold', loc=\"left\", fontsize=14)\n",
    "for j in range(n_bands):\n",
    "    axes[j].set_ylabel(\"%s Hz\" % bands_list[j])\n",
    "    axes[j].boxplot([analysis['band_%d' % j] for analysis in train_analysis], **common_args)\n",
    "for ax in axes.flatten():\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "train_analysis = []\n",
    "for subject_id in dataset.train_ids:\n",
    "    signal = dataset.get_subject_signal(subject_id, normalize_clip=False)\n",
    "    stamps = dataset.get_subject_stamps(subject_id, which_expert=1, pages_subset=constants.N2_RECORD)\n",
    "    stamps_center = stamps.mean(axis=1)\n",
    "    stamps[:, 0] = (stamps_center - window_size * fs / 2).astype(np.int32)\n",
    "    stamps[:, 1] = stamps[:, 0] + window_size * fs\n",
    "    signal_on_spindles = [signal[s_start:s_end] for (s_start, s_end) in stamps]\n",
    "    analysis = []\n",
    "    for signal_window in signal_on_spindles:\n",
    "        analysis_results = analyze_spindle_window_spectrum(signal_window, fs)\n",
    "        analysis.append(analysis_results)\n",
    "    analysis = listify_dictionaries(analysis)\n",
    "    train_analysis.append(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_freq = 2\n",
    "last_freq = 20\n",
    "fit_lowcut = 1\n",
    "fit_highcut = 25\n",
    "show_freqs = [4, 8, 12, 16, 20]\n",
    "emphasize_subjects = [14, 18] # [11, 14, 19] # None # [5, 19] # [14, 18]\n",
    "\n",
    "# Plots\n",
    "fig = plt.figure(figsize=(5.5, 6), dpi=200)\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 2.5])\n",
    "\n",
    "# True spectrum\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "subjects_spectrum_raw = []\n",
    "for i, analysis in enumerate(train_analysis):\n",
    "    freq_full = analysis[\"spectrum\"][0][:, 0]\n",
    "    first_freq_loc = np.argmin((freq_full - first_freq) ** 2)\n",
    "    last_freq_loc = np.argmin((freq_full - last_freq) ** 2)\n",
    "    freq = analysis[\"spectrum\"][0][first_freq_loc:last_freq_loc, 0]\n",
    "    powers = [spec[first_freq_loc:last_freq_loc, 1] for spec in analysis[\"spectrum\"]]\n",
    "    powers = np.stack(powers, axis=0)\n",
    "    mean_power = powers.mean(axis=0)\n",
    "    spectrum_raw = np.stack([freq, mean_power], axis=1)\n",
    "    subjects_spectrum_raw.append(spectrum_raw)\n",
    "    if emphasize_subjects is None:\n",
    "        ax.plot(freq, mean_power, linewidth=1)\n",
    "    else:\n",
    "        if dataset.train_ids[i] in emphasize_subjects:\n",
    "            ax.plot(freq, mean_power, linewidth=1, color=viz.PALETTE['red'], zorder=30)\n",
    "        else:\n",
    "            ax.plot(freq, mean_power, linewidth=1, color=viz.GREY_COLORS[5])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(show_freqs)\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ax.set_title(\"Spectrum on Spindles\\n(%d s window)\" % window_size, loc=\"left\")\n",
    "\n",
    "# True spectrum per subject\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "subjects_spectrum_raw = np.stack(subjects_spectrum_raw, axis=0)\n",
    "min_val = subjects_spectrum_raw[:, :, 1].min()\n",
    "max_val = subjects_spectrum_raw[:, :, 1].max()\n",
    "for i, subject_spectrum in enumerate(subjects_spectrum_raw):\n",
    "    spectrum = i + (subject_spectrum[:, 1] - min_val) / (max_val - min_val)\n",
    "    freq_axis = subject_spectrum[:, 0]\n",
    "    if emphasize_subjects is None:\n",
    "        line = ax.plot(freq_axis, spectrum, linewidth=0.5)\n",
    "    else:\n",
    "        if dataset.train_ids[i] in emphasize_subjects:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.PALETTE['red'], zorder=30)\n",
    "        else:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.GREY_COLORS[5])\n",
    "    line_color = line[0].get_color()\n",
    "    ax.fill_between(freq_axis, spectrum, i, facecolor=line_color, linewidth=0.5)\n",
    "ax.set_xticks(show_freqs)\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ytick_locs = np.arange(len(dataset.train_ids)) + 0.3\n",
    "ax.set_yticks(ytick_locs)\n",
    "ax.set_yticklabels(dataset.train_ids)\n",
    "ax.set_ylim([0, subjects_spectrum_raw.shape[0]])\n",
    "ax.set_ylabel(\"Subject\")\n",
    "\n",
    "# spectrum divided by sigma power\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "subjects_spectrum_flat = []\n",
    "for i, analysis in enumerate(train_analysis):\n",
    "    freq_full = analysis[\"spectrum_sigma_norm\"][0][:, 0]\n",
    "    first_freq_loc = np.argmin((freq_full - first_freq) ** 2)\n",
    "    last_freq_loc = np.argmin((freq_full - last_freq) ** 2)\n",
    "    freq = analysis[\"spectrum_sigma_norm\"][0][first_freq_loc:last_freq_loc, 0]\n",
    "    powers = [spec[first_freq_loc:last_freq_loc, 1] for spec in analysis[\"spectrum_sigma_norm\"]]\n",
    "    powers = np.stack(powers, axis=0)\n",
    "    mean_power = powers.mean(axis=0)\n",
    "    spectrum_flat = np.stack([freq, mean_power], axis=1)\n",
    "    subjects_spectrum_flat.append(spectrum_flat)\n",
    "    if emphasize_subjects is None:\n",
    "        ax.plot(freq, mean_power, linewidth=1)\n",
    "    else:\n",
    "        if dataset.train_ids[i] in emphasize_subjects:\n",
    "            ax.plot(freq, mean_power, linewidth=1, color=viz.PALETTE['red'], zorder=30)\n",
    "        else:\n",
    "            ax.plot(freq, mean_power, linewidth=1, color=viz.GREY_COLORS[5])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(show_freqs)\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ax.set_title(\"Sigma-Normalized\", loc=\"left\")\n",
    "\n",
    "# Flat spectrum per subject\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "subjects_spectrum_flat = np.stack(subjects_spectrum_flat, axis=0)\n",
    "min_val = subjects_spectrum_flat[:, :, 1].min()\n",
    "max_val = subjects_spectrum_flat[:, :, 1].max()\n",
    "for i, subject_spectrum in enumerate(subjects_spectrum_flat):\n",
    "    spectrum = i + (subject_spectrum[:, 1] - min_val) / (max_val - min_val)\n",
    "    freq_axis = subject_spectrum[:, 0]\n",
    "    if emphasize_subjects is None:\n",
    "        line = ax.plot(freq_axis, spectrum, linewidth=0.5)\n",
    "    else:\n",
    "        if dataset.train_ids[i] in emphasize_subjects:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.PALETTE['red'], zorder=30)\n",
    "        else:\n",
    "            line = ax.plot(freq_axis, spectrum, linewidth=0.5, color=viz.GREY_COLORS[5])\n",
    "    line_color = line[0].get_color()\n",
    "    ax.fill_between(freq_axis, spectrum, i, facecolor=line_color, linewidth=0.5)\n",
    "ax.set_xticks(show_freqs)\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ytick_locs = np.arange(len(dataset.train_ids)) + 0.3\n",
    "ax.set_yticks(ytick_locs)\n",
    "ax.set_yticklabels(dataset.train_ids)\n",
    "ax.set_ylim([0, subjects_spectrum_flat.shape[0]])\n",
    "# ax.set_ylabel(\"Subject\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "src_ckpt_folder = '20200724_reproduce_red_n2_train_mass_ss/v19_rep1'\n",
    "seed_id_list = [0, 1, 2, 3]\n",
    "\n",
    "which_expert = 1\n",
    "task_mode = constants.N2_RECORD\n",
    "set_list = [constants.VAL_SUBSET, constants.TRAIN_SUBSET]\n",
    "iou_thr = 0.2\n",
    "iou_hist_bins = np.linspace(0, 1, 21)\n",
    "iou_curve_axis = misc.custom_linspace(0.05, 0.95, 0.05)\n",
    "ids_dict = {constants.ALL_TRAIN_SUBSET: dataset.train_ids, constants.TEST_SUBSET: dataset.test_ids}\n",
    "ids_dict.update(misc.get_splits_dict(dataset, seed_id_list))\n",
    "\n",
    "# Load predictions\n",
    "src_predictions_dict = reader.read_prediction_with_seeds(\n",
    "    src_ckpt_folder, dataset_name, task_mode, seed_id_list, set_list=set_list, parent_dataset=dataset)\n",
    "\n",
    "# Adjust validation probabilities\n",
    "for seed_id in seed_id_list:\n",
    "    val_probas = src_predictions_dict[seed_id]['val'].probabilities_dict\n",
    "    seed_thr = OPTIMAL_THR_FOR_CKPT_DICT[src_ckpt_folder][seed_id]\n",
    "    for subject_id in ids_dict[seed_id]['val']:\n",
    "        subject_proba = val_probas[subject_id].astype(np.float32)\n",
    "        subject_proba = center_probabilities(subject_proba, seed_thr).astype(np.float16)\n",
    "        # Overwrite original probability vector\n",
    "        src_predictions_dict[seed_id]['val'].probabilities_dict[subject_id] = subject_proba\n",
    "    # Update stamps with new threshold\n",
    "    src_predictions_dict[seed_id]['val'].set_probability_threshold(0.5)\n",
    "print(\"Adjusted validation probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects 11, 14 and 19 (low precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted probability analysis\n",
    "True Spindles are simply the expert annotations.\n",
    "False Spindles are found as detections with adjusted probability greater than 0.2 not intersecting with True Spindles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_thr = 0.2\n",
    "subject_ids = [14]\n",
    "n_subjects = len(subject_ids)\n",
    "\n",
    "spindles_dict = {}\n",
    "for subject_id in subject_ids:\n",
    "    subject_real_spindles = dataset.get_subject_stamps(\n",
    "        subject_id, which_expert=which_expert, pages_subset=task_mode)\n",
    "    subject_seed = np.where([subject_id in ids_dict[seed_id]['val'] for seed_id in seed_id_list])[0][-1]\n",
    "    subject_detections = src_predictions_dict[subject_seed]['val'].get_subject_stamps(\n",
    "        subject_id, which_expert=which_expert, pages_subset=task_mode)\n",
    "    # Generate false spindles\n",
    "    src_predictions_dict[subject_seed]['val'].set_probability_threshold(false_thr)\n",
    "    subject_false_spindles_candidates = src_predictions_dict[subject_seed]['val'].get_subject_stamps(\n",
    "        subject_id, which_expert=which_expert, pages_subset=task_mode)\n",
    "    src_predictions_dict[subject_seed]['val'].set_probability_threshold(0.5)\n",
    "    _, idx_matching = metrics.matching(subject_false_spindles_candidates, subject_real_spindles)\n",
    "    false_spindles_loc = np.where(idx_matching == -1)[0]\n",
    "    subject_false_spindles = subject_false_spindles_candidates[false_spindles_loc]\n",
    "    # Compute probabilities of events\n",
    "    subject_proba = src_predictions_dict[subject_seed]['val'].probabilities_dict[subject_id]\n",
    "    real_proba_segments = [\n",
    "        subject_proba[start_sample//8:end_sample//8].astype(np.float32) \n",
    "        for (start_sample, end_sample) in subject_real_spindles\n",
    "    ]\n",
    "    false_proba_segments = [\n",
    "        subject_proba[start_sample//8:end_sample//8].astype(np.float32)\n",
    "        for (start_sample, end_sample) in subject_false_spindles\n",
    "    ]\n",
    "    real_mean_proba = np.array([np.mean(p) for p in real_proba_segments])\n",
    "    false_mean_proba = np.array([np.mean(p) for p in false_proba_segments])\n",
    "    real_max_proba = np.array([np.max(p) for p in real_proba_segments])\n",
    "    false_max_proba = np.array([np.max(p) for p in false_proba_segments])\n",
    "    spindles_dict[subject_id] = {\n",
    "        'real_events': subject_real_spindles, \n",
    "        'false_events': subject_false_spindles,\n",
    "        'real_mean_proba': real_mean_proba,\n",
    "        'false_mean_proba': false_mean_proba,\n",
    "        'real_max_proba': real_max_proba,\n",
    "        'false_max_proba': false_max_proba,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability histogram\n",
    "kwargs = dict(range=(0, 1), bins=10, density=False, histtype='step')\n",
    "fig, axes = plt.subplots(n_subjects, 1, figsize=(4, 2*n_subjects), dpi=100)\n",
    "for i in range(n_subjects):\n",
    "    subject_id = subject_ids[i]\n",
    "    ax = axes if n_subjects == 1 else axes[i]\n",
    "    ax.hist(spindles_dict[subject_id]['real_mean_proba'], color=viz.PALETTE['blue'], label=\"Real Event\", **kwargs)\n",
    "    ax.hist(spindles_dict[subject_id]['false_mean_proba'], color=viz.PALETTE['red'], label=\"False Event\", **kwargs)\n",
    "    ax.set_title(\"Subject %d\" % subject_id, loc=\"left\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Mean Probability\")\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of probability versus parameter: frequency, amplitude, spectral properties\n",
    "# For spectral properties: compute some ratios, \n",
    "# and we could compute the average spectral profile of real and average spectral profile of false, both raw and sigma-normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
