{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, gridspec\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "from scipy.signal import convolve2d, firwin\n",
    "\n",
    "project_root = '..'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.common import constants, pkeys, viz\n",
    "from sleeprnn.common.optimal_thresholds import OPTIMAL_THR_FOR_CKPT_DICT\n",
    "from sleeprnn.data import utils, stamp_correction\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection import metrics\n",
    "from sleeprnn.helpers import reader, misc\n",
    "from sleeprnn.data.mass_ss import PATH_MASS_RELATIVE, PATH_REC, PATH_MARKS, PATH_STATES\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')\n",
    "MASS_PATH = os.path.join(project_root, utils.PATH_DATA, PATH_MASS_RELATIVE)\n",
    "\n",
    "%matplotlib inline\n",
    "viz.notebook_full_width()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load subject's data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 14\n",
    "\n",
    "annot_chn = 'EEG C3-CLE'\n",
    "fs = 256\n",
    "page_duration = 20\n",
    "state_ids = np.array(['1', '2', '3', '4', 'R', 'W', '?'])\n",
    "unknown_id = '?'  # Character for unknown state in hypnogram\n",
    "n2_id = '2'  # Character for N2 identification in hypnogram\n",
    "hypno_num_dict = {'1': -1, '2': -2, '3': -3, '4': -4, 'R': 0, 'W': 1, '?': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PSG\n",
    "file_rec = os.path.join(MASS_PATH, PATH_REC, '01-02-%04d PSG.edf' % subject_id)\n",
    "signals = {}\n",
    "ignore_channels = ['Resp nasal']\n",
    "with pyedflib.EdfReader(file_rec) as file:\n",
    "    print(\"Reading %s\" % file_rec)\n",
    "    channel_names = file.getSignalLabels()\n",
    "    for i, name in enumerate(channel_names):\n",
    "        if name in ignore_channels:\n",
    "            continue\n",
    "        this_signal = file.readSignal(i)\n",
    "        fs_decimal = file.samplefrequency(i)\n",
    "        # Particular fix for mass dataset:\n",
    "        fs_rounded = int(np.round(fs_decimal))\n",
    "        # Transform the original fs frequency with decimals to rounded version\n",
    "        this_signal = utils.resample_signal_linear(this_signal, fs_old=fs_decimal, fs_new=fs_rounded)\n",
    "        # Now resample to the required frequency\n",
    "        if fs != fs_rounded:\n",
    "            print('Resampling from %d Hz to required %d Hz' % (fs_rounded, fs))\n",
    "            this_signal = utils.resample_signal(this_signal, fs_old=fs_rounded, fs_new=fs)\n",
    "        this_signal = this_signal.astype(np.float32)\n",
    "        signals[name] = this_signal\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypnogram_str2num(my_hypno, state_ids, num_values_dict):\n",
    "    hypno_num = np.zeros(my_hypno.size)\n",
    "    for single_state_id in state_ids:\n",
    "        hypno_num[my_hypno == single_state_id] = num_values_dict[single_state_id]\n",
    "    return hypno_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations -- Hypnogram\n",
    "file_states = os.path.join(MASS_PATH, PATH_STATES, '01-02-%04d Base.edf' % subject_id)\n",
    "# Total pages not necessarily equal to total_annots\n",
    "page_size = int(page_duration * fs)\n",
    "signal_length = signals[annot_chn].size\n",
    "total_pages = int(np.ceil(signal_length / page_size))\n",
    "with pyedflib.EdfReader(file_states) as file:\n",
    "    print(\"Reading %s\" % file_states)\n",
    "    annotations = file.readAnnotations()\n",
    "onsets = np.array(annotations[0])\n",
    "durations = np.round(np.array(annotations[1]))\n",
    "stages_str = annotations[2]\n",
    "# keep only 20s durations\n",
    "valid_idx = (durations == page_duration)\n",
    "onsets = onsets[valid_idx]\n",
    "onsets_pages = np.round(onsets / page_duration).astype(np.int32)\n",
    "stages_str = stages_str[valid_idx]\n",
    "stages_char = [single_annot[-1] for single_annot in stages_str]\n",
    "# Build complete hypnogram\n",
    "total_annots = len(stages_char)\n",
    "not_unkown_ids = [state_id for state_id in state_ids if state_id != unknown_id]\n",
    "not_unkown_state_dict = {}\n",
    "for state_id in not_unkown_ids:\n",
    "    state_idx = np.where([stages_char[i] == state_id for i in range(total_annots)])[0]\n",
    "    not_unkown_state_dict[state_id] = onsets_pages[state_idx]\n",
    "hypnogram = []\n",
    "for page in range(total_pages):\n",
    "    state_not_found = True\n",
    "    for state_id in not_unkown_ids:\n",
    "        if page in not_unkown_state_dict[state_id] and state_not_found:\n",
    "            hypnogram.append(state_id)\n",
    "            state_not_found = False\n",
    "    if state_not_found:\n",
    "        hypnogram.append(unknown_id)\n",
    "hypnogram = np.asarray(hypnogram)\n",
    "hypnogram_int = hypnogram_str2num(hypnogram, state_ids, hypno_num_dict)\n",
    "# Extract N2 pages\n",
    "n2_pages = np.where(hypnogram == n2_id)[0]\n",
    "# Drop first, last and second to last page of the whole registers if they where selected.\n",
    "last_page = total_pages - 1\n",
    "n2_pages = n2_pages[(n2_pages != 0) & (n2_pages != last_page) & (n2_pages != last_page - 1)]\n",
    "n2_pages = n2_pages.astype(np.int16)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations -- Spindles\n",
    "file_marks = os.path.join(MASS_PATH, PATH_MARKS, '01-02-%04d SpindleE1.edf' % subject_id)\n",
    "with pyedflib.EdfReader(file_marks) as file:\n",
    "    print(\"Reading %s\" % file_marks)\n",
    "    annotations = file.readAnnotations()\n",
    "onsets = np.array(annotations[0])\n",
    "durations = np.array(annotations[1])\n",
    "offsets = onsets + durations\n",
    "marks_time = np.stack((onsets, offsets), axis=1)  # time-stamps\n",
    "# Transforms to sample-stamps\n",
    "marks = np.round(marks_time * fs).astype(np.int32)\n",
    "# Combine marks that are too close according to standards\n",
    "marks = stamp_correction.combine_close_stamps(marks, fs, 0.3)\n",
    "# Fix durations that are outside standards\n",
    "marks = stamp_correction.filter_duration_stamps(marks, fs, 0.3, 3.0)\n",
    "# keep only N2\n",
    "marks = utils.extract_pages_for_stamps(marks, n2_pages, page_size)\n",
    "# Build binary sequence\n",
    "marks_bin = utils.stamp2seq(marks, 0, signals[annot_chn].size - 1)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_probabilities(probabilities, center):\n",
    "    \"\"\"input: probas with class 1 iff proba > center.\n",
    "    output: probas with class 1 iff proba > 0.5\n",
    "    \"\"\"\n",
    "    probabilities = probabilities.astype(np.float32)\n",
    "    bias_center = np.log(center / (1-center))\n",
    "    eps = 1e-6\n",
    "    probabilities = np.clip(probabilities, eps, 1-eps)\n",
    "    logits = np.log(probabilities / (1 - probabilities))\n",
    "    logits = logits - bias_center\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    probabilities = probabilities.astype(np.float16)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions -- RED-CWT\n",
    "ckpt_folder = '20200724_reproduce_red_n2_train_mass_ss/v19_rep1'\n",
    "fs_proba = 25\n",
    "fs_detections = 200\n",
    "seed_id_list = [0, 1, 2, 3]\n",
    "set_list = [constants.VAL_SUBSET]\n",
    "dataset_name = constants.MASS_SS_NAME\n",
    "task_mode = constants.N2_RECORD\n",
    "predictions_dict = reader.read_prediction_with_seeds(ckpt_folder, dataset_name, task_mode, seed_id_list, set_list)\n",
    "print(\"Selecting subject's probabilities and detections.\")\n",
    "for seed_id in seed_id_list:\n",
    "    seed_val_subjects = predictions_dict[seed_id][constants.VAL_SUBSET].all_ids\n",
    "    if subject_id in seed_val_subjects:\n",
    "        seed_thr = OPTIMAL_THR_FOR_CKPT_DICT[ckpt_folder][seed_id]\n",
    "        predictions_dict[seed_id][constants.VAL_SUBSET].set_probability_threshold(seed_thr)\n",
    "        probabilities = predictions_dict[seed_id][constants.VAL_SUBSET].get_subject_probabilities(subject_id).copy()\n",
    "        probabilities = center_probabilities(probabilities, seed_thr)\n",
    "        detections = predictions_dict[seed_id][constants.VAL_SUBSET].get_subject_stamps(subject_id).copy()\n",
    "        detections = (detections.astype(np.float32) * fs / fs_detections).astype(np.int32)\n",
    "        detections_bin = utils.stamp2seq(detections, 0, signals[annot_chn].size - 1)\n",
    "        break\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 4), dpi=100)\n",
    "# gs = gridspec.GridSpec(3, 1, height_ratios=[4, 2, 1])\n",
    "fig, ax = plt.subplots(5, 1, figsize=(12, 6), dpi=100, sharex=True)\n",
    "\n",
    "# Hypnogram\n",
    "hypnogram_times_to_plot = np.stack([np.arange(hypnogram.size), np.arange(1, hypnogram.size+1)], axis=1).flatten() * page_duration\n",
    "hypnogram_int_to_plot = np.stack([hypnogram_int, hypnogram_int], axis=1).flatten()\n",
    "ax[0].plot(hypnogram_times_to_plot, hypnogram_int_to_plot, linewidth=0.8)\n",
    "ax[0].set_yticks([hypno_num_dict[s] for s in state_ids])\n",
    "ax[0].set_yticklabels(state_ids)\n",
    "ax[0].set_title(\"S%02d - Hypnogram\" % subject_id)\n",
    "\n",
    "# Signal\n",
    "signal_to_plot = signals[annot_chn]\n",
    "signal_times_to_plot = np.arange(signal_to_plot.size) / fs\n",
    "ax[1].plot(signal_times_to_plot, signal_to_plot, linewidth=0.6)\n",
    "ax[1].set_title(\"S%02d - Signal %s\" % (subject_id, annot_chn))\n",
    "\n",
    "# Marks\n",
    "ax[2].plot(signal_times_to_plot, marks_bin, linewidth=0.6)\n",
    "ax[2].set_title(\"S%02d - Marks\" % subject_id)\n",
    "\n",
    "# Detections\n",
    "ax[3].plot(signal_times_to_plot, detections_bin, linewidth=0.6)\n",
    "ax[3].set_title(\"S%02d - Detections\" % subject_id)\n",
    "\n",
    "# Probabilities\n",
    "proba_times_to_plot = np.arange(probabilities.size) / fs_proba\n",
    "ax[4].plot(proba_times_to_plot, probabilities, linewidth=0.6)\n",
    "ax[4].set_title(\"S%02d - Probabilities\" % subject_id)\n",
    "ax[4].set_xlabel(\"Time [s]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of marks and predictions over night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_centers = marks.mean(axis=1) / fs\n",
    "detections_centers = detections.mean(axis=1) / fs\n",
    "\n",
    "pages_per_bin = 3\n",
    "n_pages = hypnogram.size\n",
    "bin_width = pages_per_bin * page_duration\n",
    "last_second = n_pages * page_duration\n",
    "bins = np.arange(0, last_second+1, bin_width)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 4), dpi=100, sharex=True, sharey=True)\n",
    "ax[0].hist(marks_centers, bins=bins)\n",
    "ax[0].set_title(\"Marks centers\")\n",
    "\n",
    "ax[1].hist(detections_centers, bins=bins)\n",
    "ax[1].set_title(\"Detections centers\")\n",
    "ax[1].set_xlabel(\"Time [s]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single channel visualization with bandpass filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fir_filter_np(signal, kernel):\n",
    "    new_signal = convolve2d(signal.reshape(1, -1), kernel.reshape(1, -1), mode=\"same\")\n",
    "    new_signal = new_signal.flatten()\n",
    "    return new_signal\n",
    "\n",
    "\n",
    "def lowpass_np(signal, fs, cutoff, filter_duration_ref=6, wave_expansion_factor=0.5):\n",
    "    numtaps = fs * filter_duration_ref / (cutoff ** wave_expansion_factor)\n",
    "    numtaps = int(2 * (numtaps // 2) + 1)  # ensure odd numtaps\n",
    "    lp_kernel = firwin(numtaps, cutoff=cutoff, window=\"hamming\", fs=fs).astype(np.float32)\n",
    "    lp_kernel /= lp_kernel.sum()\n",
    "    new_signal = apply_fir_filter_np(signal, lp_kernel)\n",
    "    return new_signal\n",
    "\n",
    "\n",
    "def highpass_np(signal, fs, cutoff, filter_duration_ref=6, wave_expansion_factor=0.5):\n",
    "    numtaps = fs * filter_duration_ref / (cutoff ** wave_expansion_factor)\n",
    "    numtaps = int(2 * (numtaps // 2) + 1)  # ensure odd numtaps\n",
    "    lp_kernel = firwin(numtaps, cutoff=cutoff, window=\"hamming\", fs=fs).astype(np.float32)\n",
    "    lp_kernel /= lp_kernel.sum()\n",
    "    # HP = delta - LP\n",
    "    hp_kernel = -lp_kernel\n",
    "    hp_kernel[numtaps//2] += 1\n",
    "    new_signal = apply_fir_filter_np(signal, hp_kernel)\n",
    "    return new_signal\n",
    "\n",
    "\n",
    "def bandpass_np(signal, fs, lowcut, highcut, filter_duration_ref=6, wave_expansion_factor=0.5):\n",
    "    new_signal = signal\n",
    "    if lowcut is not None:\n",
    "        new_signal = highpass_np(\n",
    "            new_signal, fs, lowcut, filter_duration_ref, wave_expansion_factor)\n",
    "    if highcut is not None:\n",
    "        new_signal = lowpass_np(\n",
    "            new_signal, fs, highcut, filter_duration_ref, wave_expansion_factor)\n",
    "    return new_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandpass_filters = {'theta': [4, 8], 'alpha': [8, 12], 'sigma': [11, 16], 'beta': [16, 30]}\n",
    "n_filters = len(bandpass_filters)\n",
    "# Filter\n",
    "band_names = ['theta', 'alpha', 'sigma', 'beta']\n",
    "band_signals = {}\n",
    "for band_name in band_names:\n",
    "    print(\"Processing band %s\" % band_name, flush=True)\n",
    "    lowcut, highcut = bandpass_filters[band_name]\n",
    "    band_signal = bandpass_np(signals[annot_chn], fs, lowcut, highcut).astype(np.float32)\n",
    "    band_signals[band_name] = band_signal\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal_with_filters(page_idx, show_only_n2=False, border=5):\n",
    "    fig = plt.figure(figsize=(12, 4), dpi=120)\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[1, 6])\n",
    "    \n",
    "    page_loc = n2_pages[page_idx] if show_only_n2 else page_idx\n",
    "    page_loc = max(page_loc, 1)\n",
    "    \n",
    "    # Probability\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    start_sample_proba = int(page_loc * page_duration * fs_proba - border * fs_proba)\n",
    "    end_sample_proba = int((page_loc + 1) * page_duration * fs_proba + border * fs_proba)\n",
    "    time_axis_proba = np.arange(start_sample_proba, end_sample_proba) / fs_proba\n",
    "    ax.plot(time_axis_proba, probabilities[start_sample_proba:end_sample_proba], linewidth=1.1, color=viz.PALETTE['red'])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlim(start_sample_proba / fs_proba, end_sample_proba / fs_proba)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticks(np.arange(start_sample_proba / fs_proba, end_sample_proba / fs_proba, 1), minor=True)\n",
    "    ax.set_yticks([0.5], minor=True)\n",
    "    ax.grid(which=\"minor\")\n",
    "    ax.set_title(\"S%02d - Page %d (Stage %s)\" % (subject_id, page_loc, hypnogram[page_loc]))\n",
    "    \n",
    "    # Signal + bands\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    start_sample = int(page_loc * page_duration * fs - border * fs)\n",
    "    end_sample = int((page_loc + 1) * page_duration * fs + border * fs)\n",
    "    time_axis = np.arange(start_sample, end_sample) / fs\n",
    "    ax.plot(time_axis, signals[annot_chn][start_sample:end_sample], linewidth=0.6, color=viz.PALETTE['dark'])\n",
    "    ax.set_xlim(start_sample / fs, end_sample / fs)\n",
    "    dy = 70\n",
    "    offset = -150 + dy\n",
    "    offsets_list = []\n",
    "    grid_list = []\n",
    "    for i, band_name in enumerate(band_names):\n",
    "        offset -= dy\n",
    "        offsets_list.append(offset)\n",
    "        grid_list.extend([offset - 20, offset - 10, offset + 10, offset + 20])\n",
    "        ax.plot(\n",
    "            time_axis, \n",
    "            band_signals[band_name][start_sample:end_sample] + offset, \n",
    "            linewidth=0.6, color=viz.PALETTE['blue'],\n",
    "            label=band_name)\n",
    "    ax.set_ylim([offset - 50, 150])\n",
    "    ax.set_yticks(offsets_list + [-100, -50, 0, 50, 100])\n",
    "    ax.set_yticks(grid_list, minor=True)\n",
    "    ax.set_yticklabels(band_names + [-100, -50, 0, 50, 100])\n",
    "    ax.set_xticks(np.arange(start_sample / fs, end_sample / fs, 1), minor=True)\n",
    "    ax.grid(which=\"minor\")\n",
    "    ax.set_xticks(np.arange(start_sample / fs, end_sample / fs + 0.1, 5))\n",
    "    \n",
    "    # Detections and marks\n",
    "    marks_in_page = utils.filter_stamps(marks, start_sample, end_sample)\n",
    "    for s_mark in marks_in_page:\n",
    "        ax.fill_between(s_mark / fs, -100, 0, alpha=0.3, facecolor=viz.PALETTE['blue'])\n",
    "    dets_in_page = utils.filter_stamps(detections, start_sample, end_sample)\n",
    "    for s_mark in dets_in_page:\n",
    "        ax.fill_between(s_mark / fs, 100, 0, alpha=0.3, facecolor=viz.PALETTE['red'])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_only_n2 = True\n",
    "start_value = 0\n",
    "\n",
    "max_pages = n2_pages.size if show_only_n2 else hypnogram.size\n",
    "max_pages -= 1\n",
    "style = {'description_width': 'initial'}\n",
    "layout= widgets.Layout(width='1000px')\n",
    "widgets.interact(\n",
    "    lambda page_idx: plot_signal_with_filters(page_idx, show_only_n2=show_only_n2),\n",
    "    page_idx=widgets.IntSlider(\n",
    "        min=0, max=max_pages, step=1, value=start_value, continuous_update=False, style=style, layout=layout));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All channels visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
