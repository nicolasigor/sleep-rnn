{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee31004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "# TF logging control\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from sleeprnn.data import utils\n",
    "from sleeprnn.detection.feeder_dataset import FeederDataset\n",
    "from sleeprnn.detection.predicted_dataset import PredictedDataset\n",
    "from sleeprnn.nn.models import WaveletBLSTM\n",
    "from sleeprnn.helpers.reader import load_dataset\n",
    "from sleeprnn.common import constants\n",
    "from sleeprnn.common import checks\n",
    "from sleeprnn.common import pkeys\n",
    "\n",
    "RESULTS_PATH = os.path.join(project_root, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33f042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2_subsampling_factors_dict = {\n",
    "    constants.MODA_SS_NAME: [12.5, 25.0, 50.0, 100.0],  # [15.0, 30.0, 60.1, 100.0],\n",
    "    constants.MASS_SS_NAME: [6.2, 12.5, 25.0, 50.0, 100.0],\n",
    "    constants.INTA_SS_NAME: [11.4, 22.8, 45.5, 100.0],\n",
    "    constants.CAP_SS_NAME: [5, 10, 20, 40, 80, 100],\n",
    "}\n",
    "\n",
    "datasets_list = [\n",
    "    dict(dataset_name=constants.MASS_SS_NAME, which_expert=1),\n",
    "    # dict(dataset_name=constants.MODA_SS_NAME, which_expert=1),\n",
    "    # dict(dataset_name=constants.INTA_SS_NAME, which_expert=1),\n",
    "    # dict(dataset_name=constants.CAP_SS_NAME, which_expert=1),\n",
    "]\n",
    "ds = datasets_list[0]\n",
    "\n",
    "dataset_name = ds['dataset_name']\n",
    "which_expert = ds['which_expert']\n",
    "dataset = load_dataset(dataset_name, verbose=False)\n",
    "subject_ids =  dataset.all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce233dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mass_ss\n",
      "ids [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 17, 18, 19]\n",
      "\n",
      "Size 6.2, number of events retrieved 11\n",
      "Size 6.2, number of pages retrieved 677\n",
      "pages [47, 57, 31, 40, 42, 44, 46, 44, 40, 43, 51, 46, 50, 46, 50]\n",
      "events [0, 1, 0, 0, 0, 5, 1, 0, 1, 0, 0, 2, 0, 1, 0]\n",
      "\n",
      "Size 12.5, number of events retrieved 107\n",
      "Size 12.5, number of pages retrieved 1357\n",
      "pages [94, 114, 62, 80, 85, 87, 93, 87, 80, 87, 103, 92, 100, 92, 101]\n",
      "events [13, 19, 1, 7, 3, 20, 11, 0, 3, 1, 0, 13, 2, 10, 4]\n",
      "\n",
      "Size 25.0, number of events retrieved 633\n",
      "Size 25.0, number of pages retrieved 2706\n",
      "pages [187, 228, 124, 159, 169, 174, 185, 174, 160, 173, 205, 183, 200, 184, 201]\n",
      "events [48, 94, 5, 22, 7, 99, 45, 38, 36, 27, 32, 62, 18, 70, 30]\n",
      "\n",
      "Size 50.0, number of events retrieved 2314\n",
      "Size 50.0, number of pages retrieved 5404\n",
      "pages [373, 456, 248, 317, 338, 347, 370, 348, 319, 345, 409, 365, 399, 368, 402]\n",
      "events [206, 287, 31, 91, 48, 286, 175, 152, 103, 151, 139, 147, 82, 333, 83]\n",
      "\n",
      "Size 100.0, number of events retrieved 9990\n",
      "Size 100.0, number of pages retrieved 10801\n",
      "pages [745, 911, 496, 633, 676, 694, 739, 696, 638, 689, 818, 730, 797, 735, 804]\n",
      "events [1044, 1143, 143, 341, 150, 912, 813, 793, 606, 706, 692, 706, 468, 1158, 315]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset:\", dataset_name)\n",
    "print(\"ids\", subject_ids)\n",
    "for train_size in n2_subsampling_factors_dict[dataset_name]:\n",
    "    data_feed = FeederDataset(\n",
    "        dataset, subject_ids, constants.N2_RECORD, which_expert=which_expert,\n",
    "        n2_subsampling_factor=(train_size / 100))\n",
    "    all_n2_pages = data_feed.get_pages(pages_subset='n2')\n",
    "    n2_sum = np.concatenate(all_n2_pages).size\n",
    "    \n",
    "    all_events = data_feed.get_stamps(pages_subset='n2')\n",
    "    events_sum = np.concatenate(all_events, axis=0).shape[0]\n",
    "    \n",
    "    print(\"\\nSize %s, number of events retrieved %d\" % (train_size, events_sum))\n",
    "    print(\"Size %s, number of pages retrieved %d\" % (train_size, n2_sum))\n",
    "    print(\"pages\", [p.size for p in all_n2_pages])\n",
    "    print(\"events\", [p.shape[0] for p in all_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f54f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
