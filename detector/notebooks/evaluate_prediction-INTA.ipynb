{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import gridspec\n",
    "import pprint\n",
    "\n",
    "detector_path = '..'\n",
    "sys.path.append(detector_path)\n",
    "\n",
    "from sleep.inta import KEY_EEG, KEY_PAGES, KEY_ID, KEY_MARKS\n",
    "from sleep.data_ops import seq2inter, inter2seq, seq2inter_with_pages\n",
    "from sleep.inta import INTA\n",
    "from evaluation import data_manipulation\n",
    "from sleep import data_ops\n",
    "from sleep import postprocessing\n",
    "from utils import param_keys\n",
    "from evaluation import metrics\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_border_size(my_p):\n",
    "    border_duration = my_p[param_keys.BORDER_DURATION]\n",
    "    fs = my_p[param_keys.FS]\n",
    "    border_size = fs * border_duration\n",
    "    return border_size\n",
    "\n",
    "\n",
    "def get_page_size(my_p):\n",
    "    page_duration = my_p[param_keys.PAGE_DURATION]\n",
    "    fs = my_p[param_keys.FS]\n",
    "    page_size = fs * page_duration\n",
    "    return page_size\n",
    "\n",
    "\n",
    "def prepare_labels(y, params):\n",
    "    \"\"\"Ensures that label data has the proper shape.\"\"\"\n",
    "    time_stride = 8\n",
    "    border_size = get_border_size(params)\n",
    "    page_size = get_page_size(params)\n",
    "    crop_size = page_size + 2 * border_size\n",
    "    if y.shape[1] == crop_size:\n",
    "        # We need to remove borders and downsampling for val labels.\n",
    "        y = y[:, border_size:-border_size:time_stride]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate average predictions\n",
    "n_try_list = [0, 1, 2, 3]\n",
    "results_dir = os.path.join('..', 'results')\n",
    "pred_train_list = []\n",
    "pred_val_list = []\n",
    "pred_test_list = []\n",
    "for n_try_single in n_try_list:\n",
    "    ckpt_folder = '20190325_v2bn_fixed_loading_train_mass/bsf_try%d' % n_try_single\n",
    "    ckpt_path = os.path.join(results_dir, ckpt_folder)\n",
    "    save_dir = os.path.join(results_dir, 'predictions_inta', ckpt_folder)\n",
    "    print('Reading from', save_dir)\n",
    "    pred_train_list.append(np.load(os.path.join(save_dir, 'y_pred_train.npy'), allow_pickle=True))\n",
    "    pred_val_list.append(np.load(os.path.join(save_dir, 'y_pred_val.npy'), allow_pickle=True))\n",
    "    pred_test_list.append(np.load(os.path.join(save_dir, 'y_pred_test.npy'), allow_pickle=True))\n",
    "\n",
    "n_train = pred_train_list[0].shape[0]\n",
    "n_val = pred_val_list[0].shape[0]\n",
    "n_test = pred_test_list[0].shape[0]    \n",
    "print(n_train, n_val, n_test)\n",
    "\n",
    "avg_pred_train = []\n",
    "for i in range(n_train):\n",
    "    mean = pred_train_list[0][i] + pred_train_list[1][i] + pred_train_list[2][i] + pred_train_list[3][i]\n",
    "    mean = mean / 4\n",
    "    avg_pred_train.append(mean)\n",
    "\n",
    "avg_pred_val = []\n",
    "for i in range(n_val):\n",
    "    mean = pred_val_list[0][i] + pred_val_list[1][i] + pred_val_list[2][i] + pred_val_list[3][i]\n",
    "    mean = mean / 4\n",
    "    avg_pred_val.append(mean)\n",
    "    \n",
    "avg_pred_test = []\n",
    "for i in range(n_test):\n",
    "    mean = pred_test_list[0][i] + pred_test_list[1][i] + pred_test_list[2][i] + pred_test_list[3][i]\n",
    "    mean = mean / 4\n",
    "    avg_pred_test.append(mean)\n",
    "\n",
    "# save as try 4\n",
    "# Save predictions\n",
    "results_dir = os.path.join('..', 'results')\n",
    "ckpt_folder = '20190325_v2bn_fixed_loading_train_mass/bsf_avg'\n",
    "save_dir = os.path.join(results_dir, 'predictions_inta', ckpt_folder)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "print('Saving predictions at %s' % save_dir)\n",
    "np.save(os.path.join(save_dir, 'y_pred_train.npy'), avg_pred_train)\n",
    "np.save(os.path.join(save_dir, 'y_pred_val.npy'), avg_pred_val)\n",
    "np.save(os.path.join(save_dir, 'y_pred_test.npy'), avg_pred_test)\n",
    "print('Predictions saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join('..', 'results')\n",
    "# ckpt_folder = 'predictions_inta/v1_bn_fixed_files'\n",
    "# ckpt_folder = 'predictions_inta/v1_bn_fixed_files_trainINTA'\n",
    "ckpt_folder = 'predictions_inta/20190328_v2bn_fixed_inta_train_inta/bsf_avg'\n",
    "\n",
    "ckpt_path = os.path.join(results_dir, ckpt_folder)\n",
    "\n",
    "# Load data\n",
    "dataset = INTA(load_checkpoint=True)\n",
    "\n",
    "params = param_keys.default_params.copy()\n",
    "\n",
    "print('Restoring from %s' % ckpt_path)\n",
    "\n",
    "# Get training set ids\n",
    "print('Loading training set and splitting')\n",
    "all_train_ids = dataset.train_ids\n",
    "# Split to form validation set\n",
    "train_ids, val_ids = data_manipulation.split_ids_list(\n",
    "    all_train_ids, seed=SEED)\n",
    "print('Training set IDs:', train_ids)\n",
    "print('Validation set IDs:', val_ids)\n",
    "\n",
    "# Get test data\n",
    "print('Loading testing')\n",
    "test_ids = dataset.test_ids\n",
    "print('Testing set IDs:', test_ids)\n",
    "\n",
    "# Get data for predictions\n",
    "augmented_page = False\n",
    "which_expert = 1\n",
    "\n",
    "border_size = get_border_size(params)\n",
    "x_train, y_train = dataset.get_subset_data(\n",
    "    train_ids, augmented_page=augmented_page, border_size=border_size,\n",
    "    which_expert=which_expert, verbose=True)\n",
    "pages_train = dataset.get_subset_pages(train_ids, verbose=True)\n",
    "\n",
    "x_val, y_val = dataset.get_subset_data(\n",
    "    val_ids, augmented_page=augmented_page, border_size=border_size,\n",
    "    which_expert=which_expert, verbose=True)\n",
    "pages_val = dataset.get_subset_pages(val_ids, verbose=True)\n",
    "\n",
    "x_test, y_test = dataset.get_subset_data(\n",
    "    test_ids, augmented_page=augmented_page, border_size=border_size,\n",
    "    which_expert=which_expert, verbose=True)\n",
    "pages_test = dataset.get_subset_pages(test_ids, verbose=True)\n",
    "\n",
    "y_train = [prepare_labels(y, params) for y in y_train]\n",
    "y_val = [prepare_labels(y, params) for y in y_val]\n",
    "y_test = [prepare_labels(y, params) for y in y_test]\n",
    "\n",
    "# We keep each patient separate, to see variation of performance\n",
    "# between individuals\n",
    "# save_dir = os.path.join(results_dir, 'predictions', ckpt_folder)\n",
    "save_dir = os.path.join(results_dir, ckpt_folder)\n",
    "y_pred_train = np.load(os.path.join(save_dir, 'y_pred_train.npy'), allow_pickle=True)\n",
    "y_pred_val = np.load(os.path.join(save_dir, 'y_pred_val.npy'), allow_pickle=True)\n",
    "y_pred_test = np.load(os.path.join(save_dir, 'y_pred_test.npy'), allow_pickle=True)\n",
    "\n",
    "# Keep only class 1 probability\n",
    "y_pred_train = [y[..., 1] for y in y_pred_train]\n",
    "y_pred_val = [y[..., 1] for y in y_pred_val]\n",
    "y_pred_test = [y[..., 1] for y in y_pred_test]\n",
    "\n",
    "print('Predictions Loaded')\n",
    "\n",
    "# Threshold to binarize\n",
    "\n",
    "thr = 0.5\n",
    "# thr = 0.001\n",
    "\n",
    "y_pred_train = [(y >= thr).astype(np.int32) for y in y_pred_train]\n",
    "y_pred_val = [(y >= thr).astype(np.int32) for y in y_pred_val]\n",
    "y_pred_test = [(y >= thr).astype(np.int32) for y in y_pred_test]\n",
    "\n",
    "# # Compute by sample stats\n",
    "# y_train_flatten = np.concatenate(y_train, axis=0).flatten()\n",
    "# y_pred_train_flatten = np.concatenate(y_pred_train, axis=0).flatten()\n",
    "\n",
    "# y_val_flatten = np.concatenate(y_val, axis=0).flatten()\n",
    "# y_pred_val_flatten = np.concatenate(y_pred_val, axis=0).flatten()\n",
    "\n",
    "# y_test_flatten = np.concatenate(y_test, axis=0).flatten()\n",
    "# y_pred_test_flatten = np.concatenate(y_pred_test, axis=0).flatten()\n",
    "\n",
    "# bs_train = metrics.by_sample_confusion(y_train_flatten, y_pred_train_flatten, input_is_binary=True)\n",
    "# bs_val = metrics.by_sample_confusion(y_val_flatten, y_pred_val_flatten, input_is_binary=True)\n",
    "# bs_test = metrics.by_sample_confusion(y_test_flatten, y_pred_test_flatten, input_is_binary=True)\n",
    "\n",
    "# print('Train BS stats')\n",
    "# pprint.pprint(bs_train)\n",
    "\n",
    "# print('Val BS stats')\n",
    "# pprint.pprint(bs_val)\n",
    "\n",
    "# print('Test BS stats')\n",
    "# pprint.pprint(bs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to concatenate the pages and transform to intervals\n",
    "y_train = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_train, pages_train)]\n",
    "y_pred_train = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_pred_train, pages_train)]\n",
    "\n",
    "y_val = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_val, pages_val)]\n",
    "y_pred_val = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_pred_val, pages_val)]\n",
    "\n",
    "y_test = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_test, pages_test)]\n",
    "y_pred_test = [data_ops.seq2inter_with_pages(y, pages) for y, pages in zip(y_pred_test, pages_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_effect = int(params[param_keys.FS] / 8)\n",
    "fs_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(marks, fs, delta_combine=0.3, min_duration=0.2, max_duration=4.0):\n",
    "    marks = postprocessing.combine_close_marks(marks, fs, delta_combine)\n",
    "    marks = postprocessing.filter_duration_marks(marks, fs, min_duration, max_duration)\n",
    "    return marks\n",
    "\n",
    "\n",
    "y_pred_train = [post_process(y, fs_effect) for y in y_pred_train]\n",
    "y_pred_val = [post_process(y, fs_effect) for y in y_pred_val]\n",
    "y_pred_test = [post_process(y, fs_effect) for y in y_pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set performance F1 vs IoU\n",
    "iou_array = np.arange(1, 10) * 0.1\n",
    "print(iou_array)\n",
    "f1_list = []\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "for iou_thr in iou_array:\n",
    "    print(iou_thr)\n",
    "    # be_train = [metrics.by_event_confusion(y, y_pred, iou_thr=0.3) for y, y_pred in zip(y_train, y_pred_train)]\n",
    "    # be_val = [metrics.by_event_confusion(y, y_pred, iou_thr=0.3) for y, y_pred in zip(y_val, y_pred_val)]\n",
    "    be_test = [metrics.by_event_confusion(y, y_pred, iou_thr=iou_thr) for y, y_pred in zip(y_test, y_pred_test)]\n",
    "    mean_f1 = np.mean([m['f1_score'] for m in be_test])\n",
    "    mean_prec = np.mean([m['precision'] for m in be_test])\n",
    "    mean_rec = np.mean([m['recall'] for m in be_test])\n",
    "    f1_list.append(mean_f1)\n",
    "    prec_list.append(mean_prec)\n",
    "    rec_list.append(mean_rec)\n",
    "print('Done')\n",
    "\n",
    "print('IoU')\n",
    "print(iou_array)\n",
    "print('')\n",
    "print('Precision')\n",
    "print(prec_list)\n",
    "print('')\n",
    "print('Recall')\n",
    "print(rec_list)\n",
    "print('')\n",
    "print('F1score')\n",
    "print(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('IoU')\n",
    "pprint.pprint(iou_array)\n",
    "print('')\n",
    "print('Precision')\n",
    "pprint.pprint(prec_list)\n",
    "print('')\n",
    "print('Recall')\n",
    "pprint.pprint(rec_list)\n",
    "print('')\n",
    "print('F1score')\n",
    "pprint.pprint(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list_try_0 = f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list_try_1 = f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list_try_2 = f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list_try_3 = f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = np.stack([iou_array, f1_list], axis=1)\n",
    "np.savetxt(\"f1_january_model_run%d.csv\" % n_try, to_save, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_values = np.stack([f1_list_try_0, f1_list_try_1, f1_list_try_2, f1_list_try_3], axis=0)\n",
    "f1_means = np.mean(f1_values, axis=0)\n",
    "f1_std = np.std(f1_values, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4), dpi=100)\n",
    "plt.errorbar(iou_array, f1_means, f1_std, color='k', linewidth=2, markersize=12, marker='.')\n",
    "plt.title('Performance on test set (mean of 4 runs)', fontsize=10)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.yticks([0.1*i for i in range(1, 10)])\n",
    "plt.xticks([0.1*i for i in range(1, 10)])\n",
    "plt.xlabel('Overlap threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4), dpi=100)\n",
    "plt.plot(iou_array, f1_list, color='k', linewidth=2, markersize=12, marker='.')\n",
    "plt.title('Performance on test set (ensemble of 4 runs)', fontsize=10)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.yticks([0.1*i for i in range(1, 10)])\n",
    "plt.xticks([0.1*i for i in range(1, 10)])\n",
    "plt.xlabel('Overlap threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "be_train = [metrics.by_event_confusion(y, y_pred, iou_thr=0.3) for y, y_pred in zip(y_train, y_pred_train)]\n",
    "be_val = [metrics.by_event_confusion(y, y_pred, iou_thr=0.3) for y, y_pred in zip(y_val, y_pred_val)]\n",
    "be_test = [metrics.by_event_confusion(y, y_pred, iou_thr=0.3) for y, y_pred in zip(y_test, y_pred_test)]\n",
    "\n",
    "print('BE train stats')\n",
    "pprint.pprint(be_train)\n",
    "\n",
    "print('BE val stats')\n",
    "pprint.pprint(be_val)\n",
    "\n",
    "print('BE test stats')\n",
    "pprint.pprint(be_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Mean F1 score: %1.4f' % np.mean([m['f1_score'] for m in be_train]))\n",
    "print('Val Mean F1 score: %1.4f' % np.mean([m['f1_score'] for m in be_val]))\n",
    "print('Test Mean F1 score: %1.4f' % np.mean([m['f1_score'] for m in be_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=100)\n",
    "text_space = 0.005\n",
    "# Training results\n",
    "for i, stats in enumerate(be_train):\n",
    "    if i==0:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='g', label='Train')\n",
    "    else:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='g')\n",
    "    ax.annotate(train_ids[i], (stats['recall']+text_space, stats['precision']+text_space))\n",
    "    \n",
    "# Validation results\n",
    "for i, stats in enumerate(be_val):\n",
    "    if i==0:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='r', label='Val')\n",
    "    else:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='r')\n",
    "    ax.annotate(val_ids[i], (stats['recall']+text_space, stats['precision']+text_space))\n",
    "    \n",
    "# Testing results\n",
    "for i, stats in enumerate(be_test):\n",
    "    if i==0:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='b', label='Test')\n",
    "    else:\n",
    "        ax.scatter(stats['recall'], stats['precision'], c='b')\n",
    "    ax.annotate(test_ids[i], (stats['recall']+text_space, stats['precision']+text_space))\n",
    "    \n",
    "\n",
    "delta = 0.01 \n",
    "x_ = np.arange(1, 100) * delta \n",
    "y_ = np.arange(1, 100) * delta \n",
    "X, Y = np.meshgrid(x_, y_)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "CS = ax.contour(X, Y, Z)\n",
    "ax.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left', bbox_to_anchor=(1.05, 0.35), labelspacing=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set  [5, 7, 9]\n",
    "subject_id = 5\n",
    "\n",
    "n2_signal, n2_marks_1 = dataset.get_subject_data(subject_id, which_expert=1, verbose=True)\n",
    "# _, n2_marks_2 = dataset.get_subject_data(subject_id, which_expert=2, verbose=True)\n",
    "pages = dataset.get_subject_pages(subject_id)\n",
    "\n",
    "last_moment = (pages[-1]+1) * 4000\n",
    "\n",
    "nnet = y_pred_test[test_ids.index(subject_id)]\n",
    "# nnet = y_pred_train[train_ids.index(subject_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read unchanged marks (INTA)\n",
    "NAMES = [\n",
    "    'ADGU101504',\n",
    "    'ALUR012904',\n",
    "    'BECA011405',  # we will skip this one for now\n",
    "    'BRCA062405',\n",
    "    'BRLO041102',\n",
    "    'BTOL083105',\n",
    "    'BTOL090105',\n",
    "    'CAPO092605',\n",
    "    'CRCA020205',\n",
    "    'ESCI031905',\n",
    "    'TAGO061203']\n",
    "\n",
    "path_marks_file = os.path.abspath(os.path.join('..', '..', 'data', 'ssdata_inta', 'label', 'marks', 'SS_%s.txt'% NAMES[subject_id-1]))\n",
    "print(path_marks_file)\n",
    "original_marks = np.loadtxt(path_marks_file)\n",
    "original_marks = original_marks[original_marks[:, -1] == 1]\n",
    "marks = original_marks[:, [0, 1]]\n",
    "raw_marks = np.round(marks).astype(np.int32)\n",
    "for i in range(raw_marks.shape[0]):\n",
    "    if raw_marks[i, 0] > raw_marks[i, 1]:\n",
    "        aux = raw_marks[i, 0]\n",
    "        raw_marks[i, 0] = raw_marks[i, 1]\n",
    "        raw_marks[i, 1] = aux\n",
    "valid = original_marks[:, 4].astype(np.int32)\n",
    "\n",
    "raw_marks_0 = raw_marks[valid == 0]\n",
    "raw_marks_1 = raw_marks[valid == 1]\n",
    "raw_marks_2 = raw_marks[valid == 2]\n",
    "\n",
    "\n",
    "subject_index = [dataset.data[i]['subject_id'] for i in range(len(dataset.data))].index(subject_id)\n",
    "signal_len = dataset.data[subject_index]['signal'].shape[0]\n",
    "this_n2_pages = dataset.data[subject_index]['pages']\n",
    "\n",
    "# Turn into segments\n",
    "raw_marks_0 = data_ops.inter2seq(raw_marks_0, 0, signal_len-1)\n",
    "raw_marks_1 = data_ops.inter2seq(raw_marks_1, 0, signal_len-1)\n",
    "raw_marks_2 = data_ops.inter2seq(raw_marks_2, 0, signal_len-1)\n",
    "\n",
    "raw_marks_0 = data_ops.extract_pages(raw_marks_0, this_n2_pages, dataset.page_size)\n",
    "raw_marks_1 = data_ops.extract_pages(raw_marks_1, this_n2_pages, dataset.page_size)\n",
    "raw_marks_2 = data_ops.extract_pages(raw_marks_2, this_n2_pages, dataset.page_size)\n",
    "\n",
    "print(raw_marks_0.shape, raw_marks_1.shape, raw_marks_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet[0, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_page(page, signal, e1_marks, nnet, n2_pages, fs):\n",
    "    fig = plt.figure(figsize=(15, 8), dpi=100) \n",
    "    gs = gridspec.GridSpec(6, 1, height_ratios=[4, 1, 1, 1, 1, 1]) \n",
    "    \n",
    "    page = page - 1\n",
    "    \n",
    "    segment_signal = signal[page, :]\n",
    "    segment_e1_marks = e1_marks[page, :]\n",
    "    segment_nn_marks = nnet[page, :]\n",
    "    time_axis = np.arange(signal.shape[1])/fs\n",
    "    \n",
    "    segment_raw0_marks = raw_marks_0[page, :]\n",
    "    segment_raw1_marks = raw_marks_1[page, :]\n",
    "    segment_raw2_marks = raw_marks_2[page, :]\n",
    "    \n",
    "    # Signal\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    ax0.plot(time_axis, segment_signal, linewidth=1)\n",
    "    ax0.set_yticks([])\n",
    "    ax0.set_xlim([0, 20])\n",
    "    ax0.set_ylim([-10, 10])\n",
    "    ax0.set_title('EEG Signal, F4-C4 (intervals of 0.5s are shown), original page %d' % n2_pages[page])\n",
    "    ax0.set_xticks([0, 5, 10, 15, 20])\n",
    "    ax0.set_xticks(np.arange(0, 20, 0.5), minor=True)\n",
    "    ax0.grid(b=True, axis='x', which='minor')\n",
    "    \n",
    "    \n",
    "    # Original\n",
    "    ax_raw0 = fig.add_subplot(gs[1])\n",
    "    ax_raw0.imshow(segment_raw0_marks[np.newaxis, :], interpolation=None, aspect='auto', cmap='gray')\n",
    "    ax_raw0.set_xticks([])\n",
    "    ax_raw0.set_yticks([])\n",
    "    ax_raw0.set_title('Raw, valid 0 (white is active)')\n",
    "    \n",
    "    ax_raw1 = fig.add_subplot(gs[2])\n",
    "    ax_raw1.imshow(segment_raw1_marks[np.newaxis, :], interpolation=None, aspect='auto', cmap='gray')\n",
    "    ax_raw1.set_xticks([])\n",
    "    ax_raw1.set_yticks([])\n",
    "    ax_raw1.set_title('Raw, valid 1 (white is active)')\n",
    "    \n",
    "    ax_raw2 = fig.add_subplot(gs[3])\n",
    "    ax_raw2.imshow(segment_raw2_marks[np.newaxis, :], interpolation=None, aspect='auto', cmap='gray')\n",
    "    ax_raw2.set_xticks([])\n",
    "    ax_raw2.set_yticks([])\n",
    "    ax_raw2.set_title('Raw, valid 2 (white is active)')\n",
    "    \n",
    "    \n",
    "    # Expert mark\n",
    "    ax2 = fig.add_subplot(gs[4])\n",
    "    ax2.imshow(segment_e1_marks[np.newaxis, :], interpolation=None, aspect='auto', cmap='gray')\n",
    "    #ax2.axis('off')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_title('Expert 1 (fixed) (white is active)')\n",
    "    \n",
    "    # Neural net\n",
    "    ax4 = fig.add_subplot(gs[5])\n",
    "    # ax4.imshow(segment_nn_marks[np.newaxis, :], interpolation=None, aspect='auto', cmap='gray', vmin=0.0, vmax=1.0)\n",
    "    ax4.plot(segment_nn_marks, linewidth=1.5, color='r')\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_ylim([0,1])\n",
    "    ax4.set_xlim([0,500])\n",
    "    ax4.set_yticks([0, 1])\n",
    "    ax4.set_yticks([0.5], minor=True)\n",
    "    ax4.grid(b=True, axis='y', which='minor')\n",
    "    # ax4.axis('off')\n",
    "    ax4.set_title('Model probability prediction (white is active)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(\n",
    "    lambda page: plot_page(page, n2_signal, n2_marks_1, nnet, pages, dataset.fs),\n",
    "    page=widgets.IntSlider(min=1,max=n2_signal.shape[0],step=1,value=1, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Particular result on test set\n",
    "\n",
    "\n",
    "subject_id = 2\n",
    "page = 480\n",
    "fs = 200\n",
    "start_time = 0\n",
    "end_time = 20\n",
    "\n",
    "n2_signal, n2_marks_1 = dataset.get_subject_data(subject_id, which_expert=1)\n",
    "_, n2_marks_2 = dataset.get_subject_data(subject_id, which_expert=2)\n",
    "pages = dataset.get_subject_pages(subject_id)\n",
    "\n",
    "complete_segment = n2_signal[page, :]\n",
    "time_axis = np.arange(complete_segment.size) / fs\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3), dpi=100) \n",
    "gs = gridspec.GridSpec(4, 1, height_ratios=[3, 1, 1, 3])\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 1), dpi=100)\n",
    "time_axis = np.arange(complete_segment.size) / fs\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.plot(time_axis, complete_segment, linewidth=0.8)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlim([start_time, end_time])\n",
    "ax.set_ylim([-5, 5])\n",
    "# ax0.set_title('EEG Signal, C3-CLE (intervals of 0.5s are shown), original page %d' % n2_pages[page])\n",
    "ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(start_time, end_time, 0.5), minor=True)\n",
    "ax.grid(b=True, axis='x', which='minor')\n",
    "# plt.show()\n",
    "\n",
    "start_sample = int(start_time*(fs))\n",
    "end_sample = int(end_time*(fs))\n",
    "\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(10, 0.3), dpi=100)\n",
    "ax = fig.add_subplot(gs[1])\n",
    "ax.imshow(n2_marks_1[page, start_sample:end_sample][np.newaxis, :], interpolation=None, aspect='auto', extent=[0, 20, 32, 1], cmap='gray')\n",
    "ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "start_sample = int(start_time*(fs/10))\n",
    "end_sample = int(end_time*(fs/10))\n",
    "\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(10, 0.3), dpi=100)\n",
    "ax = fig.add_subplot(gs[2])\n",
    "ax.imshow(test_preds[page, start_sample:end_sample][np.newaxis, :], interpolation=None, aspect='auto', extent=[0, 20, 32, 1], cmap='gray')\n",
    "ax.axis('off')\n",
    "# plt.show()\n",
    "#print(test_preds[page, start_sample:end_sample])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs = complete_segment[np.newaxis, :]\n",
    "outputs = compute_cwt(inputs, [1.5], 200, 1, 30, 32, flattening=True)\n",
    "outputs = tf.layers.average_pooling2d(inputs=outputs, pool_size=(8, 1), strides=(8, 1))\n",
    "# outputs = tf.log(outputs + 1e-3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    my_cwt = sess.run(outputs)\n",
    "# print(my_cwt.shape)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 1), dpi=100)\n",
    "ax = fig.add_subplot(gs[3])\n",
    "my_cwt_image = np.transpose(my_cwt[0, :, :, 0], (1, 0))\n",
    "ax.imshow(my_cwt_image, interpolation=None, aspect='auto', extent=[0, 20, 32, 1])\n",
    "ax.axis('off')\n",
    "# ax.set_title('CWT CMorlet, [3, 40] Hz, Fb = 1.5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'comparison_data'\n",
    "\n",
    "# Chambon\n",
    "convnet_data = np.loadtxt(os.path.join(folder, 'Convnet.csv'), delimiter=',')\n",
    "\n",
    "# Expert\n",
    "expert_data_mean = np.loadtxt(os.path.join(folder, 'MeanExpert.csv'), delimiter=',')\n",
    "expert_data_std = np.loadtxt(os.path.join(folder, 'UpperExpert.csv'), delimiter=',')\n",
    "expert_data_std[:, 1] = expert_data_std[:, 1] - expert_data_mean[:, 1]\n",
    "\n",
    "# Proposed model\n",
    "filename = 'f1_january_model_run%d.csv'\n",
    "model_data = []\n",
    "for n_try in [0, 1, 2, 3]:\n",
    "    tmp_data = np.loadtxt(os.path.join(folder, filename % n_try), delimiter=',')\n",
    "    model_data.append(tmp_data)\n",
    "model_data = np.stack(model_data, axis=0)\n",
    "model_data_mean = np.mean(model_data, axis=0)\n",
    "model_data_mean[:, 0] = np.around(model_data_mean[:, 0], decimals=1)\n",
    "model_data_std = np.std(model_data, axis=0)\n",
    "model_data_std[:, 0] = model_data_mean[:, 0]\n",
    "\n",
    "# Ensemble model\n",
    "model_data_ensemble = np.loadtxt(os.path.join(folder, 'f1_january_model_ensemble.csv'), delimiter=',')\n",
    "model_data_ensemble[:, 0] = np.around(model_data_ensemble[:, 0], decimals=1)\n",
    "\n",
    "# --hardcoding:\n",
    "model_data_ensemble[:, 1] = np.array(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4), dpi=100)\n",
    "\n",
    "# Complete plot\n",
    "ax[0].plot(expert_data_mean[:, 0], expert_data_mean[:, 1], linewidth=2, markersize=12, marker='.', label='Expert Performance')\n",
    "ax[0].fill_between(\n",
    "    expert_data_mean[:, 0], \n",
    "    expert_data_mean[:, 1]-expert_data_std[:, 1], \n",
    "    expert_data_mean[:, 1]+expert_data_std[:, 1], \n",
    "    alpha=0.3)\n",
    "\n",
    "# ax[0].plot(model_data_mean[:, 0], model_data_mean[:, 1], linewidth=2, markersize=12, marker='.', label='Proposed Model')\n",
    "ax[0].plot(model_data_ensemble[:, 0], model_data_ensemble[:, 1], linewidth=2, markersize=12, marker='.', label='Proposed Model')\n",
    "ax[0].plot(convnet_data[:, 0], convnet_data[:, 1], linewidth=2, markersize=12, marker='.', label='Chambon et al.')\n",
    "ax[0].set_title('Detection Performance', fontsize=10)\n",
    "# ax[0].axis('square')\n",
    "ax[0].set_xlim([0, 1])\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].set_yticks([0.1*i for i in range(1, 10)])\n",
    "ax[0].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[0].set_xlabel('Threshold for IoU')\n",
    "ax[0].set_ylabel('F1-score')\n",
    "# ax[0].legend(loc='lower left')\n",
    "ax[0].grid()\n",
    "\n",
    "# Zoom plot\n",
    "ax[1].plot(expert_data_mean[:, 0], expert_data_mean[:, 1], linewidth=2, markersize=12, marker='.', \n",
    "           label='Expert Performance\\nPrivate Database\\nWarby et al. 2014')\n",
    "ax[1].fill_between(\n",
    "    expert_data_mean[:, 0], \n",
    "    expert_data_mean[:, 1]-expert_data_std[:, 1], \n",
    "    expert_data_mean[:, 1]+expert_data_std[:, 1], \n",
    "    alpha=0.3)\n",
    "\n",
    "# ax[0].plot(model_data_mean[:, 0], model_data_mean[:, 1], linewidth=2, markersize=12, marker='.', label='Proposed Model')\n",
    "ax[1].plot(model_data_ensemble[:, 0], model_data_ensemble[:, 1], linewidth=2, markersize=12, marker='.', \n",
    "           label='Proposed Model\\nINTA Database')\n",
    "ax[1].plot(convnet_data[:, 0], convnet_data[:, 1], linewidth=2, markersize=12, marker='.', \n",
    "           label='ConvNet\\nMASS Database\\nChambon et al. 2018')\n",
    "ax[1].set_title('Detection Performance (ZOOM)', fontsize=10)\n",
    "# ax[1].axis('square')\n",
    "ax[1].set_xlim([0.05, 0.75])\n",
    "ax[1].set_ylim([0.6, 0.85])\n",
    "# ax[1].set_yticks([0.1*i for i in range(1, 10)])\n",
    "# ax[1].set_xticks([0.1*i for i in range(1, 10)])\n",
    "ax[1].set_xlabel('Threshold for IoU')\n",
    "ax[1].set_ylabel('F1-score')\n",
    "ax[1].legend(loc='lower left', bbox_to_anchor=(1.05, 0.15), labelspacing=3)\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"f1_vs_iou_expert_mean.csv\", expert_data_mean, delimiter=\",\")\n",
    "np.savetxt(\"f1_vs_iou_expert_std.csv\", expert_data_std, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
